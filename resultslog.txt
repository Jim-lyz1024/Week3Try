python train.py --gpu 0 --seed 134 --output-dir output/CLIPAdapter-ViTB32-PACS-art_painting --dataset PACS --source-domains cartoon photo sketch --target-domains art_painting            --model CLIPAdapter --model-config-file config/clipadapter.yaml
E:\Python3\Lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 16
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 16
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/2] batch [5/496] loss 0.4683 (0.6346) acc 87.5000 (71.2500) lr 1.0000e-05 eta 1:43:15
epoch [1/2] batch [10/496] loss 0.4248 (0.5270) acc 81.2500 (78.1250) lr 1.0000e-05 eta 0:51:34
epoch [1/2] batch [15/496] loss 0.4028 (0.5240) acc 87.5000 (79.1667) lr 1.0000e-05 eta 0:34:20
epoch [1/2] batch [20/496] loss 0.5249 (0.4970) acc 87.5000 (80.9375) lr 1.0000e-05 eta 0:25:43
epoch [1/2] batch [25/496] loss 1.0264 (0.5256) acc 62.5000 (80.7500) lr 1.0000e-05 eta 0:20:33
epoch [1/2] batch [30/496] loss 0.4788 (0.5153) acc 75.0000 (80.0000) lr 1.0000e-05 eta 0:17:06
epoch [1/2] batch [35/496] loss 0.4885 (0.5278) acc 87.5000 (80.0000) lr 1.0000e-05 eta 0:14:38
epoch [1/2] batch [40/496] loss 0.4827 (0.5271) acc 81.2500 (80.4688) lr 1.0000e-05 eta 0:12:47
epoch [1/2] batch [45/496] loss 0.3999 (0.5136) acc 81.2500 (80.5556) lr 1.0000e-05 eta 0:11:21
epoch [1/2] batch [50/496] loss 0.4204 (0.5201) acc 87.5000 (80.3750) lr 1.0000e-05 eta 0:10:12
epoch [1/2] batch [55/496] loss 0.5317 (0.5150) acc 81.2500 (80.9091) lr 1.0000e-05 eta 0:09:15
epoch [1/2] batch [60/496] loss 0.5620 (0.5243) acc 75.0000 (80.5208) lr 1.0000e-05 eta 0:08:28
epoch [1/2] batch [65/496] loss 0.2091 (0.5084) acc 87.5000 (81.1538) lr 1.0000e-05 eta 0:07:48
epoch [1/2] batch [70/496] loss 0.2737 (0.5153) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:07:14
epoch [1/2] batch [75/496] loss 0.3711 (0.5170) acc 81.2500 (80.8333) lr 1.0000e-05 eta 0:06:44
epoch [1/2] batch [80/496] loss 0.4368 (0.5191) acc 93.7500 (80.7812) lr 1.0000e-05 eta 0:06:18
epoch [1/2] batch [85/496] loss 0.3149 (0.5101) acc 87.5000 (81.2500) lr 1.0000e-05 eta 0:05:56
epoch [1/2] batch [90/496] loss 0.4827 (0.5132) acc 87.5000 (81.3889) lr 1.0000e-05 eta 0:05:35
epoch [1/2] batch [95/496] loss 0.3757 (0.5112) acc 81.2500 (81.3816) lr 1.0000e-05 eta 0:05:17
epoch [1/2] batch [100/496] loss 0.9341 (0.5102) acc 62.5000 (81.4375) lr 1.0000e-05 eta 0:05:00
epoch [1/2] batch [105/496] loss 0.8706 (0.5116) acc 68.7500 (81.3095) lr 1.0000e-05 eta 0:04:46
epoch [1/2] batch [110/496] loss 0.3406 (0.5143) acc 87.5000 (81.1932) lr 1.0000e-05 eta 0:04:32
epoch [1/2] batch [115/496] loss 0.8452 (0.5139) acc 56.2500 (81.1957) lr 1.0000e-05 eta 0:04:20
epoch [1/2] batch [120/496] loss 0.9966 (0.5106) acc 62.5000 (81.3021) lr 1.0000e-05 eta 0:04:08
epoch [1/2] batch [125/496] loss 0.7119 (0.5089) acc 68.7500 (81.2500) lr 1.0000e-05 eta 0:03:58
epoch [1/2] batch [130/496] loss 0.4902 (0.5055) acc 87.5000 (81.4423) lr 1.0000e-05 eta 0:03:48
epoch [1/2] batch [135/496] loss 0.5820 (0.4973) acc 81.2500 (81.7130) lr 1.0000e-05 eta 0:03:39
epoch [1/2] batch [140/496] loss 0.3337 (0.5027) acc 93.7500 (81.6071) lr 1.0000e-05 eta 0:03:31
epoch [1/2] batch [145/496] loss 1.0703 (0.5062) acc 62.5000 (81.5086) lr 1.0000e-05 eta 0:03:23
epoch [1/2] batch [150/496] loss 0.2856 (0.5105) acc 87.5000 (81.2500) lr 1.0000e-05 eta 0:03:16
epoch [1/2] batch [155/496] loss 0.7925 (0.5100) acc 75.0000 (81.3710) lr 1.0000e-05 eta 0:03:09
epoch [1/2] batch [160/496] loss 0.5972 (0.5085) acc 62.5000 (81.3281) lr 1.0000e-05 eta 0:03:03
epoch [1/2] batch [165/496] loss 0.2262 (0.5064) acc 87.5000 (81.4015) lr 1.0000e-05 eta 0:02:57
epoch [1/2] batch [170/496] loss 0.3745 (0.5059) acc 81.2500 (81.3603) lr 1.0000e-05 eta 0:02:51
epoch [1/2] batch [175/496] loss 0.4436 (0.5063) acc 81.2500 (81.2500) lr 1.0000e-05 eta 0:02:46
epoch [1/2] batch [180/496] loss 0.4744 (0.5069) acc 87.5000 (81.1806) lr 1.0000e-05 eta 0:02:41
epoch [1/2] batch [185/496] loss 0.3574 (0.5065) acc 93.7500 (81.2500) lr 1.0000e-05 eta 0:02:36
epoch [1/2] batch [190/496] loss 0.7339 (0.5078) acc 75.0000 (81.2500) lr 1.0000e-05 eta 0:02:31
epoch [1/2] batch [195/496] loss 0.5264 (0.5083) acc 75.0000 (81.2179) lr 1.0000e-05 eta 0:02:27
epoch [1/2] batch [200/496] loss 0.3630 (0.5075) acc 87.5000 (81.1875) lr 1.0000e-05 eta 0:02:23
epoch [1/2] batch [205/496] loss 0.5405 (0.5131) acc 81.2500 (81.0671) lr 1.0000e-05 eta 0:02:19
epoch [1/2] batch [210/496] loss 0.8325 (0.5126) acc 68.7500 (81.1310) lr 1.0000e-05 eta 0:02:15
epoch [1/2] batch [215/496] loss 0.3196 (0.5146) acc 87.5000 (81.0756) lr 1.0000e-05 eta 0:02:12
epoch [1/2] batch [220/496] loss 0.4353 (0.5136) acc 81.2500 (81.0795) lr 1.0000e-05 eta 0:02:08
epoch [1/2] batch [225/496] loss 0.6890 (0.5139) acc 75.0000 (81.1389) lr 1.0000e-05 eta 0:02:05
epoch [1/2] batch [230/496] loss 0.4790 (0.5139) acc 75.0000 (81.0598) lr 1.0000e-05 eta 0:02:02
epoch [1/2] batch [235/496] loss 0.5161 (0.5133) acc 81.2500 (81.0638) lr 1.0000e-05 eta 0:01:59
epoch [1/2] batch [240/496] loss 0.4817 (0.5110) acc 81.2500 (81.1198) lr 1.0000e-05 eta 0:01:56
epoch [1/2] batch [245/496] loss 0.2563 (0.5115) acc 93.7500 (81.1224) lr 1.0000e-05 eta 0:01:53
epoch [1/2] batch [250/496] loss 0.5225 (0.5115) acc 81.2500 (81.1250) lr 1.0000e-05 eta 0:01:51
epoch [1/2] batch [255/496] loss 0.4946 (0.5139) acc 81.2500 (81.0049) lr 1.0000e-05 eta 0:01:48
epoch [1/2] batch [260/496] loss 0.5601 (0.5157) acc 75.0000 (80.9375) lr 1.0000e-05 eta 0:01:46
epoch [1/2] batch [265/496] loss 0.4487 (0.5172) acc 81.2500 (80.8726) lr 1.0000e-05 eta 0:01:43
epoch [1/2] batch [270/496] loss 0.7017 (0.5199) acc 68.7500 (80.7870) lr 1.0000e-05 eta 0:01:41
epoch [1/2] batch [275/496] loss 0.6304 (0.5219) acc 81.2500 (80.7500) lr 1.0000e-05 eta 0:01:39
epoch [1/2] batch [280/496] loss 0.4702 (0.5230) acc 87.5000 (80.7589) lr 1.0000e-05 eta 0:01:37
epoch [1/2] batch [285/496] loss 0.6870 (0.5277) acc 75.0000 (80.5702) lr 1.0000e-05 eta 0:01:35
epoch [1/2] batch [290/496] loss 0.7529 (0.5298) acc 68.7500 (80.5172) lr 1.0000e-05 eta 0:01:33
epoch [1/2] batch [295/496] loss 0.3928 (0.5312) acc 81.2500 (80.4449) lr 1.0000e-05 eta 0:01:31
epoch [1/2] batch [300/496] loss 0.3730 (0.5299) acc 81.2500 (80.4375) lr 1.0000e-05 eta 0:01:29
epoch [1/2] batch [305/496] loss 0.5767 (0.5307) acc 75.0000 (80.3893) lr 1.0000e-05 eta 0:01:27
epoch [1/2] batch [310/496] loss 0.7324 (0.5328) acc 75.0000 (80.3226) lr 1.0000e-05 eta 0:01:25
epoch [1/2] batch [315/496] loss 0.4775 (0.5355) acc 81.2500 (80.1984) lr 1.0000e-05 eta 0:01:23
epoch [1/2] batch [320/496] loss 0.3252 (0.5343) acc 87.5000 (80.2148) lr 1.0000e-05 eta 0:01:22
epoch [1/2] batch [325/496] loss 0.5400 (0.5334) acc 75.0000 (80.2115) lr 1.0000e-05 eta 0:01:20
epoch [1/2] batch [330/496] loss 0.8945 (0.5348) acc 56.2500 (80.1326) lr 1.0000e-05 eta 0:01:19
epoch [1/2] batch [335/496] loss 0.7607 (0.5345) acc 75.0000 (80.0933) lr 1.0000e-05 eta 0:01:17
epoch [1/2] batch [340/496] loss 0.3750 (0.5328) acc 87.5000 (80.1654) lr 1.0000e-05 eta 0:01:16
epoch [1/2] batch [345/496] loss 0.7271 (0.5308) acc 81.2500 (80.2536) lr 1.0000e-05 eta 0:01:14
epoch [1/2] batch [350/496] loss 0.4819 (0.5308) acc 81.2500 (80.2321) lr 1.0000e-05 eta 0:01:13
epoch [1/2] batch [355/496] loss 0.9370 (0.5340) acc 68.7500 (80.0880) lr 1.0000e-05 eta 0:01:11
epoch [1/2] batch [360/496] loss 0.4204 (0.5335) acc 81.2500 (80.1042) lr 1.0000e-05 eta 0:01:10
epoch [1/2] batch [365/496] loss 0.0657 (0.5322) acc 100.0000 (80.1541) lr 1.0000e-05 eta 0:01:09
epoch [1/2] batch [370/496] loss 0.3916 (0.5306) acc 87.5000 (80.2027) lr 1.0000e-05 eta 0:01:07
epoch [1/2] batch [375/496] loss 0.5391 (0.5306) acc 75.0000 (80.1833) lr 1.0000e-05 eta 0:01:06
epoch [1/2] batch [380/496] loss 0.3250 (0.5296) acc 87.5000 (80.2138) lr 1.0000e-05 eta 0:01:05
epoch [1/2] batch [385/496] loss 0.5615 (0.5310) acc 81.2500 (80.1786) lr 1.0000e-05 eta 0:01:04
epoch [1/2] batch [390/496] loss 0.5479 (0.5299) acc 81.2500 (80.2083) lr 1.0000e-05 eta 0:01:03
epoch [1/2] batch [395/496] loss 0.4846 (0.5292) acc 81.2500 (80.2532) lr 1.0000e-05 eta 0:01:02
epoch [1/2] batch [400/496] loss 0.5762 (0.5309) acc 75.0000 (80.1719) lr 1.0000e-05 eta 0:01:00
epoch [1/2] batch [405/496] loss 0.5107 (0.5300) acc 75.0000 (80.2006) lr 1.0000e-05 eta 0:00:59
epoch [1/2] batch [410/496] loss 0.4365 (0.5301) acc 81.2500 (80.1982) lr 1.0000e-05 eta 0:00:58
epoch [1/2] batch [415/496] loss 0.4946 (0.5301) acc 81.2500 (80.1958) lr 1.0000e-05 eta 0:00:57
epoch [1/2] batch [420/496] loss 0.4187 (0.5307) acc 87.5000 (80.1935) lr 1.0000e-05 eta 0:00:56
epoch [1/2] batch [425/496] loss 0.5615 (0.5305) acc 81.2500 (80.2353) lr 1.0000e-05 eta 0:00:55
epoch [1/2] batch [430/496] loss 0.6416 (0.5300) acc 75.0000 (80.2471) lr 1.0000e-05 eta 0:00:54
epoch [1/2] batch [435/496] loss 0.3545 (0.5288) acc 87.5000 (80.2730) lr 1.0000e-05 eta 0:00:53
epoch [1/2] batch [440/496] loss 0.1642 (0.5274) acc 100.0000 (80.3409) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [445/496] loss 0.1956 (0.5259) acc 93.7500 (80.4213) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [450/496] loss 0.1376 (0.5246) acc 93.7500 (80.4861) lr 1.0000e-05 eta 0:00:51
epoch [1/2] batch [455/496] loss 0.4617 (0.5243) acc 87.5000 (80.4808) lr 1.0000e-05 eta 0:00:50
epoch [1/2] batch [460/496] loss 0.4312 (0.5247) acc 81.2500 (80.4891) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [465/496] loss 0.7725 (0.5257) acc 75.0000 (80.4839) lr 1.0000e-05 eta 0:00:48
epoch [1/2] batch [470/496] loss 0.3149 (0.5250) acc 93.7500 (80.5186) lr 1.0000e-05 eta 0:00:47
epoch [1/2] batch [475/496] loss 0.5547 (0.5237) acc 81.2500 (80.5658) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [480/496] loss 0.5073 (0.5243) acc 81.2500 (80.5208) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [485/496] loss 0.4119 (0.5228) acc 87.5000 (80.5541) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [490/496] loss 0.5791 (0.5234) acc 87.5000 (80.5740) lr 1.0000e-05 eta 0:00:44
epoch [1/2] batch [495/496] loss 0.7397 (0.5256) acc 68.7500 (80.5051) lr 1.0000e-05 eta 0:00:43
epoch [2/2] batch [5/496] loss 0.5820 (0.4646) acc 68.7500 (81.2500) lr 2.0000e-03 eta 0:51:07
epoch [2/2] batch [10/496] loss 0.4062 (0.4256) acc 81.2500 (84.3750) lr 2.0000e-03 eta 0:25:24
epoch [2/2] batch [15/496] loss 0.4729 (0.4781) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:16:49
epoch [2/2] batch [20/496] loss 0.9512 (0.5142) acc 62.5000 (80.6250) lr 2.0000e-03 eta 0:12:32
epoch [2/2] batch [25/496] loss 0.7300 (0.5078) acc 75.0000 (80.7500) lr 2.0000e-03 eta 0:09:58
epoch [2/2] batch [30/496] loss 0.5781 (0.5138) acc 87.5000 (81.0417) lr 2.0000e-03 eta 0:08:15
epoch [2/2] batch [35/496] loss 0.8232 (0.5181) acc 75.0000 (80.8929) lr 2.0000e-03 eta 0:07:01
epoch [2/2] batch [40/496] loss 0.4199 (0.5050) acc 87.5000 (81.7188) lr 2.0000e-03 eta 0:06:06
epoch [2/2] batch [45/496] loss 0.7554 (0.5134) acc 81.2500 (81.1111) lr 2.0000e-03 eta 0:05:23
epoch [2/2] batch [50/496] loss 0.5298 (0.5270) acc 87.5000 (80.6250) lr 2.0000e-03 eta 0:04:48
epoch [2/2] batch [55/496] loss 0.7300 (0.5291) acc 68.7500 (80.6818) lr 2.0000e-03 eta 0:04:20
epoch [2/2] batch [60/496] loss 0.5537 (0.5302) acc 75.0000 (80.3125) lr 2.0000e-03 eta 0:03:57
epoch [2/2] batch [65/496] loss 0.4287 (0.5260) acc 81.2500 (80.6731) lr 2.0000e-03 eta 0:03:37
epoch [2/2] batch [70/496] loss 0.6548 (0.5269) acc 81.2500 (80.6250) lr 2.0000e-03 eta 0:03:20
epoch [2/2] batch [75/496] loss 0.3176 (0.5210) acc 81.2500 (80.9167) lr 2.0000e-03 eta 0:03:05
epoch [2/2] batch [80/496] loss 0.3982 (0.5172) acc 81.2500 (80.9375) lr 2.0000e-03 eta 0:02:52
epoch [2/2] batch [85/496] loss 0.2236 (0.5073) acc 93.7500 (81.4706) lr 2.0000e-03 eta 0:02:40
epoch [2/2] batch [90/496] loss 0.3003 (0.5049) acc 93.7500 (81.5972) lr 2.0000e-03 eta 0:02:30
epoch [2/2] batch [95/496] loss 0.6406 (0.5051) acc 75.0000 (81.6447) lr 2.0000e-03 eta 0:02:21
epoch [2/2] batch [100/496] loss 0.3081 (0.5011) acc 93.7500 (81.8125) lr 2.0000e-03 eta 0:02:13
epoch [2/2] batch [105/496] loss 0.8447 (0.5036) acc 56.2500 (81.6071) lr 2.0000e-03 eta 0:02:05
epoch [2/2] batch [110/496] loss 0.1654 (0.4923) acc 93.7500 (82.1591) lr 2.0000e-03 eta 0:01:58
epoch [2/2] batch [115/496] loss 0.3679 (0.4956) acc 87.5000 (82.0109) lr 2.0000e-03 eta 0:01:52
epoch [2/2] batch [120/496] loss 0.4253 (0.4963) acc 87.5000 (82.0312) lr 2.0000e-03 eta 0:01:46
epoch [2/2] batch [125/496] loss 0.8760 (0.4942) acc 75.0000 (82.2000) lr 2.0000e-03 eta 0:01:41
epoch [2/2] batch [130/496] loss 0.4014 (0.4932) acc 81.2500 (82.1635) lr 2.0000e-03 eta 0:01:36
epoch [2/2] batch [135/496] loss 0.3513 (0.4897) acc 81.2500 (82.1759) lr 2.0000e-03 eta 0:01:32
epoch [2/2] batch [140/496] loss 0.3706 (0.4902) acc 81.2500 (81.9643) lr 2.0000e-03 eta 0:01:28
epoch [2/2] batch [145/496] loss 0.3198 (0.4919) acc 87.5000 (81.7672) lr 2.0000e-03 eta 0:01:24
epoch [2/2] batch [150/496] loss 0.2450 (0.4947) acc 93.7500 (81.7083) lr 2.0000e-03 eta 0:01:20
epoch [2/2] batch [155/496] loss 0.3025 (0.4943) acc 81.2500 (81.7339) lr 2.0000e-03 eta 0:01:17
epoch [2/2] batch [160/496] loss 0.4961 (0.4898) acc 81.2500 (81.8359) lr 2.0000e-03 eta 0:01:13
epoch [2/2] batch [165/496] loss 0.1560 (0.4884) acc 93.7500 (81.8561) lr 2.0000e-03 eta 0:01:10
epoch [2/2] batch [170/496] loss 0.4727 (0.4860) acc 81.2500 (81.9853) lr 2.0000e-03 eta 0:01:07
epoch [2/2] batch [175/496] loss 0.1656 (0.4815) acc 93.7500 (82.2500) lr 2.0000e-03 eta 0:01:05
epoch [2/2] batch [180/496] loss 0.4055 (0.4846) acc 87.5000 (82.2569) lr 2.0000e-03 eta 0:01:02
epoch [2/2] batch [185/496] loss 0.7021 (0.4845) acc 87.5000 (82.3986) lr 2.0000e-03 eta 0:01:00
epoch [2/2] batch [190/496] loss 0.6147 (0.4913) acc 81.2500 (82.1711) lr 2.0000e-03 eta 0:00:57
epoch [2/2] batch [195/496] loss 0.6816 (0.4935) acc 81.2500 (82.1154) lr 2.0000e-03 eta 0:00:55
epoch [2/2] batch [200/496] loss 0.4805 (0.4910) acc 81.2500 (82.1250) lr 2.0000e-03 eta 0:00:53
epoch [2/2] batch [205/496] loss 0.6401 (0.4918) acc 75.0000 (82.1341) lr 2.0000e-03 eta 0:00:51
epoch [2/2] batch [210/496] loss 0.4519 (0.4895) acc 87.5000 (82.2917) lr 2.0000e-03 eta 0:00:49
epoch [2/2] batch [215/496] loss 0.5361 (0.4869) acc 81.2500 (82.3837) lr 2.0000e-03 eta 0:00:47
epoch [2/2] batch [220/496] loss 0.6665 (0.4871) acc 75.0000 (82.3580) lr 2.0000e-03 eta 0:00:46
epoch [2/2] batch [225/496] loss 0.4226 (0.4882) acc 87.5000 (82.3056) lr 2.0000e-03 eta 0:00:44
epoch [2/2] batch [230/496] loss 0.2683 (0.4879) acc 87.5000 (82.3098) lr 2.0000e-03 eta 0:00:42
epoch [2/2] batch [235/496] loss 0.6323 (0.4916) acc 68.7500 (82.1543) lr 2.0000e-03 eta 0:00:41
epoch [2/2] batch [240/496] loss 0.4841 (0.4912) acc 75.0000 (82.1094) lr 2.0000e-03 eta 0:00:39
epoch [2/2] batch [245/496] loss 0.5166 (0.4899) acc 81.2500 (82.1429) lr 2.0000e-03 eta 0:00:38
epoch [2/2] batch [250/496] loss 0.3333 (0.4869) acc 87.5000 (82.3000) lr 2.0000e-03 eta 0:00:36
epoch [2/2] batch [255/496] loss 0.2744 (0.4864) acc 100.0000 (82.3284) lr 2.0000e-03 eta 0:00:35
epoch [2/2] batch [260/496] loss 0.5181 (0.4877) acc 81.2500 (82.2596) lr 2.0000e-03 eta 0:00:34
epoch [2/2] batch [265/496] loss 0.3752 (0.4877) acc 87.5000 (82.2877) lr 2.0000e-03 eta 0:00:33
epoch [2/2] batch [270/496] loss 0.5781 (0.4865) acc 75.0000 (82.3380) lr 2.0000e-03 eta 0:00:31
epoch [2/2] batch [275/496] loss 0.3660 (0.4852) acc 81.2500 (82.3864) lr 2.0000e-03 eta 0:00:30
epoch [2/2] batch [280/496] loss 0.4844 (0.4856) acc 81.2500 (82.3661) lr 2.0000e-03 eta 0:00:29
epoch [2/2] batch [285/496] loss 0.5850 (0.4883) acc 68.7500 (82.1930) lr 2.0000e-03 eta 0:00:28
epoch [2/2] batch [290/496] loss 0.7354 (0.4906) acc 75.0000 (82.1121) lr 2.0000e-03 eta 0:00:27
epoch [2/2] batch [295/496] loss 0.3357 (0.4914) acc 87.5000 (82.0551) lr 2.0000e-03 eta 0:00:26
epoch [2/2] batch [300/496] loss 0.6416 (0.4934) acc 81.2500 (81.9792) lr 2.0000e-03 eta 0:00:25
epoch [2/2] batch [305/496] loss 0.5620 (0.4948) acc 75.0000 (81.8852) lr 2.0000e-03 eta 0:00:24
epoch [2/2] batch [310/496] loss 0.3333 (0.4930) acc 93.7500 (81.9556) lr 2.0000e-03 eta 0:00:23
epoch [2/2] batch [315/496] loss 0.5420 (0.4920) acc 81.2500 (81.9841) lr 2.0000e-03 eta 0:00:22
epoch [2/2] batch [320/496] loss 0.3748 (0.4928) acc 87.5000 (81.9922) lr 2.0000e-03 eta 0:00:21
epoch [2/2] batch [325/496] loss 0.7617 (0.4922) acc 68.7500 (82.0192) lr 2.0000e-03 eta 0:00:20
epoch [2/2] batch [330/496] loss 0.3975 (0.4920) acc 81.2500 (82.0644) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [335/496] loss 0.3057 (0.4939) acc 93.7500 (81.9590) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [340/496] loss 0.8389 (0.4936) acc 62.5000 (81.9485) lr 2.0000e-03 eta 0:00:18
epoch [2/2] batch [345/496] loss 0.1775 (0.4927) acc 100.0000 (81.9746) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [350/496] loss 0.3691 (0.4909) acc 81.2500 (82.0179) lr 2.0000e-03 eta 0:00:16
epoch [2/2] batch [355/496] loss 0.2661 (0.4904) acc 93.7500 (82.0423) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [360/496] loss 0.3579 (0.4892) acc 87.5000 (82.0660) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [365/496] loss 0.3750 (0.4899) acc 87.5000 (82.0377) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [370/496] loss 0.7368 (0.4890) acc 81.2500 (82.1115) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [375/496] loss 0.6211 (0.4896) acc 75.0000 (82.1000) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [380/496] loss 0.8032 (0.4904) acc 68.7500 (82.0395) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [385/496] loss 0.6274 (0.4897) acc 68.7500 (82.0292) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [390/496] loss 0.3577 (0.4909) acc 87.5000 (82.0673) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [395/496] loss 0.5293 (0.4918) acc 81.2500 (82.0728) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [400/496] loss 0.5703 (0.4938) acc 75.0000 (81.9688) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [405/496] loss 0.5156 (0.4924) acc 87.5000 (82.0525) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [410/496] loss 0.1969 (0.4919) acc 100.0000 (82.1037) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [415/496] loss 0.6191 (0.4923) acc 75.0000 (82.0783) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [420/496] loss 0.8452 (0.4930) acc 62.5000 (82.0536) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [425/496] loss 0.3325 (0.4916) acc 87.5000 (82.1029) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [430/496] loss 0.5830 (0.4911) acc 81.2500 (82.1657) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [435/496] loss 0.3032 (0.4903) acc 87.5000 (82.1983) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [440/496] loss 0.8721 (0.4915) acc 62.5000 (82.1449) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [445/496] loss 0.4758 (0.4915) acc 81.2500 (82.1770) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [450/496] loss 0.2573 (0.4907) acc 93.7500 (82.2222) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [455/496] loss 0.5620 (0.4907) acc 68.7500 (82.1703) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [460/496] loss 0.1848 (0.4899) acc 93.7500 (82.1875) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [465/496] loss 0.8843 (0.4896) acc 62.5000 (82.2043) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [470/496] loss 1.0527 (0.4908) acc 56.2500 (82.1410) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [475/496] loss 0.4526 (0.4903) acc 87.5000 (82.1447) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [480/496] loss 0.6465 (0.4888) acc 87.5000 (82.2396) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [485/496] loss 0.4678 (0.4888) acc 81.2500 (82.2552) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [490/496] loss 0.5908 (0.4894) acc 75.0000 (82.2321) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [495/496] loss 0.4121 (0.4891) acc 81.2500 (82.2475) lr 2.0000e-03 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting\model.pth.tar-2
Finish Training
Evaluate on the Test Set

  0%|          | 0/128 [00:00<?, ?it/s]
  1%|          | 1/128 [00:31<1:06:04, 31.22s/it]
  2%|��         | 2/128 [00:31<27:07, 12.91s/it]  
  5%|��         | 6/128 [00:31<06:03,  2.98s/it]
  8%|��         | 10/128 [00:31<02:49,  1.43s/it]
 12%|����        | 15/128 [00:31<01:24,  1.33it/s]
 16%|����        | 20/128 [00:31<00:49,  2.20it/s]
 20%|����        | 25/128 [00:31<00:30,  3.38it/s]
 23%|������       | 29/128 [00:31<00:21,  4.64it/s]
 27%|������       | 34/128 [00:32<00:14,  6.70it/s]
 30%|������       | 39/128 [00:32<00:09,  9.29it/s]
 34%|��������      | 44/128 [00:32<00:06, 12.33it/s]
 38%|��������      | 49/128 [00:32<00:04, 15.81it/s]
 42%|����������     | 54/128 [00:32<00:03, 19.54it/s]
 46%|����������     | 59/128 [00:32<00:02, 23.35it/s]
 50%|����������     | 64/128 [00:32<00:02, 26.96it/s]
 54%|������������    | 69/128 [00:32<00:01, 29.95it/s]
 58%|������������    | 74/128 [00:33<00:01, 32.72it/s]
 62%|��������������   | 79/128 [00:33<00:01, 34.83it/s]
 66%|��������������   | 84/128 [00:33<00:01, 36.53it/s]
 70%|��������������   | 89/128 [00:33<00:01, 37.61it/s]
 73%|����������������  | 94/128 [00:33<00:00, 38.63it/s]
 77%|����������������  | 99/128 [00:33<00:00, 39.38it/s]
 81%|������������������ | 104/128 [00:33<00:00, 39.59it/s]
 85%|������������������ | 109/128 [00:33<00:00, 39.74it/s]
 89%|������������������ | 114/128 [00:34<00:00, 39.98it/s]
 93%|��������������������| 119/128 [00:34<00:00, 40.31it/s]
 97%|��������������������| 124/128 [00:34<00:00, 40.75it/s]
100%|��������������������| 128/128 [00:35<00:00,  3.60it/s]
----------  ------
Total #     2,048
Correct #   1,977
Accuracy    96.53%
Error Rate  3.47%
Macro_F1    96.35%
----------  ------
python train.py --gpu 0 --seed 232 --output-dir output/CLIPAdapter-ViTB32-PACS-art_painting --dataset PACS --source-domains cartoon photo sketch --target-domains art_painting            --model CLIPAdapter --model-config-file config/clipadapter.yaml
E:\Python3\Lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 232
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 16
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 16
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 232
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/2] batch [5/496] loss 0.8213 (0.6122) acc 68.7500 (75.0000) lr 1.0000e-05 eta 1:44:26
epoch [1/2] batch [10/496] loss 0.7632 (0.6402) acc 81.2500 (78.1250) lr 1.0000e-05 eta 0:52:11
epoch [1/2] batch [15/496] loss 0.4141 (0.5856) acc 87.5000 (78.7500) lr 1.0000e-05 eta 0:34:45
epoch [1/2] batch [20/496] loss 0.2402 (0.5309) acc 93.7500 (80.0000) lr 1.0000e-05 eta 0:26:02
epoch [1/2] batch [25/496] loss 0.3569 (0.5145) acc 87.5000 (81.0000) lr 1.0000e-05 eta 0:20:48
epoch [1/2] batch [30/496] loss 0.5518 (0.5031) acc 81.2500 (81.6667) lr 1.0000e-05 eta 0:17:18
epoch [1/2] batch [35/496] loss 0.3423 (0.5163) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:14:49
epoch [1/2] batch [40/496] loss 0.6812 (0.5144) acc 75.0000 (80.7812) lr 1.0000e-05 eta 0:12:56
epoch [1/2] batch [45/496] loss 0.2219 (0.4964) acc 87.5000 (81.5278) lr 1.0000e-05 eta 0:11:29
epoch [1/2] batch [50/496] loss 0.5054 (0.5044) acc 75.0000 (81.2500) lr 1.0000e-05 eta 0:10:19
epoch [1/2] batch [55/496] loss 0.3701 (0.5062) acc 87.5000 (81.0227) lr 1.0000e-05 eta 0:09:22
epoch [1/2] batch [60/496] loss 0.5093 (0.5051) acc 75.0000 (81.0417) lr 1.0000e-05 eta 0:08:34
epoch [1/2] batch [65/496] loss 0.2201 (0.5087) acc 93.7500 (80.7692) lr 1.0000e-05 eta 0:07:54
epoch [1/2] batch [70/496] loss 1.1162 (0.5177) acc 62.5000 (80.7143) lr 1.0000e-05 eta 0:07:19
epoch [1/2] batch [75/496] loss 0.6167 (0.5151) acc 75.0000 (80.6667) lr 1.0000e-05 eta 0:06:49
epoch [1/2] batch [80/496] loss 0.9111 (0.5238) acc 62.5000 (80.3125) lr 1.0000e-05 eta 0:06:23
epoch [1/2] batch [85/496] loss 0.4055 (0.5138) acc 87.5000 (80.8088) lr 1.0000e-05 eta 0:06:00
epoch [1/2] batch [90/496] loss 0.2395 (0.5287) acc 93.7500 (80.4167) lr 1.0000e-05 eta 0:05:40
epoch [1/2] batch [95/496] loss 0.4353 (0.5259) acc 87.5000 (80.3289) lr 1.0000e-05 eta 0:05:21
epoch [1/2] batch [100/496] loss 0.3899 (0.5226) acc 87.5000 (80.4375) lr 1.0000e-05 eta 0:05:05
epoch [1/2] batch [105/496] loss 0.7168 (0.5244) acc 75.0000 (80.2976) lr 1.0000e-05 eta 0:04:49
epoch [1/2] batch [110/496] loss 0.7515 (0.5242) acc 75.0000 (80.3409) lr 1.0000e-05 eta 0:04:36
epoch [1/2] batch [115/496] loss 0.6128 (0.5192) acc 75.0000 (80.6522) lr 1.0000e-05 eta 0:04:23
epoch [1/2] batch [120/496] loss 0.3594 (0.5187) acc 87.5000 (80.6771) lr 1.0000e-05 eta 0:04:12
epoch [1/2] batch [125/496] loss 0.3438 (0.5197) acc 87.5000 (80.7000) lr 1.0000e-05 eta 0:04:01
epoch [1/2] batch [130/496] loss 0.4805 (0.5169) acc 75.0000 (80.7692) lr 1.0000e-05 eta 0:03:51
epoch [1/2] batch [135/496] loss 0.1604 (0.5172) acc 93.7500 (80.7407) lr 1.0000e-05 eta 0:03:42
epoch [1/2] batch [140/496] loss 0.6558 (0.5190) acc 75.0000 (80.5357) lr 1.0000e-05 eta 0:03:34
epoch [1/2] batch [145/496] loss 0.7666 (0.5191) acc 75.0000 (80.6034) lr 1.0000e-05 eta 0:03:26
epoch [1/2] batch [150/496] loss 0.4900 (0.5185) acc 87.5000 (80.6667) lr 1.0000e-05 eta 0:03:19
epoch [1/2] batch [155/496] loss 0.7744 (0.5233) acc 81.2500 (80.5645) lr 1.0000e-05 eta 0:03:12
epoch [1/2] batch [160/496] loss 0.4792 (0.5200) acc 81.2500 (80.5469) lr 1.0000e-05 eta 0:03:05
epoch [1/2] batch [165/496] loss 0.8989 (0.5242) acc 68.7500 (80.3409) lr 1.0000e-05 eta 0:02:59
epoch [1/2] batch [170/496] loss 0.4871 (0.5279) acc 81.2500 (80.2206) lr 1.0000e-05 eta 0:02:53
epoch [1/2] batch [175/496] loss 0.6982 (0.5270) acc 81.2500 (80.3214) lr 1.0000e-05 eta 0:02:48
epoch [1/2] batch [180/496] loss 0.4326 (0.5242) acc 75.0000 (80.2083) lr 1.0000e-05 eta 0:02:43
epoch [1/2] batch [185/496] loss 0.4282 (0.5230) acc 87.5000 (80.3716) lr 1.0000e-05 eta 0:02:38
epoch [1/2] batch [190/496] loss 0.7119 (0.5278) acc 75.0000 (80.1974) lr 1.0000e-05 eta 0:02:33
epoch [1/2] batch [195/496] loss 0.2896 (0.5234) acc 93.7500 (80.4487) lr 1.0000e-05 eta 0:02:29
epoch [1/2] batch [200/496] loss 0.6934 (0.5165) acc 68.7500 (80.6562) lr 1.0000e-05 eta 0:02:25
epoch [1/2] batch [205/496] loss 0.2214 (0.5153) acc 93.7500 (80.7317) lr 1.0000e-05 eta 0:02:21
epoch [1/2] batch [210/496] loss 0.7915 (0.5174) acc 87.5000 (80.7143) lr 1.0000e-05 eta 0:02:17
epoch [1/2] batch [215/496] loss 0.4932 (0.5196) acc 81.2500 (80.6395) lr 1.0000e-05 eta 0:02:14
epoch [1/2] batch [220/496] loss 0.5581 (0.5220) acc 87.5000 (80.6250) lr 1.0000e-05 eta 0:02:10
epoch [1/2] batch [225/496] loss 0.5073 (0.5215) acc 75.0000 (80.6111) lr 1.0000e-05 eta 0:02:07
epoch [1/2] batch [230/496] loss 0.6382 (0.5238) acc 81.2500 (80.4348) lr 1.0000e-05 eta 0:02:04
epoch [1/2] batch [235/496] loss 0.4121 (0.5233) acc 81.2500 (80.4521) lr 1.0000e-05 eta 0:02:01
epoch [1/2] batch [240/496] loss 0.9795 (0.5265) acc 75.0000 (80.3385) lr 1.0000e-05 eta 0:01:58
epoch [1/2] batch [245/496] loss 0.7100 (0.5277) acc 75.0000 (80.2551) lr 1.0000e-05 eta 0:01:55
epoch [1/2] batch [250/496] loss 0.4351 (0.5264) acc 81.2500 (80.2750) lr 1.0000e-05 eta 0:01:52
epoch [1/2] batch [255/496] loss 0.2273 (0.5243) acc 87.5000 (80.3431) lr 1.0000e-05 eta 0:01:50
epoch [1/2] batch [260/496] loss 0.1400 (0.5220) acc 100.0000 (80.4087) lr 1.0000e-05 eta 0:01:47
epoch [1/2] batch [265/496] loss 0.6621 (0.5222) acc 75.0000 (80.3538) lr 1.0000e-05 eta 0:01:45
epoch [1/2] batch [270/496] loss 0.6768 (0.5220) acc 68.7500 (80.3472) lr 1.0000e-05 eta 0:01:42
epoch [1/2] batch [275/496] loss 0.4546 (0.5219) acc 81.2500 (80.3182) lr 1.0000e-05 eta 0:01:40
epoch [1/2] batch [280/496] loss 0.6772 (0.5227) acc 81.2500 (80.3348) lr 1.0000e-05 eta 0:01:38
epoch [1/2] batch [285/496] loss 0.4929 (0.5203) acc 81.2500 (80.4167) lr 1.0000e-05 eta 0:01:36
epoch [1/2] batch [290/496] loss 0.6982 (0.5225) acc 62.5000 (80.3017) lr 1.0000e-05 eta 0:01:34
epoch [1/2] batch [295/496] loss 0.2002 (0.5232) acc 93.7500 (80.2966) lr 1.0000e-05 eta 0:01:32
epoch [1/2] batch [300/496] loss 0.5718 (0.5247) acc 75.0000 (80.2292) lr 1.0000e-05 eta 0:01:30
epoch [1/2] batch [305/496] loss 0.6758 (0.5245) acc 81.2500 (80.3074) lr 1.0000e-05 eta 0:01:28
epoch [1/2] batch [310/496] loss 0.4614 (0.5215) acc 81.2500 (80.4234) lr 1.0000e-05 eta 0:01:26
epoch [1/2] batch [315/496] loss 0.7568 (0.5199) acc 75.0000 (80.4762) lr 1.0000e-05 eta 0:01:25
epoch [1/2] batch [320/496] loss 1.0918 (0.5232) acc 75.0000 (80.3906) lr 1.0000e-05 eta 0:01:23
epoch [1/2] batch [325/496] loss 0.5063 (0.5218) acc 81.2500 (80.4808) lr 1.0000e-05 eta 0:01:21
epoch [1/2] batch [330/496] loss 0.3867 (0.5212) acc 87.5000 (80.4924) lr 1.0000e-05 eta 0:01:20
epoch [1/2] batch [335/496] loss 0.8076 (0.5210) acc 75.0000 (80.5224) lr 1.0000e-05 eta 0:01:18
epoch [1/2] batch [340/496] loss 0.2837 (0.5212) acc 87.5000 (80.5882) lr 1.0000e-05 eta 0:01:17
epoch [1/2] batch [345/496] loss 0.7168 (0.5209) acc 68.7500 (80.5978) lr 1.0000e-05 eta 0:01:15
epoch [1/2] batch [350/496] loss 0.4558 (0.5205) acc 75.0000 (80.5714) lr 1.0000e-05 eta 0:01:14
epoch [1/2] batch [355/496] loss 0.2920 (0.5186) acc 93.7500 (80.5986) lr 1.0000e-05 eta 0:01:12
epoch [1/2] batch [360/496] loss 0.5503 (0.5203) acc 81.2500 (80.5382) lr 1.0000e-05 eta 0:01:11
epoch [1/2] batch [365/496] loss 0.2118 (0.5172) acc 93.7500 (80.6678) lr 1.0000e-05 eta 0:01:10
epoch [1/2] batch [370/496] loss 0.4751 (0.5198) acc 81.2500 (80.5912) lr 1.0000e-05 eta 0:01:08
epoch [1/2] batch [375/496] loss 0.8423 (0.5193) acc 68.7500 (80.5833) lr 1.0000e-05 eta 0:01:07
epoch [1/2] batch [380/496] loss 0.6777 (0.5196) acc 81.2500 (80.5757) lr 1.0000e-05 eta 0:01:06
epoch [1/2] batch [385/496] loss 0.5532 (0.5189) acc 81.2500 (80.6331) lr 1.0000e-05 eta 0:01:05
epoch [1/2] batch [390/496] loss 0.8184 (0.5209) acc 68.7500 (80.5929) lr 1.0000e-05 eta 0:01:03
epoch [1/2] batch [395/496] loss 0.6675 (0.5213) acc 75.0000 (80.5696) lr 1.0000e-05 eta 0:01:02
epoch [1/2] batch [400/496] loss 0.4548 (0.5197) acc 87.5000 (80.6250) lr 1.0000e-05 eta 0:01:01
epoch [1/2] batch [405/496] loss 0.7627 (0.5195) acc 68.7500 (80.6327) lr 1.0000e-05 eta 0:01:00
epoch [1/2] batch [410/496] loss 0.1682 (0.5186) acc 100.0000 (80.6707) lr 1.0000e-05 eta 0:00:59
epoch [1/2] batch [415/496] loss 0.4927 (0.5169) acc 75.0000 (80.7078) lr 1.0000e-05 eta 0:00:58
epoch [1/2] batch [420/496] loss 0.1653 (0.5163) acc 93.7500 (80.7738) lr 1.0000e-05 eta 0:00:57
epoch [1/2] batch [425/496] loss 0.6772 (0.5183) acc 75.0000 (80.7059) lr 1.0000e-05 eta 0:00:56
epoch [1/2] batch [430/496] loss 0.2913 (0.5158) acc 87.5000 (80.7703) lr 1.0000e-05 eta 0:00:55
epoch [1/2] batch [435/496] loss 0.3420 (0.5147) acc 81.2500 (80.8046) lr 1.0000e-05 eta 0:00:54
epoch [1/2] batch [440/496] loss 0.3618 (0.5153) acc 87.5000 (80.7812) lr 1.0000e-05 eta 0:00:53
epoch [1/2] batch [445/496] loss 0.9097 (0.5159) acc 68.7500 (80.7865) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [450/496] loss 0.5757 (0.5166) acc 81.2500 (80.8194) lr 1.0000e-05 eta 0:00:51
epoch [1/2] batch [455/496] loss 0.3464 (0.5153) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:00:50
epoch [1/2] batch [460/496] loss 0.2229 (0.5139) acc 93.7500 (80.9511) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [465/496] loss 0.9756 (0.5143) acc 75.0000 (80.9677) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [470/496] loss 0.5835 (0.5149) acc 81.2500 (80.9309) lr 1.0000e-05 eta 0:00:48
epoch [1/2] batch [475/496] loss 0.5449 (0.5155) acc 68.7500 (80.8816) lr 1.0000e-05 eta 0:00:47
epoch [1/2] batch [480/496] loss 0.2568 (0.5148) acc 93.7500 (80.8984) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [485/496] loss 0.2527 (0.5135) acc 93.7500 (80.9665) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [490/496] loss 0.3733 (0.5127) acc 93.7500 (81.0459) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [495/496] loss 0.3975 (0.5121) acc 81.2500 (81.0732) lr 1.0000e-05 eta 0:00:44
epoch [2/2] batch [5/496] loss 0.6948 (0.4993) acc 75.0000 (81.2500) lr 2.0000e-03 eta 0:52:57
epoch [2/2] batch [10/496] loss 0.6460 (0.4968) acc 75.0000 (81.8750) lr 2.0000e-03 eta 0:26:18
epoch [2/2] batch [15/496] loss 0.4055 (0.4875) acc 93.7500 (82.0833) lr 2.0000e-03 eta 0:17:25
epoch [2/2] batch [20/496] loss 0.9385 (0.5377) acc 81.2500 (80.9375) lr 2.0000e-03 eta 0:12:58
epoch [2/2] batch [25/496] loss 0.6040 (0.5711) acc 75.0000 (80.0000) lr 2.0000e-03 eta 0:10:18
epoch [2/2] batch [30/496] loss 0.6353 (0.5750) acc 68.7500 (78.9583) lr 2.0000e-03 eta 0:08:32
epoch [2/2] batch [35/496] loss 0.8198 (0.5820) acc 68.7500 (78.7500) lr 2.0000e-03 eta 0:07:15
epoch [2/2] batch [40/496] loss 0.3450 (0.5779) acc 81.2500 (78.7500) lr 2.0000e-03 eta 0:06:18
epoch [2/2] batch [45/496] loss 0.4246 (0.5635) acc 81.2500 (79.3056) lr 2.0000e-03 eta 0:05:34
epoch [2/2] batch [50/496] loss 0.4258 (0.5635) acc 81.2500 (79.2500) lr 2.0000e-03 eta 0:04:58
epoch [2/2] batch [55/496] loss 0.4568 (0.5562) acc 81.2500 (79.7727) lr 2.0000e-03 eta 0:04:29
epoch [2/2] batch [60/496] loss 0.4265 (0.5509) acc 87.5000 (80.2083) lr 2.0000e-03 eta 0:04:05
epoch [2/2] batch [65/496] loss 0.4177 (0.5381) acc 81.2500 (80.6731) lr 2.0000e-03 eta 0:03:44
epoch [2/2] batch [70/496] loss 0.3430 (0.5269) acc 87.5000 (81.0714) lr 2.0000e-03 eta 0:03:26
epoch [2/2] batch [75/496] loss 0.4424 (0.5243) acc 87.5000 (81.0833) lr 2.0000e-03 eta 0:03:11
epoch [2/2] batch [80/496] loss 0.4409 (0.5158) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:02:57
epoch [2/2] batch [85/496] loss 0.3394 (0.5197) acc 81.2500 (80.9559) lr 2.0000e-03 eta 0:02:46
epoch [2/2] batch [90/496] loss 0.3467 (0.5164) acc 81.2500 (81.1111) lr 2.0000e-03 eta 0:02:35
epoch [2/2] batch [95/496] loss 0.3823 (0.5140) acc 87.5000 (81.0526) lr 2.0000e-03 eta 0:02:26
epoch [2/2] batch [100/496] loss 0.7505 (0.5096) acc 68.7500 (81.1250) lr 2.0000e-03 eta 0:02:17
epoch [2/2] batch [105/496] loss 0.4780 (0.5135) acc 87.5000 (80.9524) lr 2.0000e-03 eta 0:02:09
epoch [2/2] batch [110/496] loss 0.3667 (0.5135) acc 93.7500 (80.9091) lr 2.0000e-03 eta 0:02:02
epoch [2/2] batch [115/496] loss 0.3093 (0.5074) acc 93.7500 (81.2500) lr 2.0000e-03 eta 0:01:56
epoch [2/2] batch [120/496] loss 0.5820 (0.5015) acc 87.5000 (81.5625) lr 2.0000e-03 eta 0:01:50
epoch [2/2] batch [125/496] loss 0.5928 (0.5012) acc 68.7500 (81.5500) lr 2.0000e-03 eta 0:01:45
epoch [2/2] batch [130/496] loss 0.5278 (0.5081) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:01:39
epoch [2/2] batch [135/496] loss 0.4385 (0.5053) acc 87.5000 (81.2963) lr 2.0000e-03 eta 0:01:35
epoch [2/2] batch [140/496] loss 0.4873 (0.5044) acc 87.5000 (81.4286) lr 2.0000e-03 eta 0:01:30
epoch [2/2] batch [145/496] loss 0.3271 (0.5010) acc 87.5000 (81.5517) lr 2.0000e-03 eta 0:01:26
epoch [2/2] batch [150/496] loss 0.6738 (0.5007) acc 68.7500 (81.6250) lr 2.0000e-03 eta 0:01:23
epoch [2/2] batch [155/496] loss 1.0498 (0.5035) acc 68.7500 (81.6935) lr 2.0000e-03 eta 0:01:19
epoch [2/2] batch [160/496] loss 0.3633 (0.5013) acc 87.5000 (81.7188) lr 2.0000e-03 eta 0:01:16
epoch [2/2] batch [165/496] loss 0.5444 (0.5007) acc 75.0000 (81.7424) lr 2.0000e-03 eta 0:01:13
epoch [2/2] batch [170/496] loss 0.6313 (0.5000) acc 75.0000 (81.7279) lr 2.0000e-03 eta 0:01:10
epoch [2/2] batch [175/496] loss 0.5483 (0.4963) acc 81.2500 (81.8571) lr 2.0000e-03 eta 0:01:07
epoch [2/2] batch [180/496] loss 0.3323 (0.4980) acc 87.5000 (81.7014) lr 2.0000e-03 eta 0:01:04
epoch [2/2] batch [185/496] loss 0.3635 (0.4996) acc 87.5000 (81.5878) lr 2.0000e-03 eta 0:01:02
epoch [2/2] batch [190/496] loss 0.3821 (0.5007) acc 93.7500 (81.5461) lr 2.0000e-03 eta 0:00:59
epoch [2/2] batch [195/496] loss 0.6851 (0.5016) acc 68.7500 (81.5064) lr 2.0000e-03 eta 0:00:57
epoch [2/2] batch [200/496] loss 0.7871 (0.5010) acc 62.5000 (81.5625) lr 2.0000e-03 eta 0:00:55
epoch [2/2] batch [205/496] loss 0.5220 (0.4994) acc 75.0000 (81.6159) lr 2.0000e-03 eta 0:00:53
epoch [2/2] batch [210/496] loss 0.3809 (0.5000) acc 81.2500 (81.6071) lr 2.0000e-03 eta 0:00:51
epoch [2/2] batch [215/496] loss 0.6562 (0.5015) acc 81.2500 (81.5988) lr 2.0000e-03 eta 0:00:49
epoch [2/2] batch [220/496] loss 0.3442 (0.4996) acc 87.5000 (81.6477) lr 2.0000e-03 eta 0:00:47
epoch [2/2] batch [225/496] loss 0.7026 (0.4981) acc 75.0000 (81.7500) lr 2.0000e-03 eta 0:00:45
epoch [2/2] batch [230/496] loss 0.8765 (0.4990) acc 75.0000 (81.6848) lr 2.0000e-03 eta 0:00:44
epoch [2/2] batch [235/496] loss 0.6738 (0.5015) acc 62.5000 (81.5160) lr 2.0000e-03 eta 0:00:42
epoch [2/2] batch [240/496] loss 0.3279 (0.5024) acc 87.5000 (81.4844) lr 2.0000e-03 eta 0:00:40
epoch [2/2] batch [245/496] loss 0.2866 (0.5014) acc 87.5000 (81.5561) lr 2.0000e-03 eta 0:00:39
epoch [2/2] batch [250/496] loss 0.7437 (0.5053) acc 68.7500 (81.3500) lr 2.0000e-03 eta 0:00:37
epoch [2/2] batch [255/496] loss 0.3025 (0.5053) acc 87.5000 (81.3971) lr 2.0000e-03 eta 0:00:36
epoch [2/2] batch [260/496] loss 0.4946 (0.5050) acc 81.2500 (81.4183) lr 2.0000e-03 eta 0:00:35
epoch [2/2] batch [265/496] loss 0.4985 (0.5043) acc 81.2500 (81.4151) lr 2.0000e-03 eta 0:00:33
epoch [2/2] batch [270/496] loss 0.8931 (0.5075) acc 56.2500 (81.3194) lr 2.0000e-03 eta 0:00:32
epoch [2/2] batch [275/496] loss 0.1860 (0.5052) acc 93.7500 (81.4318) lr 2.0000e-03 eta 0:00:31
epoch [2/2] batch [280/496] loss 0.0760 (0.5019) acc 100.0000 (81.5402) lr 2.0000e-03 eta 0:00:30
epoch [2/2] batch [285/496] loss 0.2323 (0.5015) acc 93.7500 (81.5132) lr 2.0000e-03 eta 0:00:29
epoch [2/2] batch [290/496] loss 0.1888 (0.5002) acc 93.7500 (81.4655) lr 2.0000e-03 eta 0:00:28
epoch [2/2] batch [295/496] loss 0.4951 (0.4999) acc 87.5000 (81.5254) lr 2.0000e-03 eta 0:00:27
epoch [2/2] batch [300/496] loss 0.3164 (0.4993) acc 87.5000 (81.5625) lr 2.0000e-03 eta 0:00:26
epoch [2/2] batch [305/496] loss 0.4407 (0.4984) acc 81.2500 (81.5984) lr 2.0000e-03 eta 0:00:25
epoch [2/2] batch [310/496] loss 0.8633 (0.4987) acc 68.7500 (81.5726) lr 2.0000e-03 eta 0:00:24
epoch [2/2] batch [315/496] loss 0.3872 (0.4972) acc 87.5000 (81.6270) lr 2.0000e-03 eta 0:00:23
epoch [2/2] batch [320/496] loss 0.4773 (0.4958) acc 75.0000 (81.6602) lr 2.0000e-03 eta 0:00:22
epoch [2/2] batch [325/496] loss 0.2581 (0.4932) acc 87.5000 (81.7692) lr 2.0000e-03 eta 0:00:21
epoch [2/2] batch [330/496] loss 0.2598 (0.4922) acc 93.7500 (81.8371) lr 2.0000e-03 eta 0:00:20
epoch [2/2] batch [335/496] loss 0.6050 (0.4919) acc 75.0000 (81.8470) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [340/496] loss 0.4207 (0.4918) acc 87.5000 (81.8750) lr 2.0000e-03 eta 0:00:18
epoch [2/2] batch [345/496] loss 0.8643 (0.4915) acc 75.0000 (81.9022) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [350/496] loss 0.1931 (0.4900) acc 93.7500 (81.9821) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [355/496] loss 0.1318 (0.4898) acc 100.0000 (81.9718) lr 2.0000e-03 eta 0:00:16
epoch [2/2] batch [360/496] loss 0.1904 (0.4884) acc 93.7500 (82.0660) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [365/496] loss 0.3538 (0.4884) acc 93.7500 (82.0719) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [370/496] loss 0.2361 (0.4863) acc 100.0000 (82.1959) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [375/496] loss 0.3188 (0.4847) acc 81.2500 (82.2333) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [380/496] loss 0.7529 (0.4853) acc 75.0000 (82.2204) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [385/496] loss 0.5415 (0.4843) acc 81.2500 (82.2565) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [390/496] loss 0.4734 (0.4856) acc 87.5000 (82.2115) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [395/496] loss 0.2056 (0.4840) acc 87.5000 (82.2627) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [400/496] loss 0.4368 (0.4846) acc 81.2500 (82.2031) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [405/496] loss 0.4285 (0.4829) acc 81.2500 (82.2840) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [410/496] loss 0.5161 (0.4828) acc 75.0000 (82.2866) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [415/496] loss 0.4226 (0.4841) acc 81.2500 (82.2289) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [420/496] loss 0.2837 (0.4839) acc 87.5000 (82.2024) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [425/496] loss 0.2448 (0.4837) acc 93.7500 (82.2353) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [430/496] loss 0.1436 (0.4819) acc 93.7500 (82.2820) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [435/496] loss 0.4478 (0.4811) acc 93.7500 (82.3276) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [440/496] loss 0.5239 (0.4808) acc 81.2500 (82.3153) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [445/496] loss 0.4888 (0.4831) acc 81.2500 (82.1910) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [450/496] loss 0.7612 (0.4834) acc 81.2500 (82.2083) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [455/496] loss 0.5825 (0.4818) acc 81.2500 (82.2527) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [460/496] loss 0.8496 (0.4827) acc 68.7500 (82.2418) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [465/496] loss 0.7031 (0.4835) acc 68.7500 (82.1909) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [470/496] loss 0.3701 (0.4821) acc 87.5000 (82.2473) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [475/496] loss 0.3279 (0.4803) acc 87.5000 (82.3684) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [480/496] loss 0.5342 (0.4809) acc 81.2500 (82.3307) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [485/496] loss 0.4663 (0.4808) acc 93.7500 (82.3454) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [490/496] loss 0.5278 (0.4806) acc 81.2500 (82.3469) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [495/496] loss 0.5293 (0.4809) acc 81.2500 (82.3232) lr 2.0000e-03 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting\model.pth.tar-2
Finish Training
Evaluate on the Test Set

  0%|          | 0/128 [00:00<?, ?it/s]
  1%|          | 1/128 [00:31<1:06:09, 31.25s/it]
  4%|��         | 5/128 [00:31<09:35,  4.68s/it]  
  7%|��         | 9/128 [00:31<04:12,  2.12s/it]
 11%|��         | 14/128 [00:31<02:03,  1.08s/it]
 15%|����        | 19/128 [00:31<01:10,  1.55it/s]
 19%|����        | 24/128 [00:31<00:43,  2.42it/s]
 23%|������       | 29/128 [00:31<00:27,  3.59it/s]
 27%|������       | 34/128 [00:32<00:18,  5.16it/s]
 30%|������       | 39/128 [00:32<00:12,  7.19it/s]
 34%|��������      | 44/128 [00:32<00:08,  9.73it/s]
 38%|��������      | 49/128 [00:32<00:06, 12.73it/s]
 42%|����������     | 54/128 [00:32<00:04, 16.13it/s]
 46%|����������     | 59/128 [00:32<00:03, 19.77it/s]
 50%|����������     | 64/128 [00:32<00:02, 23.44it/s]
 54%|������������    | 69/128 [00:32<00:02, 26.96it/s]
 58%|������������    | 74/128 [00:33<00:01, 30.23it/s]
 62%|��������������   | 79/128 [00:33<00:01, 32.98it/s]
 66%|��������������   | 84/128 [00:33<00:01, 35.18it/s]
 70%|��������������   | 89/128 [00:33<00:01, 36.75it/s]
 73%|����������������  | 94/128 [00:33<00:00, 37.87it/s]
 77%|����������������  | 99/128 [00:33<00:00, 38.62it/s]
 81%|������������������ | 104/128 [00:33<00:00, 39.39it/s]
 85%|������������������ | 109/128 [00:33<00:00, 39.91it/s]
 89%|������������������ | 114/128 [00:33<00:00, 40.37it/s]
 93%|��������������������| 119/128 [00:34<00:00, 40.85it/s]
 97%|��������������������| 124/128 [00:34<00:00, 41.33it/s]
100%|��������������������| 128/128 [00:35<00:00,  3.62it/s]
----------  ------
Total #     2,048
Correct #   1,978
Accuracy    96.58%
Error Rate  3.42%
Macro_F1    96.40%
----------  ------
python train.py --gpu 0 --seed 607 --output-dir output/CLIPAdapter-ViTB32-PACS-art_painting --dataset PACS --source-domains cartoon photo sketch --target-domains art_painting            --model CLIPAdapter --model-config-file config/clipadapter.yaml
E:\Python3\Lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)