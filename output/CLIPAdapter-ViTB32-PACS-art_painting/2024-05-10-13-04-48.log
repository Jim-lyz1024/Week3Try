*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 1
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'art_painting']
target_domains: ['sketch']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 64
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'art_painting']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['sketch']
GPU: 1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 1
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'art_painting']
Target Domains  ['sketch']
# Classes       7
# Train Data    6,062
# Val Data      616
# Test Data     3,928
--------------  ------------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
{'original': ['a picture of a dog.', 'a picture of a elephant.', 'a picture of a giraffe.', 'a picture of a guitar.', 'a picture of a horse.', 'a picture of a house.', 'a picture of a person.'], 'cartoon': ['a picture of a cartoon dog.', 'a picture of a cartoon elephant.', 'a picture of a cartoon giraffe.', 'a picture of a cartoon guitar.', 'a picture of a cartoon horse.', 'a picture of a cartoon house.', 'a picture of a cartoon person.'], 'photo': ['a picture of a photo dog.', 'a picture of a photo elephant.', 'a picture of a photo giraffe.', 'a picture of a photo guitar.', 'a picture of a photo horse.', 'a picture of a photo house.', 'a picture of a photo person.'], 'art_painting': ['a picture of a art painting dog.', 'a picture of a art painting elephant.', 'a picture of a art painting giraffe.', 'a picture of a art painting guitar.', 'a picture of a art painting horse.', 'a picture of a art painting house.', 'a picture of a art painting person.']}
{'original': ['a picture of a dog.', 'a picture of a elephant.', 'a picture of a giraffe.', 'a picture of a guitar.', 'a picture of a horse.', 'a picture of a house.', 'a picture of a person.'], 'sketch': ['a picture of a sketch dog.', 'a picture of a sketch elephant.', 'a picture of a sketch giraffe.', 'a picture of a sketch guitar.', 'a picture of a sketch horse.', 'a picture of a sketch house.', 'a picture of a sketch person.']}
key:original
v:tensor([[ 0.0130,  0.0088, -0.0393,  ..., -0.0427, -0.0132,  0.0196],
        [ 0.0052,  0.0047, -0.0051,  ..., -0.0147,  0.0049,  0.0211],
        [ 0.0317, -0.0135, -0.0394,  ..., -0.0197, -0.0197, -0.0249],
        ...,
        [ 0.0157, -0.0074, -0.0182,  ..., -0.0357, -0.0072,  0.0031],
        [ 0.0064,  0.0098, -0.0112,  ..., -0.0517, -0.0120, -0.0190],
        [ 0.0049,  0.0079, -0.0012,  ..., -0.0592, -0.0150, -0.0023]],
       device='cuda:1', dtype=torch.float16)
key:cartoon
v:tensor([[ 0.0121,  0.0077, -0.0332,  ..., -0.0228, -0.0125,  0.0219],
        [-0.0028,  0.0028, -0.0092,  ..., -0.0011,  0.0078,  0.0374],
        [ 0.0187, -0.0066, -0.0338,  ..., -0.0048, -0.0202, -0.0085],
        ...,
        [ 0.0010,  0.0093, -0.0051,  ..., -0.0157, -0.0022,  0.0104],
        [-0.0029,  0.0059,  0.0048,  ..., -0.0195, -0.0060, -0.0013],
        [-0.0036,  0.0185, -0.0177,  ..., -0.0428, -0.0072, -0.0013]],
       device='cuda:1', dtype=torch.float16)
key:photo
v:tensor([[ 0.0228,  0.0416, -0.0377,  ..., -0.0820, -0.0252,  0.0203],
        [ 0.0073,  0.0297, -0.0122,  ..., -0.0445, -0.0020,  0.0318],
        [ 0.0240,  0.0198, -0.0415,  ..., -0.0452, -0.0294, -0.0078],
        ...,
        [ 0.0157,  0.0311, -0.0157,  ..., -0.0651, -0.0243,  0.0078],
        [-0.0037,  0.0155, -0.0159,  ..., -0.0697, -0.0306, -0.0125],
        [ 0.0129,  0.0236, -0.0192,  ..., -0.0815, -0.0204, -0.0023]],
       device='cuda:1', dtype=torch.float16)
key:art_painting
v:tensor([[ 0.0347, -0.0056, -0.0271,  ..., -0.0314, -0.0006, -0.0017],
        [ 0.0241, -0.0112, -0.0081,  ...,  0.0004,  0.0161,  0.0137],
        [ 0.0443, -0.0284, -0.0266,  ..., -0.0102, -0.0107, -0.0368],
        ...,
        [ 0.0285, -0.0029, -0.0031,  ..., -0.0209, -0.0044, -0.0133],
        [ 0.0181,  0.0093, -0.0034,  ..., -0.0334, -0.0261, -0.0240],
        [ 0.0221, -0.0028,  0.0012,  ..., -0.0517, -0.0204, -0.0327]],
       device='cuda:1', dtype=torch.float16)
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapters.2.fc.0.weight', 'adapters.1.fc.0.weight', 'adapters.0.fc.0.weight', 'adapters.0.fc.2.weight', 'adapters.2.fc.2.weight', 'adapter.fc.0.weight', 'adapters.1.fc.2.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/1] batch [5/94] loss -12.2734 (-9.8812) acc 21.8750 (20.6250) lr 1.0000e-05 eta 0:00:29
epoch [1/1] batch [10/94] loss -10.8594 (-10.0645) acc 21.8750 (23.1250) lr 1.0000e-05 eta 0:00:18
epoch [1/1] batch [15/94] loss -6.0078 (-10.4477) acc 31.2500 (23.2292) lr 1.0000e-05 eta 0:00:14
epoch [1/1] batch [20/94] loss -11.2031 (-10.6057) acc 34.3750 (23.9844) lr 1.0000e-05 eta 0:00:11
epoch [1/1] batch [25/94] loss -5.9727 (-10.6719) acc 20.3125 (23.4375) lr 1.0000e-05 eta 0:00:10
epoch [1/1] batch [30/94] loss -9.2188 (-10.3582) acc 20.3125 (23.5417) lr 1.0000e-05 eta 0:00:09
epoch [1/1] batch [35/94] loss -9.6953 (-10.5213) acc 26.5625 (22.9911) lr 1.0000e-05 eta 0:00:08
epoch [1/1] batch [40/94] loss -11.7578 (-10.2422) acc 23.4375 (22.8516) lr 1.0000e-05 eta 0:00:07
epoch [1/1] batch [45/94] loss -6.3906 (-9.9475) acc 31.2500 (23.0556) lr 1.0000e-05 eta 0:00:06
epoch [1/1] batch [50/94] loss -7.8203 (-9.9016) acc 20.3125 (22.9688) lr 1.0000e-05 eta 0:00:05
epoch [1/1] batch [55/94] loss -13.7031 (-10.0978) acc 32.8125 (23.2386) lr 1.0000e-05 eta 0:00:04
epoch [1/1] batch [60/94] loss -7.6367 (-10.1401) acc 15.6250 (23.0990) lr 1.0000e-05 eta 0:00:04
epoch [1/1] batch [65/94] loss -9.7656 (-10.0867) acc 26.5625 (23.2212) lr 1.0000e-05 eta 0:00:03
epoch [1/1] batch [70/94] loss -8.6250 (-10.1161) acc 21.8750 (23.3705) lr 1.0000e-05 eta 0:00:02
epoch [1/1] batch [75/94] loss -14.1094 (-10.2109) acc 20.3125 (23.3125) lr 1.0000e-05 eta 0:00:02
epoch [1/1] batch [80/94] loss -12.7734 (-10.1549) acc 17.1875 (22.9688) lr 1.0000e-05 eta 0:00:01
epoch [1/1] batch [85/94] loss -9.6797 (-10.1802) acc 20.3125 (22.9044) lr 1.0000e-05 eta 0:00:01
epoch [1/1] batch [90/94] loss -13.3906 (-10.1784) acc 12.5000 (22.6215) lr 1.0000e-05 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting/model.pth.tar-1
Finish Training
Evaluate on the Test Set
Model output: torch.Size([64, 28])
