*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 64
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 5
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Class:  ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']
Domains:  ['art_painting', 'cartoon', 'photo', 'sketch']
Prompts: ['a picture of a art_painting dog.', 'a picture of a art_painting elephant.', 'a picture of a art_painting giraffe.', 'a picture of a art_painting guitar.', 'a picture of a art_painting horse.', 'a picture of a art_painting house.', 'a picture of a art_painting person.', 'a picture of a cartoon dog.', 'a picture of a cartoon elephant.', 'a picture of a cartoon giraffe.', 'a picture of a cartoon guitar.', 'a picture of a cartoon horse.', 'a picture of a cartoon house.', 'a picture of a cartoon person.', 'a picture of a photo dog.', 'a picture of a photo elephant.', 'a picture of a photo giraffe.', 'a picture of a photo guitar.', 'a picture of a photo horse.', 'a picture of a photo house.', 'a picture of a photo person.', 'a picture of a sketch dog.', 'a picture of a sketch elephant.', 'a picture of a sketch giraffe.', 'a picture of a sketch guitar.', 'a picture of a sketch horse.', 'a picture of a sketch house.', 'a picture of a sketch person.']
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/5] batch [5/124] loss 3.5605 (3.3809) acc 3.1250 (4.3750) lr 1.0000e-05 eta 0:04:08
epoch [1/5] batch [10/124] loss 3.3027 (3.3689) acc 3.1250 (4.0625) lr 1.0000e-05 eta 0:02:10
epoch [1/5] batch [15/124] loss 3.5078 (3.3488) acc 7.8125 (4.6875) lr 1.0000e-05 eta 0:01:30
epoch [1/5] batch [20/124] loss 2.9121 (3.3289) acc 1.5625 (5.0000) lr 1.0000e-05 eta 0:01:10
epoch [1/5] batch [25/124] loss 3.3652 (3.3305) acc 1.5625 (4.8750) lr 1.0000e-05 eta 0:00:58
epoch [1/5] batch [30/124] loss 3.1543 (3.3364) acc 15.6250 (5.4688) lr 1.0000e-05 eta 0:00:50
epoch [1/5] batch [35/124] loss 3.1270 (3.3356) acc 9.3750 (5.5804) lr 1.0000e-05 eta 0:00:45
epoch [1/5] batch [40/124] loss 3.3750 (3.3329) acc 6.2500 (5.4688) lr 1.0000e-05 eta 0:00:41
epoch [1/5] batch [45/124] loss 3.2324 (3.3269) acc 3.1250 (5.5208) lr 1.0000e-05 eta 0:00:38
epoch [1/5] batch [50/124] loss 3.1309 (3.3230) acc 3.1250 (5.2500) lr 1.0000e-05 eta 0:00:35
epoch [1/5] batch [55/124] loss 3.0879 (3.3250) acc 7.8125 (5.3693) lr 1.0000e-05 eta 0:00:33
epoch [1/5] batch [60/124] loss 2.9570 (3.3202) acc 14.0625 (5.5729) lr 1.0000e-05 eta 0:00:31
epoch [1/5] batch [65/124] loss 3.1738 (3.3267) acc 4.6875 (5.6731) lr 1.0000e-05 eta 0:00:30
epoch [1/5] batch [70/124] loss 3.3242 (3.3354) acc 6.2500 (5.5804) lr 1.0000e-05 eta 0:00:29
epoch [1/5] batch [75/124] loss 3.1719 (3.3347) acc 3.1250 (5.5833) lr 1.0000e-05 eta 0:00:28
epoch [1/5] batch [80/124] loss 3.2598 (3.3344) acc 4.6875 (5.5469) lr 1.0000e-05 eta 0:00:26
epoch [1/5] batch [85/124] loss 3.2637 (3.3300) acc 3.1250 (5.4412) lr 1.0000e-05 eta 0:00:26
epoch [1/5] batch [90/124] loss 2.9941 (3.3299) acc 12.5000 (5.4167) lr 1.0000e-05 eta 0:00:25
epoch [1/5] batch [95/124] loss 3.0566 (3.3291) acc 4.6875 (5.4112) lr 1.0000e-05 eta 0:00:24
epoch [1/5] batch [100/124] loss 3.1328 (3.3205) acc 6.2500 (5.5469) lr 1.0000e-05 eta 0:00:23
epoch [1/5] batch [105/124] loss 3.1035 (3.3169) acc 9.3750 (5.6399) lr 1.0000e-05 eta 0:00:23
epoch [1/5] batch [110/124] loss 3.4746 (3.3238) acc 6.2500 (5.6392) lr 1.0000e-05 eta 0:00:22
epoch [1/5] batch [115/124] loss 3.3262 (3.3233) acc 6.2500 (5.6386) lr 1.0000e-05 eta 0:00:21
epoch [1/5] batch [120/124] loss 3.3262 (3.3192) acc 6.2500 (5.6380) lr 1.0000e-05 eta 0:00:21
epoch [2/5] batch [5/124] loss 2.9609 (3.2414) acc 9.3750 (6.8750) lr 2.0000e-03 eta 0:01:12
epoch [2/5] batch [10/124] loss 3.2441 (3.1230) acc 6.2500 (6.4062) lr 2.0000e-03 eta 0:00:44
epoch [2/5] batch [15/124] loss 2.4902 (2.9177) acc 15.6250 (8.1250) lr 2.0000e-03 eta 0:00:33
epoch [2/5] batch [20/124] loss 1.8721 (2.7341) acc 29.6875 (11.4062) lr 2.0000e-03 eta 0:00:27
epoch [2/5] batch [25/124] loss 1.1777 (2.4623) acc 57.8125 (18.8750) lr 2.0000e-03 eta 0:00:24
epoch [2/5] batch [30/124] loss 0.8086 (2.2312) acc 78.1250 (26.3021) lr 2.0000e-03 eta 0:00:21
epoch [2/5] batch [35/124] loss 0.8643 (2.0129) acc 71.8750 (33.4375) lr 2.0000e-03 eta 0:00:20
epoch [2/5] batch [40/124] loss 0.4954 (1.8271) acc 84.3750 (39.6875) lr 2.0000e-03 eta 0:00:19
epoch [2/5] batch [45/124] loss 0.4587 (1.6840) acc 85.9375 (44.3750) lr 2.0000e-03 eta 0:00:18
epoch [2/5] batch [50/124] loss 0.4670 (1.5773) acc 84.3750 (47.9062) lr 2.0000e-03 eta 0:00:17
epoch [2/5] batch [55/124] loss 0.5732 (1.4799) acc 82.8125 (51.0795) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [60/124] loss 0.5698 (1.4017) acc 81.2500 (53.5417) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [65/124] loss 0.4016 (1.3357) acc 85.9375 (55.4567) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [70/124] loss 0.6494 (1.2814) acc 81.2500 (57.2321) lr 2.0000e-03 eta 0:00:15
epoch [2/5] batch [75/124] loss 0.6226 (1.2302) acc 73.4375 (58.8333) lr 2.0000e-03 eta 0:00:15
epoch [2/5] batch [80/124] loss 0.4851 (1.1878) acc 85.9375 (60.1367) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [85/124] loss 0.5801 (1.1503) acc 78.1250 (61.3051) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [90/124] loss 0.2817 (1.1100) acc 92.1875 (62.7431) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [95/124] loss 0.3992 (1.0788) acc 85.9375 (63.6842) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [100/124] loss 0.3555 (1.0499) acc 89.0625 (64.5938) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [105/124] loss 0.4995 (1.0242) acc 79.6875 (65.3869) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [110/124] loss 0.3347 (0.9971) acc 90.6250 (66.2784) lr 2.0000e-03 eta 0:00:12
epoch [2/5] batch [115/124] loss 0.5225 (0.9752) acc 84.3750 (66.9701) lr 2.0000e-03 eta 0:00:12
epoch [2/5] batch [120/124] loss 0.5225 (0.9567) acc 85.9375 (67.5911) lr 2.0000e-03 eta 0:00:12
epoch [3/5] batch [5/124] loss 0.2993 (0.5338) acc 92.1875 (82.1875) lr 1.8090e-03 eta 0:00:50
epoch [3/5] batch [10/124] loss 0.3850 (0.4805) acc 89.0625 (83.2812) lr 1.8090e-03 eta 0:00:30
epoch [3/5] batch [15/124] loss 0.5576 (0.4851) acc 81.2500 (82.8125) lr 1.8090e-03 eta 0:00:22
epoch [3/5] batch [20/124] loss 0.4463 (0.4712) acc 81.2500 (83.5156) lr 1.8090e-03 eta 0:00:19
epoch [3/5] batch [25/124] loss 0.4963 (0.4904) acc 85.9375 (82.5000) lr 1.8090e-03 eta 0:00:17
epoch [3/5] batch [30/124] loss 0.3921 (0.4808) acc 89.0625 (82.8125) lr 1.8090e-03 eta 0:00:16
epoch [3/5] batch [35/124] loss 0.5874 (0.4890) acc 82.8125 (82.5446) lr 1.8090e-03 eta 0:00:15
epoch [3/5] batch [40/124] loss 0.5225 (0.4865) acc 79.6875 (82.6172) lr 1.8090e-03 eta 0:00:14
epoch [3/5] batch [45/124] loss 0.5352 (0.4888) acc 79.6875 (82.6389) lr 1.8090e-03 eta 0:00:13
epoch [3/5] batch [50/124] loss 0.4121 (0.4908) acc 84.3750 (82.6562) lr 1.8090e-03 eta 0:00:12
epoch [3/5] batch [55/124] loss 0.7104 (0.4942) acc 68.7500 (82.4432) lr 1.8090e-03 eta 0:00:12
epoch [3/5] batch [60/124] loss 0.5107 (0.4927) acc 81.2500 (82.5000) lr 1.8090e-03 eta 0:00:11
epoch [3/5] batch [65/124] loss 0.4094 (0.4936) acc 85.9375 (82.5721) lr 1.8090e-03 eta 0:00:11
epoch [3/5] batch [70/124] loss 0.3975 (0.4911) acc 82.8125 (82.6339) lr 1.8090e-03 eta 0:00:11
epoch [3/5] batch [75/124] loss 0.4451 (0.4869) acc 85.9375 (82.8542) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [80/124] loss 0.6504 (0.4861) acc 79.6875 (82.9102) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [85/124] loss 0.4822 (0.4897) acc 84.3750 (82.8676) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [90/124] loss 0.4946 (0.4859) acc 79.6875 (83.0556) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [95/124] loss 0.4707 (0.4853) acc 79.6875 (82.9934) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [100/124] loss 0.3364 (0.4840) acc 89.0625 (83.1094) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [105/124] loss 0.4451 (0.4862) acc 84.3750 (82.9762) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [110/124] loss 0.6997 (0.4899) acc 75.0000 (82.7841) lr 1.8090e-03 eta 0:00:08
epoch [3/5] batch [115/124] loss 0.4900 (0.4902) acc 84.3750 (82.7989) lr 1.8090e-03 eta 0:00:08
epoch [3/5] batch [120/124] loss 0.4937 (0.4905) acc 82.8125 (82.7865) lr 1.8090e-03 eta 0:00:08
epoch [4/5] batch [5/124] loss 0.5034 (0.5543) acc 79.6875 (80.9375) lr 1.3090e-03 eta 0:00:36
epoch [4/5] batch [10/124] loss 0.3970 (0.4933) acc 90.6250 (82.8125) lr 1.3090e-03 eta 0:00:20
epoch [4/5] batch [15/124] loss 0.4907 (0.4807) acc 81.2500 (83.7500) lr 1.3090e-03 eta 0:00:15
epoch [4/5] batch [20/124] loss 0.5708 (0.4952) acc 76.5625 (83.0469) lr 1.3090e-03 eta 0:00:12
epoch [4/5] batch [25/124] loss 0.3857 (0.4898) acc 85.9375 (83.1875) lr 1.3090e-03 eta 0:00:11
epoch [4/5] batch [30/124] loss 0.4321 (0.4901) acc 81.2500 (83.1250) lr 1.3090e-03 eta 0:00:10
epoch [4/5] batch [35/124] loss 0.5195 (0.4850) acc 78.1250 (83.1250) lr 1.3090e-03 eta 0:00:09
epoch [4/5] batch [40/124] loss 0.4912 (0.4955) acc 81.2500 (82.5391) lr 1.3090e-03 eta 0:00:08
epoch [4/5] batch [45/124] loss 0.5186 (0.5024) acc 79.6875 (82.3264) lr 1.3090e-03 eta 0:00:08
epoch [4/5] batch [50/124] loss 0.5391 (0.5000) acc 82.8125 (82.5625) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [55/124] loss 0.6079 (0.5027) acc 78.1250 (82.3864) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [60/124] loss 0.7383 (0.5032) acc 71.8750 (82.2917) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [65/124] loss 0.5645 (0.5094) acc 82.8125 (82.1635) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [70/124] loss 0.4707 (0.5044) acc 82.8125 (82.4107) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [75/124] loss 0.4827 (0.5049) acc 78.1250 (82.2708) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [80/124] loss 0.5425 (0.5026) acc 76.5625 (82.2852) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [85/124] loss 0.5630 (0.5054) acc 78.1250 (82.0772) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [90/124] loss 0.6470 (0.5044) acc 73.4375 (82.0833) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [95/124] loss 0.3813 (0.5038) acc 90.6250 (82.2039) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [100/124] loss 0.4380 (0.5002) acc 79.6875 (82.3125) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [105/124] loss 0.5654 (0.5037) acc 81.2500 (82.2470) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [110/124] loss 0.5098 (0.5051) acc 84.3750 (82.1733) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [115/124] loss 0.5034 (0.5050) acc 78.1250 (82.1332) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [120/124] loss 0.5176 (0.5026) acc 82.8125 (82.3177) lr 1.3090e-03 eta 0:00:04
epoch [5/5] batch [5/124] loss 0.5347 (0.5550) acc 78.1250 (78.4375) lr 6.9098e-04 eta 0:00:15
