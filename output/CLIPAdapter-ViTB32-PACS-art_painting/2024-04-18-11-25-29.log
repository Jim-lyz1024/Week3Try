*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 5
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Class: ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']
Domains: ['cartoon', 'photo', 'sketch']
{'original': ['a picture of a dog.', 'a picture of a elephant.', 'a picture of a giraffe.', 'a picture of a guitar.', 'a picture of a horse.', 'a picture of a house.', 'a picture of a person.'], 'cartoon': ['a picture of a cartoon dog.', 'a picture of a cartoon elephant.', 'a picture of a cartoon giraffe.', 'a picture of a cartoon guitar.', 'a picture of a cartoon horse.', 'a picture of a cartoon house.', 'a picture of a cartoon person.'], 'photo': ['a picture of a photo dog.', 'a picture of a photo elephant.', 'a picture of a photo giraffe.', 'a picture of a photo guitar.', 'a picture of a photo horse.', 'a picture of a photo house.', 'a picture of a photo person.'], 'sketch': ['a picture of a sketch dog.', 'a picture of a sketch elephant.', 'a picture of a sketch giraffe.', 'a picture of a sketch guitar.', 'a picture of a sketch horse.', 'a picture of a sketch house.', 'a picture of a sketch person.']}
Encoded prompts (cartoon): torch.Size([7, 512])
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
Logits domain: {'original': tensor([[18.4844, 19.3750, 18.0000, 19.3750, 20.3281, 24.7656, 21.1562],
        [28.8125, 28.5781, 27.7812, 23.2969, 30.6875, 21.5000, 26.4219],
        [20.7031, 19.5938, 20.7812, 20.6250, 19.9688, 22.0312, 24.1875],
        [26.8750, 22.4688, 22.7188, 21.3906, 23.6719, 20.7188, 23.8125],
        [20.4531, 27.5156, 21.3281, 17.6094, 20.5156, 18.1875, 22.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>), 'cartoon': tensor([[19.1406, 19.9062, 18.8438, 18.3438, 20.1094, 28.1562, 22.7656],
        [30.1250, 30.9531, 30.4531, 24.7188, 32.2812, 24.6250, 27.6562],
        [21.7969, 22.5000, 23.7812, 23.5938, 21.7656, 24.9531, 25.2188],
        [27.2969, 24.2812, 24.2500, 21.3594, 24.8281, 22.3125, 24.3906],
        [22.5938, 30.8906, 24.7500, 21.3750, 23.5156, 24.2969, 26.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>), 'photo': tensor([[16.3906, 17.9531, 16.2812, 15.7812, 18.2969, 22.6562, 20.7812],
        [21.6094, 23.0938, 22.8125, 17.4375, 25.4375, 18.8438, 21.3281],
        [18.2188, 18.9219, 19.8281, 18.8438, 18.5000, 22.5000, 22.6406],
        [21.9688, 17.9062, 18.2969, 17.4375, 19.5000, 18.9219, 20.9219],
        [16.1562, 26.0312, 19.2500, 14.5234, 17.9688, 18.3438, 20.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>), 'sketch': tensor([[20.2188, 18.6250, 17.8906, 18.0625, 18.9062, 23.5469, 19.6094],
        [31.1719, 30.6719, 30.4531, 25.4844, 32.1875, 22.1406, 27.8281],
        [19.7031, 18.1562, 19.7500, 19.6562, 18.3281, 22.0312, 22.6094],
        [27.5938, 23.2031, 23.7031, 22.2031, 24.0625, 20.8906, 24.6094],
        [20.9688, 26.7812, 21.0625, 17.5625, 19.3594, 17.7344, 21.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)}
