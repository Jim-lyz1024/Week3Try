*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 64
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 5
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Prompts: ['a picture of a dog.', 'a picture of a elephant.', 'a picture of a giraffe.', 'a picture of a guitar.', 'a picture of a horse.', 'a picture of a house.', 'a picture of a person.']
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/5] batch [5/124] loss 0.4036 (0.5212) acc 85.9375 (79.6875) lr 1.0000e-05 eta 0:04:02
epoch [1/5] batch [10/124] loss 0.6704 (0.5519) acc 75.0000 (77.9688) lr 1.0000e-05 eta 0:02:07
epoch [1/5] batch [15/124] loss 0.6733 (0.5189) acc 73.4375 (79.6875) lr 1.0000e-05 eta 0:01:28
epoch [1/5] batch [20/124] loss 0.3191 (0.5085) acc 87.5000 (80.2344) lr 1.0000e-05 eta 0:01:09
epoch [1/5] batch [25/124] loss 0.4871 (0.5166) acc 84.3750 (80.1875) lr 1.0000e-05 eta 0:00:57
epoch [1/5] batch [30/124] loss 0.5317 (0.5094) acc 79.6875 (80.7292) lr 1.0000e-05 eta 0:00:49
epoch [1/5] batch [35/124] loss 0.4578 (0.5048) acc 82.8125 (81.0714) lr 1.0000e-05 eta 0:00:44
epoch [1/5] batch [40/124] loss 0.4602 (0.4978) acc 75.0000 (81.2109) lr 1.0000e-05 eta 0:00:40
epoch [1/5] batch [45/124] loss 0.4055 (0.4906) acc 84.3750 (81.3889) lr 1.0000e-05 eta 0:00:37
epoch [1/5] batch [50/124] loss 0.4717 (0.4928) acc 84.3750 (81.5312) lr 1.0000e-05 eta 0:00:35
epoch [1/5] batch [55/124] loss 0.5859 (0.5033) acc 78.1250 (81.1932) lr 1.0000e-05 eta 0:00:33
epoch [1/5] batch [60/124] loss 0.2444 (0.4956) acc 93.7500 (81.5104) lr 1.0000e-05 eta 0:00:31
epoch [1/5] batch [65/124] loss 0.5103 (0.5039) acc 82.8125 (81.1779) lr 1.0000e-05 eta 0:00:29
epoch [1/5] batch [70/124] loss 0.7134 (0.5131) acc 76.5625 (80.8036) lr 1.0000e-05 eta 0:00:28
epoch [1/5] batch [75/124] loss 0.4763 (0.5145) acc 84.3750 (80.7708) lr 1.0000e-05 eta 0:00:27
epoch [1/5] batch [80/124] loss 0.5015 (0.5157) acc 82.8125 (80.7227) lr 1.0000e-05 eta 0:00:26
epoch [1/5] batch [85/124] loss 0.4333 (0.5152) acc 85.9375 (80.7721) lr 1.0000e-05 eta 0:00:25
epoch [1/5] batch [90/124] loss 0.3352 (0.5149) acc 87.5000 (80.7465) lr 1.0000e-05 eta 0:00:25
epoch [1/5] batch [95/124] loss 0.3838 (0.5127) acc 87.5000 (80.8388) lr 1.0000e-05 eta 0:00:24
epoch [1/5] batch [100/124] loss 0.5493 (0.5145) acc 75.0000 (80.7344) lr 1.0000e-05 eta 0:00:23
epoch [1/5] batch [105/124] loss 0.5674 (0.5161) acc 81.2500 (80.6845) lr 1.0000e-05 eta 0:00:23
epoch [1/5] batch [110/124] loss 0.4775 (0.5165) acc 84.3750 (80.7244) lr 1.0000e-05 eta 0:00:22
epoch [1/5] batch [115/124] loss 0.5420 (0.5171) acc 81.2500 (80.7880) lr 1.0000e-05 eta 0:00:21
epoch [1/5] batch [120/124] loss 0.6768 (0.5133) acc 71.8750 (80.8724) lr 1.0000e-05 eta 0:00:21
epoch [2/5] batch [5/124] loss 0.4729 (0.5643) acc 79.6875 (78.4375) lr 2.0000e-03 eta 0:01:15
epoch [2/5] batch [10/124] loss 0.6006 (0.5144) acc 84.3750 (81.5625) lr 2.0000e-03 eta 0:00:43
epoch [2/5] batch [15/124] loss 0.5312 (0.5182) acc 82.8125 (81.1458) lr 2.0000e-03 eta 0:00:32
epoch [2/5] batch [20/124] loss 0.5493 (0.5113) acc 82.8125 (81.4844) lr 2.0000e-03 eta 0:00:27
epoch [2/5] batch [25/124] loss 0.4873 (0.5076) acc 79.6875 (81.3125) lr 2.0000e-03 eta 0:00:24
epoch [2/5] batch [30/124] loss 0.4910 (0.5146) acc 84.3750 (81.4062) lr 2.0000e-03 eta 0:00:22
epoch [2/5] batch [35/124] loss 0.6553 (0.5158) acc 76.5625 (81.2500) lr 2.0000e-03 eta 0:00:20
epoch [2/5] batch [40/124] loss 0.3633 (0.5025) acc 84.3750 (81.5625) lr 2.0000e-03 eta 0:00:19
epoch [2/5] batch [45/124] loss 0.4993 (0.5026) acc 81.2500 (81.7014) lr 2.0000e-03 eta 0:00:18
epoch [2/5] batch [50/124] loss 0.4497 (0.5109) acc 78.1250 (81.3125) lr 2.0000e-03 eta 0:00:17
epoch [2/5] batch [55/124] loss 0.5679 (0.5090) acc 76.5625 (81.4773) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [60/124] loss 0.5586 (0.5098) acc 75.0000 (81.2500) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [65/124] loss 0.3962 (0.5119) acc 85.9375 (81.0577) lr 2.0000e-03 eta 0:00:16
epoch [2/5] batch [70/124] loss 0.6128 (0.5158) acc 82.8125 (81.0268) lr 2.0000e-03 eta 0:00:15
epoch [2/5] batch [75/124] loss 0.6328 (0.5172) acc 73.4375 (80.8750) lr 2.0000e-03 eta 0:00:15
epoch [2/5] batch [80/124] loss 0.4756 (0.5191) acc 84.3750 (80.8203) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [85/124] loss 0.5591 (0.5206) acc 82.8125 (80.7721) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [90/124] loss 0.2534 (0.5141) acc 90.6250 (81.1111) lr 2.0000e-03 eta 0:00:14
epoch [2/5] batch [95/124] loss 0.4070 (0.5143) acc 84.3750 (81.1184) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [100/124] loss 0.3809 (0.5138) acc 85.9375 (81.1406) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [105/124] loss 0.5151 (0.5145) acc 82.8125 (81.1161) lr 2.0000e-03 eta 0:00:13
epoch [2/5] batch [110/124] loss 0.3157 (0.5116) acc 89.0625 (81.1790) lr 2.0000e-03 eta 0:00:12
epoch [2/5] batch [115/124] loss 0.5190 (0.5104) acc 81.2500 (81.2228) lr 2.0000e-03 eta 0:00:12
epoch [2/5] batch [120/124] loss 0.5210 (0.5111) acc 85.9375 (81.2240) lr 2.0000e-03 eta 0:00:12
epoch [3/5] batch [5/124] loss 0.3062 (0.5236) acc 89.0625 (80.9375) lr 1.8090e-03 eta 0:00:49
epoch [3/5] batch [10/124] loss 0.3835 (0.4734) acc 87.5000 (82.8125) lr 1.8090e-03 eta 0:00:30
epoch [3/5] batch [15/124] loss 0.5391 (0.4750) acc 78.1250 (82.5000) lr 1.8090e-03 eta 0:00:22
epoch [3/5] batch [20/124] loss 0.3948 (0.4631) acc 84.3750 (82.8906) lr 1.8090e-03 eta 0:00:19
epoch [3/5] batch [25/124] loss 0.5371 (0.4812) acc 84.3750 (82.0625) lr 1.8090e-03 eta 0:00:17
epoch [3/5] batch [30/124] loss 0.3794 (0.4720) acc 89.0625 (82.8125) lr 1.8090e-03 eta 0:00:15
epoch [3/5] batch [35/124] loss 0.6152 (0.4779) acc 84.3750 (82.9464) lr 1.8090e-03 eta 0:00:14
epoch [3/5] batch [40/124] loss 0.5449 (0.4775) acc 79.6875 (82.7344) lr 1.8090e-03 eta 0:00:13
epoch [3/5] batch [45/124] loss 0.5352 (0.4804) acc 79.6875 (82.7083) lr 1.8090e-03 eta 0:00:13
epoch [3/5] batch [50/124] loss 0.3872 (0.4835) acc 85.9375 (82.6562) lr 1.8090e-03 eta 0:00:12
epoch [3/5] batch [55/124] loss 0.7676 (0.4869) acc 67.1875 (82.5000) lr 1.8090e-03 eta 0:00:12
epoch [3/5] batch [60/124] loss 0.4744 (0.4868) acc 79.6875 (82.3958) lr 1.8090e-03 eta 0:00:11
epoch [3/5] batch [65/124] loss 0.4133 (0.4876) acc 89.0625 (82.4519) lr 1.8090e-03 eta 0:00:11
epoch [3/5] batch [70/124] loss 0.3875 (0.4855) acc 85.9375 (82.5223) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [75/124] loss 0.4534 (0.4815) acc 84.3750 (82.6667) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [80/124] loss 0.6963 (0.4801) acc 71.8750 (82.6562) lr 1.8090e-03 eta 0:00:10
epoch [3/5] batch [85/124] loss 0.4849 (0.4834) acc 81.2500 (82.3529) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [90/124] loss 0.4875 (0.4795) acc 78.1250 (82.5174) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [95/124] loss 0.4458 (0.4777) acc 81.2500 (82.4178) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [100/124] loss 0.3555 (0.4769) acc 85.9375 (82.4531) lr 1.8090e-03 eta 0:00:09
epoch [3/5] batch [105/124] loss 0.4885 (0.4797) acc 79.6875 (82.2917) lr 1.8090e-03 eta 0:00:08
epoch [3/5] batch [110/124] loss 0.7036 (0.4829) acc 73.4375 (82.1733) lr 1.8090e-03 eta 0:00:08
epoch [3/5] batch [115/124] loss 0.4712 (0.4827) acc 90.6250 (82.2690) lr 1.8090e-03 eta 0:00:08
epoch [3/5] batch [120/124] loss 0.4656 (0.4827) acc 84.3750 (82.3047) lr 1.8090e-03 eta 0:00:08
epoch [4/5] batch [5/124] loss 0.4968 (0.5400) acc 78.1250 (81.2500) lr 1.3090e-03 eta 0:00:32
epoch [4/5] batch [10/124] loss 0.4082 (0.4842) acc 90.6250 (82.9688) lr 1.3090e-03 eta 0:00:18
epoch [4/5] batch [15/124] loss 0.4495 (0.4683) acc 78.1250 (83.8542) lr 1.3090e-03 eta 0:00:14
epoch [4/5] batch [20/124] loss 0.5850 (0.4865) acc 76.5625 (83.1250) lr 1.3090e-03 eta 0:00:12
epoch [4/5] batch [25/124] loss 0.3855 (0.4807) acc 85.9375 (82.8750) lr 1.3090e-03 eta 0:00:11
epoch [4/5] batch [30/124] loss 0.4517 (0.4789) acc 81.2500 (82.9167) lr 1.3090e-03 eta 0:00:09
epoch [4/5] batch [35/124] loss 0.5630 (0.4764) acc 79.6875 (82.9018) lr 1.3090e-03 eta 0:00:09
epoch [4/5] batch [40/124] loss 0.4836 (0.4871) acc 84.3750 (82.4219) lr 1.3090e-03 eta 0:00:08
epoch [4/5] batch [45/124] loss 0.5161 (0.4942) acc 76.5625 (81.9792) lr 1.3090e-03 eta 0:00:08
epoch [4/5] batch [50/124] loss 0.5513 (0.4911) acc 79.6875 (82.0938) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [55/124] loss 0.5840 (0.4941) acc 76.5625 (81.9886) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [60/124] loss 0.7617 (0.4951) acc 71.8750 (81.9010) lr 1.3090e-03 eta 0:00:07
epoch [4/5] batch [65/124] loss 0.5435 (0.5017) acc 81.2500 (81.6587) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [70/124] loss 0.4219 (0.4957) acc 85.9375 (82.0089) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [75/124] loss 0.5308 (0.4962) acc 79.6875 (81.9167) lr 1.3090e-03 eta 0:00:06
epoch [4/5] batch [80/124] loss 0.5688 (0.4949) acc 76.5625 (81.9336) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [85/124] loss 0.5586 (0.4982) acc 76.5625 (81.7831) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [90/124] loss 0.6602 (0.4973) acc 71.8750 (81.7708) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [95/124] loss 0.3582 (0.4971) acc 90.6250 (81.8421) lr 1.3090e-03 eta 0:00:05
epoch [4/5] batch [100/124] loss 0.3958 (0.4934) acc 81.2500 (82.0625) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [105/124] loss 0.5635 (0.4973) acc 79.6875 (81.9643) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [110/124] loss 0.4912 (0.4986) acc 87.5000 (81.9602) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [115/124] loss 0.4934 (0.4978) acc 78.1250 (81.9293) lr 1.3090e-03 eta 0:00:04
epoch [4/5] batch [120/124] loss 0.5220 (0.4950) acc 81.2500 (82.0964) lr 1.3090e-03 eta 0:00:04
epoch [5/5] batch [5/124] loss 0.5381 (0.5512) acc 75.0000 (77.8125) lr 6.9098e-04 eta 0:00:16
epoch [5/5] batch [10/124] loss 0.4109 (0.5117) acc 85.9375 (80.7812) lr 6.9098e-04 eta 0:00:09
epoch [5/5] batch [15/124] loss 0.6226 (0.5183) acc 78.1250 (80.9375) lr 6.9098e-04 eta 0:00:06
epoch [5/5] batch [20/124] loss 0.5005 (0.5095) acc 85.9375 (81.4062) lr 6.9098e-04 eta 0:00:05
epoch [5/5] batch [25/124] loss 0.4482 (0.5197) acc 85.9375 (81.1250) lr 6.9098e-04 eta 0:00:04
epoch [5/5] batch [30/124] loss 0.4988 (0.5116) acc 79.6875 (81.3021) lr 6.9098e-04 eta 0:00:04
epoch [5/5] batch [35/124] loss 0.4026 (0.4986) acc 87.5000 (81.6964) lr 6.9098e-04 eta 0:00:03
epoch [5/5] batch [40/124] loss 0.4604 (0.4990) acc 84.3750 (81.7969) lr 6.9098e-04 eta 0:00:03
epoch [5/5] batch [45/124] loss 0.6909 (0.5047) acc 71.8750 (81.3889) lr 6.9098e-04 eta 0:00:03
epoch [5/5] batch [50/124] loss 0.2280 (0.4946) acc 93.7500 (81.8438) lr 6.9098e-04 eta 0:00:02
epoch [5/5] batch [55/124] loss 0.4255 (0.4951) acc 89.0625 (81.8750) lr 6.9098e-04 eta 0:00:02
epoch [5/5] batch [60/124] loss 0.5127 (0.4869) acc 84.3750 (82.1875) lr 6.9098e-04 eta 0:00:02
epoch [5/5] batch [65/124] loss 0.3406 (0.4835) acc 85.9375 (82.2115) lr 6.9098e-04 eta 0:00:02
epoch [5/5] batch [70/124] loss 0.3333 (0.4793) acc 90.6250 (82.3884) lr 6.9098e-04 eta 0:00:01
epoch [5/5] batch [75/124] loss 0.4272 (0.4773) acc 87.5000 (82.5208) lr 6.9098e-04 eta 0:00:01
epoch [5/5] batch [80/124] loss 0.5957 (0.4780) acc 79.6875 (82.5391) lr 6.9098e-04 eta 0:00:01
epoch [5/5] batch [85/124] loss 0.2720 (0.4724) acc 92.1875 (82.7022) lr 6.9098e-04 eta 0:00:01
epoch [5/5] batch [90/124] loss 0.4329 (0.4724) acc 81.2500 (82.5347) lr 6.9098e-04 eta 0:00:01
epoch [5/5] batch [95/124] loss 0.4668 (0.4698) acc 82.8125 (82.7303) lr 6.9098e-04 eta 0:00:00
epoch [5/5] batch [100/124] loss 0.3667 (0.4675) acc 87.5000 (82.7812) lr 6.9098e-04 eta 0:00:00
epoch [5/5] batch [105/124] loss 0.5752 (0.4678) acc 81.2500 (82.8869) lr 6.9098e-04 eta 0:00:00
epoch [5/5] batch [110/124] loss 0.4788 (0.4703) acc 84.3750 (82.8409) lr 6.9098e-04 eta 0:00:00
epoch [5/5] batch [115/124] loss 0.5059 (0.4702) acc 81.2500 (82.8533) lr 6.9098e-04 eta 0:00:00
epoch [5/5] batch [120/124] loss 0.6436 (0.4723) acc 75.0000 (82.7604) lr 6.9098e-04 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting/model.pth.tar-5
Finish Training
Evaluate on the Test Set
----------  ------
Total #     2,048
Correct #   1,978
Accuracy    96.58%
Error Rate  3.42%
Macro_F1    96.42%
----------  ------
