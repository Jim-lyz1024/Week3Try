*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 134
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 16
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 16
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/2] batch [5/496] loss 0.4683 (0.6346) acc 87.5000 (71.2500) lr 1.0000e-05 eta 1:43:15
epoch [1/2] batch [10/496] loss 0.4248 (0.5270) acc 81.2500 (78.1250) lr 1.0000e-05 eta 0:51:34
epoch [1/2] batch [15/496] loss 0.4028 (0.5240) acc 87.5000 (79.1667) lr 1.0000e-05 eta 0:34:20
epoch [1/2] batch [20/496] loss 0.5249 (0.4970) acc 87.5000 (80.9375) lr 1.0000e-05 eta 0:25:43
epoch [1/2] batch [25/496] loss 1.0264 (0.5256) acc 62.5000 (80.7500) lr 1.0000e-05 eta 0:20:33
epoch [1/2] batch [30/496] loss 0.4788 (0.5153) acc 75.0000 (80.0000) lr 1.0000e-05 eta 0:17:06
epoch [1/2] batch [35/496] loss 0.4885 (0.5278) acc 87.5000 (80.0000) lr 1.0000e-05 eta 0:14:38
epoch [1/2] batch [40/496] loss 0.4827 (0.5271) acc 81.2500 (80.4688) lr 1.0000e-05 eta 0:12:47
epoch [1/2] batch [45/496] loss 0.3999 (0.5136) acc 81.2500 (80.5556) lr 1.0000e-05 eta 0:11:21
epoch [1/2] batch [50/496] loss 0.4204 (0.5201) acc 87.5000 (80.3750) lr 1.0000e-05 eta 0:10:12
epoch [1/2] batch [55/496] loss 0.5317 (0.5150) acc 81.2500 (80.9091) lr 1.0000e-05 eta 0:09:15
epoch [1/2] batch [60/496] loss 0.5620 (0.5243) acc 75.0000 (80.5208) lr 1.0000e-05 eta 0:08:28
epoch [1/2] batch [65/496] loss 0.2091 (0.5084) acc 87.5000 (81.1538) lr 1.0000e-05 eta 0:07:48
epoch [1/2] batch [70/496] loss 0.2737 (0.5153) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:07:14
epoch [1/2] batch [75/496] loss 0.3711 (0.5170) acc 81.2500 (80.8333) lr 1.0000e-05 eta 0:06:44
epoch [1/2] batch [80/496] loss 0.4368 (0.5191) acc 93.7500 (80.7812) lr 1.0000e-05 eta 0:06:18
epoch [1/2] batch [85/496] loss 0.3149 (0.5101) acc 87.5000 (81.2500) lr 1.0000e-05 eta 0:05:56
epoch [1/2] batch [90/496] loss 0.4827 (0.5132) acc 87.5000 (81.3889) lr 1.0000e-05 eta 0:05:35
epoch [1/2] batch [95/496] loss 0.3757 (0.5112) acc 81.2500 (81.3816) lr 1.0000e-05 eta 0:05:17
epoch [1/2] batch [100/496] loss 0.9341 (0.5102) acc 62.5000 (81.4375) lr 1.0000e-05 eta 0:05:00
epoch [1/2] batch [105/496] loss 0.8706 (0.5116) acc 68.7500 (81.3095) lr 1.0000e-05 eta 0:04:46
epoch [1/2] batch [110/496] loss 0.3406 (0.5143) acc 87.5000 (81.1932) lr 1.0000e-05 eta 0:04:32
epoch [1/2] batch [115/496] loss 0.8452 (0.5139) acc 56.2500 (81.1957) lr 1.0000e-05 eta 0:04:20
epoch [1/2] batch [120/496] loss 0.9966 (0.5106) acc 62.5000 (81.3021) lr 1.0000e-05 eta 0:04:08
epoch [1/2] batch [125/496] loss 0.7119 (0.5089) acc 68.7500 (81.2500) lr 1.0000e-05 eta 0:03:58
epoch [1/2] batch [130/496] loss 0.4902 (0.5055) acc 87.5000 (81.4423) lr 1.0000e-05 eta 0:03:48
epoch [1/2] batch [135/496] loss 0.5820 (0.4973) acc 81.2500 (81.7130) lr 1.0000e-05 eta 0:03:39
epoch [1/2] batch [140/496] loss 0.3337 (0.5027) acc 93.7500 (81.6071) lr 1.0000e-05 eta 0:03:31
epoch [1/2] batch [145/496] loss 1.0703 (0.5062) acc 62.5000 (81.5086) lr 1.0000e-05 eta 0:03:23
epoch [1/2] batch [150/496] loss 0.2856 (0.5105) acc 87.5000 (81.2500) lr 1.0000e-05 eta 0:03:16
epoch [1/2] batch [155/496] loss 0.7925 (0.5100) acc 75.0000 (81.3710) lr 1.0000e-05 eta 0:03:09
epoch [1/2] batch [160/496] loss 0.5972 (0.5085) acc 62.5000 (81.3281) lr 1.0000e-05 eta 0:03:03
epoch [1/2] batch [165/496] loss 0.2262 (0.5064) acc 87.5000 (81.4015) lr 1.0000e-05 eta 0:02:57
epoch [1/2] batch [170/496] loss 0.3745 (0.5059) acc 81.2500 (81.3603) lr 1.0000e-05 eta 0:02:51
epoch [1/2] batch [175/496] loss 0.4436 (0.5063) acc 81.2500 (81.2500) lr 1.0000e-05 eta 0:02:46
epoch [1/2] batch [180/496] loss 0.4744 (0.5069) acc 87.5000 (81.1806) lr 1.0000e-05 eta 0:02:41
epoch [1/2] batch [185/496] loss 0.3574 (0.5065) acc 93.7500 (81.2500) lr 1.0000e-05 eta 0:02:36
epoch [1/2] batch [190/496] loss 0.7339 (0.5078) acc 75.0000 (81.2500) lr 1.0000e-05 eta 0:02:31
epoch [1/2] batch [195/496] loss 0.5264 (0.5083) acc 75.0000 (81.2179) lr 1.0000e-05 eta 0:02:27
epoch [1/2] batch [200/496] loss 0.3630 (0.5075) acc 87.5000 (81.1875) lr 1.0000e-05 eta 0:02:23
epoch [1/2] batch [205/496] loss 0.5405 (0.5131) acc 81.2500 (81.0671) lr 1.0000e-05 eta 0:02:19
epoch [1/2] batch [210/496] loss 0.8325 (0.5126) acc 68.7500 (81.1310) lr 1.0000e-05 eta 0:02:15
epoch [1/2] batch [215/496] loss 0.3196 (0.5146) acc 87.5000 (81.0756) lr 1.0000e-05 eta 0:02:12
epoch [1/2] batch [220/496] loss 0.4353 (0.5136) acc 81.2500 (81.0795) lr 1.0000e-05 eta 0:02:08
epoch [1/2] batch [225/496] loss 0.6890 (0.5139) acc 75.0000 (81.1389) lr 1.0000e-05 eta 0:02:05
epoch [1/2] batch [230/496] loss 0.4790 (0.5139) acc 75.0000 (81.0598) lr 1.0000e-05 eta 0:02:02
epoch [1/2] batch [235/496] loss 0.5161 (0.5133) acc 81.2500 (81.0638) lr 1.0000e-05 eta 0:01:59
epoch [1/2] batch [240/496] loss 0.4817 (0.5110) acc 81.2500 (81.1198) lr 1.0000e-05 eta 0:01:56
epoch [1/2] batch [245/496] loss 0.2563 (0.5115) acc 93.7500 (81.1224) lr 1.0000e-05 eta 0:01:53
epoch [1/2] batch [250/496] loss 0.5225 (0.5115) acc 81.2500 (81.1250) lr 1.0000e-05 eta 0:01:51
epoch [1/2] batch [255/496] loss 0.4946 (0.5139) acc 81.2500 (81.0049) lr 1.0000e-05 eta 0:01:48
epoch [1/2] batch [260/496] loss 0.5601 (0.5157) acc 75.0000 (80.9375) lr 1.0000e-05 eta 0:01:46
epoch [1/2] batch [265/496] loss 0.4487 (0.5172) acc 81.2500 (80.8726) lr 1.0000e-05 eta 0:01:43
epoch [1/2] batch [270/496] loss 0.7017 (0.5199) acc 68.7500 (80.7870) lr 1.0000e-05 eta 0:01:41
epoch [1/2] batch [275/496] loss 0.6304 (0.5219) acc 81.2500 (80.7500) lr 1.0000e-05 eta 0:01:39
epoch [1/2] batch [280/496] loss 0.4702 (0.5230) acc 87.5000 (80.7589) lr 1.0000e-05 eta 0:01:37
epoch [1/2] batch [285/496] loss 0.6870 (0.5277) acc 75.0000 (80.5702) lr 1.0000e-05 eta 0:01:35
epoch [1/2] batch [290/496] loss 0.7529 (0.5298) acc 68.7500 (80.5172) lr 1.0000e-05 eta 0:01:33
epoch [1/2] batch [295/496] loss 0.3928 (0.5312) acc 81.2500 (80.4449) lr 1.0000e-05 eta 0:01:31
epoch [1/2] batch [300/496] loss 0.3730 (0.5299) acc 81.2500 (80.4375) lr 1.0000e-05 eta 0:01:29
epoch [1/2] batch [305/496] loss 0.5767 (0.5307) acc 75.0000 (80.3893) lr 1.0000e-05 eta 0:01:27
epoch [1/2] batch [310/496] loss 0.7324 (0.5328) acc 75.0000 (80.3226) lr 1.0000e-05 eta 0:01:25
epoch [1/2] batch [315/496] loss 0.4775 (0.5355) acc 81.2500 (80.1984) lr 1.0000e-05 eta 0:01:23
epoch [1/2] batch [320/496] loss 0.3252 (0.5343) acc 87.5000 (80.2148) lr 1.0000e-05 eta 0:01:22
epoch [1/2] batch [325/496] loss 0.5400 (0.5334) acc 75.0000 (80.2115) lr 1.0000e-05 eta 0:01:20
epoch [1/2] batch [330/496] loss 0.8945 (0.5348) acc 56.2500 (80.1326) lr 1.0000e-05 eta 0:01:19
epoch [1/2] batch [335/496] loss 0.7607 (0.5345) acc 75.0000 (80.0933) lr 1.0000e-05 eta 0:01:17
epoch [1/2] batch [340/496] loss 0.3750 (0.5328) acc 87.5000 (80.1654) lr 1.0000e-05 eta 0:01:16
epoch [1/2] batch [345/496] loss 0.7271 (0.5308) acc 81.2500 (80.2536) lr 1.0000e-05 eta 0:01:14
epoch [1/2] batch [350/496] loss 0.4819 (0.5308) acc 81.2500 (80.2321) lr 1.0000e-05 eta 0:01:13
epoch [1/2] batch [355/496] loss 0.9370 (0.5340) acc 68.7500 (80.0880) lr 1.0000e-05 eta 0:01:11
epoch [1/2] batch [360/496] loss 0.4204 (0.5335) acc 81.2500 (80.1042) lr 1.0000e-05 eta 0:01:10
epoch [1/2] batch [365/496] loss 0.0657 (0.5322) acc 100.0000 (80.1541) lr 1.0000e-05 eta 0:01:09
epoch [1/2] batch [370/496] loss 0.3916 (0.5306) acc 87.5000 (80.2027) lr 1.0000e-05 eta 0:01:07
epoch [1/2] batch [375/496] loss 0.5391 (0.5306) acc 75.0000 (80.1833) lr 1.0000e-05 eta 0:01:06
epoch [1/2] batch [380/496] loss 0.3250 (0.5296) acc 87.5000 (80.2138) lr 1.0000e-05 eta 0:01:05
epoch [1/2] batch [385/496] loss 0.5615 (0.5310) acc 81.2500 (80.1786) lr 1.0000e-05 eta 0:01:04
epoch [1/2] batch [390/496] loss 0.5479 (0.5299) acc 81.2500 (80.2083) lr 1.0000e-05 eta 0:01:03
epoch [1/2] batch [395/496] loss 0.4846 (0.5292) acc 81.2500 (80.2532) lr 1.0000e-05 eta 0:01:02
epoch [1/2] batch [400/496] loss 0.5762 (0.5309) acc 75.0000 (80.1719) lr 1.0000e-05 eta 0:01:00
epoch [1/2] batch [405/496] loss 0.5107 (0.5300) acc 75.0000 (80.2006) lr 1.0000e-05 eta 0:00:59
epoch [1/2] batch [410/496] loss 0.4365 (0.5301) acc 81.2500 (80.1982) lr 1.0000e-05 eta 0:00:58
epoch [1/2] batch [415/496] loss 0.4946 (0.5301) acc 81.2500 (80.1958) lr 1.0000e-05 eta 0:00:57
epoch [1/2] batch [420/496] loss 0.4187 (0.5307) acc 87.5000 (80.1935) lr 1.0000e-05 eta 0:00:56
epoch [1/2] batch [425/496] loss 0.5615 (0.5305) acc 81.2500 (80.2353) lr 1.0000e-05 eta 0:00:55
epoch [1/2] batch [430/496] loss 0.6416 (0.5300) acc 75.0000 (80.2471) lr 1.0000e-05 eta 0:00:54
epoch [1/2] batch [435/496] loss 0.3545 (0.5288) acc 87.5000 (80.2730) lr 1.0000e-05 eta 0:00:53
epoch [1/2] batch [440/496] loss 0.1642 (0.5274) acc 100.0000 (80.3409) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [445/496] loss 0.1956 (0.5259) acc 93.7500 (80.4213) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [450/496] loss 0.1376 (0.5246) acc 93.7500 (80.4861) lr 1.0000e-05 eta 0:00:51
epoch [1/2] batch [455/496] loss 0.4617 (0.5243) acc 87.5000 (80.4808) lr 1.0000e-05 eta 0:00:50
epoch [1/2] batch [460/496] loss 0.4312 (0.5247) acc 81.2500 (80.4891) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [465/496] loss 0.7725 (0.5257) acc 75.0000 (80.4839) lr 1.0000e-05 eta 0:00:48
epoch [1/2] batch [470/496] loss 0.3149 (0.5250) acc 93.7500 (80.5186) lr 1.0000e-05 eta 0:00:47
epoch [1/2] batch [475/496] loss 0.5547 (0.5237) acc 81.2500 (80.5658) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [480/496] loss 0.5073 (0.5243) acc 81.2500 (80.5208) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [485/496] loss 0.4119 (0.5228) acc 87.5000 (80.5541) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [490/496] loss 0.5791 (0.5234) acc 87.5000 (80.5740) lr 1.0000e-05 eta 0:00:44
epoch [1/2] batch [495/496] loss 0.7397 (0.5256) acc 68.7500 (80.5051) lr 1.0000e-05 eta 0:00:43
epoch [2/2] batch [5/496] loss 0.5820 (0.4646) acc 68.7500 (81.2500) lr 2.0000e-03 eta 0:51:07
epoch [2/2] batch [10/496] loss 0.4062 (0.4256) acc 81.2500 (84.3750) lr 2.0000e-03 eta 0:25:24
epoch [2/2] batch [15/496] loss 0.4729 (0.4781) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:16:49
epoch [2/2] batch [20/496] loss 0.9512 (0.5142) acc 62.5000 (80.6250) lr 2.0000e-03 eta 0:12:32
epoch [2/2] batch [25/496] loss 0.7300 (0.5078) acc 75.0000 (80.7500) lr 2.0000e-03 eta 0:09:58
epoch [2/2] batch [30/496] loss 0.5781 (0.5138) acc 87.5000 (81.0417) lr 2.0000e-03 eta 0:08:15
epoch [2/2] batch [35/496] loss 0.8232 (0.5181) acc 75.0000 (80.8929) lr 2.0000e-03 eta 0:07:01
epoch [2/2] batch [40/496] loss 0.4199 (0.5050) acc 87.5000 (81.7188) lr 2.0000e-03 eta 0:06:06
epoch [2/2] batch [45/496] loss 0.7554 (0.5134) acc 81.2500 (81.1111) lr 2.0000e-03 eta 0:05:23
epoch [2/2] batch [50/496] loss 0.5298 (0.5270) acc 87.5000 (80.6250) lr 2.0000e-03 eta 0:04:48
epoch [2/2] batch [55/496] loss 0.7300 (0.5291) acc 68.7500 (80.6818) lr 2.0000e-03 eta 0:04:20
epoch [2/2] batch [60/496] loss 0.5537 (0.5302) acc 75.0000 (80.3125) lr 2.0000e-03 eta 0:03:57
epoch [2/2] batch [65/496] loss 0.4287 (0.5260) acc 81.2500 (80.6731) lr 2.0000e-03 eta 0:03:37
epoch [2/2] batch [70/496] loss 0.6548 (0.5269) acc 81.2500 (80.6250) lr 2.0000e-03 eta 0:03:20
epoch [2/2] batch [75/496] loss 0.3176 (0.5210) acc 81.2500 (80.9167) lr 2.0000e-03 eta 0:03:05
epoch [2/2] batch [80/496] loss 0.3982 (0.5172) acc 81.2500 (80.9375) lr 2.0000e-03 eta 0:02:52
epoch [2/2] batch [85/496] loss 0.2236 (0.5073) acc 93.7500 (81.4706) lr 2.0000e-03 eta 0:02:40
epoch [2/2] batch [90/496] loss 0.3003 (0.5049) acc 93.7500 (81.5972) lr 2.0000e-03 eta 0:02:30
epoch [2/2] batch [95/496] loss 0.6406 (0.5051) acc 75.0000 (81.6447) lr 2.0000e-03 eta 0:02:21
epoch [2/2] batch [100/496] loss 0.3081 (0.5011) acc 93.7500 (81.8125) lr 2.0000e-03 eta 0:02:13
epoch [2/2] batch [105/496] loss 0.8447 (0.5036) acc 56.2500 (81.6071) lr 2.0000e-03 eta 0:02:05
epoch [2/2] batch [110/496] loss 0.1654 (0.4923) acc 93.7500 (82.1591) lr 2.0000e-03 eta 0:01:58
epoch [2/2] batch [115/496] loss 0.3679 (0.4956) acc 87.5000 (82.0109) lr 2.0000e-03 eta 0:01:52
epoch [2/2] batch [120/496] loss 0.4253 (0.4963) acc 87.5000 (82.0312) lr 2.0000e-03 eta 0:01:46
epoch [2/2] batch [125/496] loss 0.8760 (0.4942) acc 75.0000 (82.2000) lr 2.0000e-03 eta 0:01:41
epoch [2/2] batch [130/496] loss 0.4014 (0.4932) acc 81.2500 (82.1635) lr 2.0000e-03 eta 0:01:36
epoch [2/2] batch [135/496] loss 0.3513 (0.4897) acc 81.2500 (82.1759) lr 2.0000e-03 eta 0:01:32
epoch [2/2] batch [140/496] loss 0.3706 (0.4902) acc 81.2500 (81.9643) lr 2.0000e-03 eta 0:01:28
epoch [2/2] batch [145/496] loss 0.3198 (0.4919) acc 87.5000 (81.7672) lr 2.0000e-03 eta 0:01:24
epoch [2/2] batch [150/496] loss 0.2450 (0.4947) acc 93.7500 (81.7083) lr 2.0000e-03 eta 0:01:20
epoch [2/2] batch [155/496] loss 0.3025 (0.4943) acc 81.2500 (81.7339) lr 2.0000e-03 eta 0:01:17
epoch [2/2] batch [160/496] loss 0.4961 (0.4898) acc 81.2500 (81.8359) lr 2.0000e-03 eta 0:01:13
epoch [2/2] batch [165/496] loss 0.1560 (0.4884) acc 93.7500 (81.8561) lr 2.0000e-03 eta 0:01:10
epoch [2/2] batch [170/496] loss 0.4727 (0.4860) acc 81.2500 (81.9853) lr 2.0000e-03 eta 0:01:07
epoch [2/2] batch [175/496] loss 0.1656 (0.4815) acc 93.7500 (82.2500) lr 2.0000e-03 eta 0:01:05
epoch [2/2] batch [180/496] loss 0.4055 (0.4846) acc 87.5000 (82.2569) lr 2.0000e-03 eta 0:01:02
epoch [2/2] batch [185/496] loss 0.7021 (0.4845) acc 87.5000 (82.3986) lr 2.0000e-03 eta 0:01:00
epoch [2/2] batch [190/496] loss 0.6147 (0.4913) acc 81.2500 (82.1711) lr 2.0000e-03 eta 0:00:57
epoch [2/2] batch [195/496] loss 0.6816 (0.4935) acc 81.2500 (82.1154) lr 2.0000e-03 eta 0:00:55
epoch [2/2] batch [200/496] loss 0.4805 (0.4910) acc 81.2500 (82.1250) lr 2.0000e-03 eta 0:00:53
epoch [2/2] batch [205/496] loss 0.6401 (0.4918) acc 75.0000 (82.1341) lr 2.0000e-03 eta 0:00:51
epoch [2/2] batch [210/496] loss 0.4519 (0.4895) acc 87.5000 (82.2917) lr 2.0000e-03 eta 0:00:49
epoch [2/2] batch [215/496] loss 0.5361 (0.4869) acc 81.2500 (82.3837) lr 2.0000e-03 eta 0:00:47
epoch [2/2] batch [220/496] loss 0.6665 (0.4871) acc 75.0000 (82.3580) lr 2.0000e-03 eta 0:00:46
epoch [2/2] batch [225/496] loss 0.4226 (0.4882) acc 87.5000 (82.3056) lr 2.0000e-03 eta 0:00:44
epoch [2/2] batch [230/496] loss 0.2683 (0.4879) acc 87.5000 (82.3098) lr 2.0000e-03 eta 0:00:42
epoch [2/2] batch [235/496] loss 0.6323 (0.4916) acc 68.7500 (82.1543) lr 2.0000e-03 eta 0:00:41
epoch [2/2] batch [240/496] loss 0.4841 (0.4912) acc 75.0000 (82.1094) lr 2.0000e-03 eta 0:00:39
epoch [2/2] batch [245/496] loss 0.5166 (0.4899) acc 81.2500 (82.1429) lr 2.0000e-03 eta 0:00:38
epoch [2/2] batch [250/496] loss 0.3333 (0.4869) acc 87.5000 (82.3000) lr 2.0000e-03 eta 0:00:36
epoch [2/2] batch [255/496] loss 0.2744 (0.4864) acc 100.0000 (82.3284) lr 2.0000e-03 eta 0:00:35
epoch [2/2] batch [260/496] loss 0.5181 (0.4877) acc 81.2500 (82.2596) lr 2.0000e-03 eta 0:00:34
epoch [2/2] batch [265/496] loss 0.3752 (0.4877) acc 87.5000 (82.2877) lr 2.0000e-03 eta 0:00:33
epoch [2/2] batch [270/496] loss 0.5781 (0.4865) acc 75.0000 (82.3380) lr 2.0000e-03 eta 0:00:31
epoch [2/2] batch [275/496] loss 0.3660 (0.4852) acc 81.2500 (82.3864) lr 2.0000e-03 eta 0:00:30
epoch [2/2] batch [280/496] loss 0.4844 (0.4856) acc 81.2500 (82.3661) lr 2.0000e-03 eta 0:00:29
epoch [2/2] batch [285/496] loss 0.5850 (0.4883) acc 68.7500 (82.1930) lr 2.0000e-03 eta 0:00:28
epoch [2/2] batch [290/496] loss 0.7354 (0.4906) acc 75.0000 (82.1121) lr 2.0000e-03 eta 0:00:27
epoch [2/2] batch [295/496] loss 0.3357 (0.4914) acc 87.5000 (82.0551) lr 2.0000e-03 eta 0:00:26
epoch [2/2] batch [300/496] loss 0.6416 (0.4934) acc 81.2500 (81.9792) lr 2.0000e-03 eta 0:00:25
epoch [2/2] batch [305/496] loss 0.5620 (0.4948) acc 75.0000 (81.8852) lr 2.0000e-03 eta 0:00:24
epoch [2/2] batch [310/496] loss 0.3333 (0.4930) acc 93.7500 (81.9556) lr 2.0000e-03 eta 0:00:23
epoch [2/2] batch [315/496] loss 0.5420 (0.4920) acc 81.2500 (81.9841) lr 2.0000e-03 eta 0:00:22
epoch [2/2] batch [320/496] loss 0.3748 (0.4928) acc 87.5000 (81.9922) lr 2.0000e-03 eta 0:00:21
epoch [2/2] batch [325/496] loss 0.7617 (0.4922) acc 68.7500 (82.0192) lr 2.0000e-03 eta 0:00:20
epoch [2/2] batch [330/496] loss 0.3975 (0.4920) acc 81.2500 (82.0644) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [335/496] loss 0.3057 (0.4939) acc 93.7500 (81.9590) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [340/496] loss 0.8389 (0.4936) acc 62.5000 (81.9485) lr 2.0000e-03 eta 0:00:18
epoch [2/2] batch [345/496] loss 0.1775 (0.4927) acc 100.0000 (81.9746) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [350/496] loss 0.3691 (0.4909) acc 81.2500 (82.0179) lr 2.0000e-03 eta 0:00:16
epoch [2/2] batch [355/496] loss 0.2661 (0.4904) acc 93.7500 (82.0423) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [360/496] loss 0.3579 (0.4892) acc 87.5000 (82.0660) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [365/496] loss 0.3750 (0.4899) acc 87.5000 (82.0377) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [370/496] loss 0.7368 (0.4890) acc 81.2500 (82.1115) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [375/496] loss 0.6211 (0.4896) acc 75.0000 (82.1000) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [380/496] loss 0.8032 (0.4904) acc 68.7500 (82.0395) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [385/496] loss 0.6274 (0.4897) acc 68.7500 (82.0292) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [390/496] loss 0.3577 (0.4909) acc 87.5000 (82.0673) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [395/496] loss 0.5293 (0.4918) acc 81.2500 (82.0728) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [400/496] loss 0.5703 (0.4938) acc 75.0000 (81.9688) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [405/496] loss 0.5156 (0.4924) acc 87.5000 (82.0525) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [410/496] loss 0.1969 (0.4919) acc 100.0000 (82.1037) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [415/496] loss 0.6191 (0.4923) acc 75.0000 (82.0783) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [420/496] loss 0.8452 (0.4930) acc 62.5000 (82.0536) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [425/496] loss 0.3325 (0.4916) acc 87.5000 (82.1029) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [430/496] loss 0.5830 (0.4911) acc 81.2500 (82.1657) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [435/496] loss 0.3032 (0.4903) acc 87.5000 (82.1983) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [440/496] loss 0.8721 (0.4915) acc 62.5000 (82.1449) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [445/496] loss 0.4758 (0.4915) acc 81.2500 (82.1770) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [450/496] loss 0.2573 (0.4907) acc 93.7500 (82.2222) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [455/496] loss 0.5620 (0.4907) acc 68.7500 (82.1703) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [460/496] loss 0.1848 (0.4899) acc 93.7500 (82.1875) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [465/496] loss 0.8843 (0.4896) acc 62.5000 (82.2043) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [470/496] loss 1.0527 (0.4908) acc 56.2500 (82.1410) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [475/496] loss 0.4526 (0.4903) acc 87.5000 (82.1447) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [480/496] loss 0.6465 (0.4888) acc 87.5000 (82.2396) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [485/496] loss 0.4678 (0.4888) acc 81.2500 (82.2552) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [490/496] loss 0.5908 (0.4894) acc 75.0000 (82.2321) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [495/496] loss 0.4121 (0.4891) acc 81.2500 (82.2475) lr 2.0000e-03 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting\model.pth.tar-2
Finish Training
Evaluate on the Test Set
----------  ------
Total #     2,048
Correct #   1,977
Accuracy    96.53%
Error Rate  3.47%
Macro_F1    96.35%
----------  ------
