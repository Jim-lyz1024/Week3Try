*** Config ***
***************
** Arguments **
***************
dataset: PACS
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-PACS-art_painting
root: ./data/
seed: 232
source_domains: ['cartoon', 'photo', 'sketch']
target_domains: ['art_painting']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 16
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 16
    SAMPLER: RandomSampler
DATASET:
  NAME: PACS
  ROOT: ./data/
  SOURCE_DOMAINS: ['cartoon', 'photo', 'sketch']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['art_painting']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-PACS-art_painting
SEED: 232
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: PACS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ------------------------------
Dataset         PACS
Source Domains  ['cartoon', 'photo', 'sketch']
Target Domains  ['art_painting']
# Classes       7
# Train Data    7,942
# Val Data      806
# Test Data     2,048
--------------  ------------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.0.weight', 'adapter.fc.2.weight'}
Build Evaluator: Classification
epoch [1/2] batch [5/496] loss 0.8213 (0.6122) acc 68.7500 (75.0000) lr 1.0000e-05 eta 1:44:26
epoch [1/2] batch [10/496] loss 0.7632 (0.6402) acc 81.2500 (78.1250) lr 1.0000e-05 eta 0:52:11
epoch [1/2] batch [15/496] loss 0.4141 (0.5856) acc 87.5000 (78.7500) lr 1.0000e-05 eta 0:34:45
epoch [1/2] batch [20/496] loss 0.2402 (0.5309) acc 93.7500 (80.0000) lr 1.0000e-05 eta 0:26:02
epoch [1/2] batch [25/496] loss 0.3569 (0.5145) acc 87.5000 (81.0000) lr 1.0000e-05 eta 0:20:48
epoch [1/2] batch [30/496] loss 0.5518 (0.5031) acc 81.2500 (81.6667) lr 1.0000e-05 eta 0:17:18
epoch [1/2] batch [35/496] loss 0.3423 (0.5163) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:14:49
epoch [1/2] batch [40/496] loss 0.6812 (0.5144) acc 75.0000 (80.7812) lr 1.0000e-05 eta 0:12:56
epoch [1/2] batch [45/496] loss 0.2219 (0.4964) acc 87.5000 (81.5278) lr 1.0000e-05 eta 0:11:29
epoch [1/2] batch [50/496] loss 0.5054 (0.5044) acc 75.0000 (81.2500) lr 1.0000e-05 eta 0:10:19
epoch [1/2] batch [55/496] loss 0.3701 (0.5062) acc 87.5000 (81.0227) lr 1.0000e-05 eta 0:09:22
epoch [1/2] batch [60/496] loss 0.5093 (0.5051) acc 75.0000 (81.0417) lr 1.0000e-05 eta 0:08:34
epoch [1/2] batch [65/496] loss 0.2201 (0.5087) acc 93.7500 (80.7692) lr 1.0000e-05 eta 0:07:54
epoch [1/2] batch [70/496] loss 1.1162 (0.5177) acc 62.5000 (80.7143) lr 1.0000e-05 eta 0:07:19
epoch [1/2] batch [75/496] loss 0.6167 (0.5151) acc 75.0000 (80.6667) lr 1.0000e-05 eta 0:06:49
epoch [1/2] batch [80/496] loss 0.9111 (0.5238) acc 62.5000 (80.3125) lr 1.0000e-05 eta 0:06:23
epoch [1/2] batch [85/496] loss 0.4055 (0.5138) acc 87.5000 (80.8088) lr 1.0000e-05 eta 0:06:00
epoch [1/2] batch [90/496] loss 0.2395 (0.5287) acc 93.7500 (80.4167) lr 1.0000e-05 eta 0:05:40
epoch [1/2] batch [95/496] loss 0.4353 (0.5259) acc 87.5000 (80.3289) lr 1.0000e-05 eta 0:05:21
epoch [1/2] batch [100/496] loss 0.3899 (0.5226) acc 87.5000 (80.4375) lr 1.0000e-05 eta 0:05:05
epoch [1/2] batch [105/496] loss 0.7168 (0.5244) acc 75.0000 (80.2976) lr 1.0000e-05 eta 0:04:49
epoch [1/2] batch [110/496] loss 0.7515 (0.5242) acc 75.0000 (80.3409) lr 1.0000e-05 eta 0:04:36
epoch [1/2] batch [115/496] loss 0.6128 (0.5192) acc 75.0000 (80.6522) lr 1.0000e-05 eta 0:04:23
epoch [1/2] batch [120/496] loss 0.3594 (0.5187) acc 87.5000 (80.6771) lr 1.0000e-05 eta 0:04:12
epoch [1/2] batch [125/496] loss 0.3438 (0.5197) acc 87.5000 (80.7000) lr 1.0000e-05 eta 0:04:01
epoch [1/2] batch [130/496] loss 0.4805 (0.5169) acc 75.0000 (80.7692) lr 1.0000e-05 eta 0:03:51
epoch [1/2] batch [135/496] loss 0.1604 (0.5172) acc 93.7500 (80.7407) lr 1.0000e-05 eta 0:03:42
epoch [1/2] batch [140/496] loss 0.6558 (0.5190) acc 75.0000 (80.5357) lr 1.0000e-05 eta 0:03:34
epoch [1/2] batch [145/496] loss 0.7666 (0.5191) acc 75.0000 (80.6034) lr 1.0000e-05 eta 0:03:26
epoch [1/2] batch [150/496] loss 0.4900 (0.5185) acc 87.5000 (80.6667) lr 1.0000e-05 eta 0:03:19
epoch [1/2] batch [155/496] loss 0.7744 (0.5233) acc 81.2500 (80.5645) lr 1.0000e-05 eta 0:03:12
epoch [1/2] batch [160/496] loss 0.4792 (0.5200) acc 81.2500 (80.5469) lr 1.0000e-05 eta 0:03:05
epoch [1/2] batch [165/496] loss 0.8989 (0.5242) acc 68.7500 (80.3409) lr 1.0000e-05 eta 0:02:59
epoch [1/2] batch [170/496] loss 0.4871 (0.5279) acc 81.2500 (80.2206) lr 1.0000e-05 eta 0:02:53
epoch [1/2] batch [175/496] loss 0.6982 (0.5270) acc 81.2500 (80.3214) lr 1.0000e-05 eta 0:02:48
epoch [1/2] batch [180/496] loss 0.4326 (0.5242) acc 75.0000 (80.2083) lr 1.0000e-05 eta 0:02:43
epoch [1/2] batch [185/496] loss 0.4282 (0.5230) acc 87.5000 (80.3716) lr 1.0000e-05 eta 0:02:38
epoch [1/2] batch [190/496] loss 0.7119 (0.5278) acc 75.0000 (80.1974) lr 1.0000e-05 eta 0:02:33
epoch [1/2] batch [195/496] loss 0.2896 (0.5234) acc 93.7500 (80.4487) lr 1.0000e-05 eta 0:02:29
epoch [1/2] batch [200/496] loss 0.6934 (0.5165) acc 68.7500 (80.6562) lr 1.0000e-05 eta 0:02:25
epoch [1/2] batch [205/496] loss 0.2214 (0.5153) acc 93.7500 (80.7317) lr 1.0000e-05 eta 0:02:21
epoch [1/2] batch [210/496] loss 0.7915 (0.5174) acc 87.5000 (80.7143) lr 1.0000e-05 eta 0:02:17
epoch [1/2] batch [215/496] loss 0.4932 (0.5196) acc 81.2500 (80.6395) lr 1.0000e-05 eta 0:02:14
epoch [1/2] batch [220/496] loss 0.5581 (0.5220) acc 87.5000 (80.6250) lr 1.0000e-05 eta 0:02:10
epoch [1/2] batch [225/496] loss 0.5073 (0.5215) acc 75.0000 (80.6111) lr 1.0000e-05 eta 0:02:07
epoch [1/2] batch [230/496] loss 0.6382 (0.5238) acc 81.2500 (80.4348) lr 1.0000e-05 eta 0:02:04
epoch [1/2] batch [235/496] loss 0.4121 (0.5233) acc 81.2500 (80.4521) lr 1.0000e-05 eta 0:02:01
epoch [1/2] batch [240/496] loss 0.9795 (0.5265) acc 75.0000 (80.3385) lr 1.0000e-05 eta 0:01:58
epoch [1/2] batch [245/496] loss 0.7100 (0.5277) acc 75.0000 (80.2551) lr 1.0000e-05 eta 0:01:55
epoch [1/2] batch [250/496] loss 0.4351 (0.5264) acc 81.2500 (80.2750) lr 1.0000e-05 eta 0:01:52
epoch [1/2] batch [255/496] loss 0.2273 (0.5243) acc 87.5000 (80.3431) lr 1.0000e-05 eta 0:01:50
epoch [1/2] batch [260/496] loss 0.1400 (0.5220) acc 100.0000 (80.4087) lr 1.0000e-05 eta 0:01:47
epoch [1/2] batch [265/496] loss 0.6621 (0.5222) acc 75.0000 (80.3538) lr 1.0000e-05 eta 0:01:45
epoch [1/2] batch [270/496] loss 0.6768 (0.5220) acc 68.7500 (80.3472) lr 1.0000e-05 eta 0:01:42
epoch [1/2] batch [275/496] loss 0.4546 (0.5219) acc 81.2500 (80.3182) lr 1.0000e-05 eta 0:01:40
epoch [1/2] batch [280/496] loss 0.6772 (0.5227) acc 81.2500 (80.3348) lr 1.0000e-05 eta 0:01:38
epoch [1/2] batch [285/496] loss 0.4929 (0.5203) acc 81.2500 (80.4167) lr 1.0000e-05 eta 0:01:36
epoch [1/2] batch [290/496] loss 0.6982 (0.5225) acc 62.5000 (80.3017) lr 1.0000e-05 eta 0:01:34
epoch [1/2] batch [295/496] loss 0.2002 (0.5232) acc 93.7500 (80.2966) lr 1.0000e-05 eta 0:01:32
epoch [1/2] batch [300/496] loss 0.5718 (0.5247) acc 75.0000 (80.2292) lr 1.0000e-05 eta 0:01:30
epoch [1/2] batch [305/496] loss 0.6758 (0.5245) acc 81.2500 (80.3074) lr 1.0000e-05 eta 0:01:28
epoch [1/2] batch [310/496] loss 0.4614 (0.5215) acc 81.2500 (80.4234) lr 1.0000e-05 eta 0:01:26
epoch [1/2] batch [315/496] loss 0.7568 (0.5199) acc 75.0000 (80.4762) lr 1.0000e-05 eta 0:01:25
epoch [1/2] batch [320/496] loss 1.0918 (0.5232) acc 75.0000 (80.3906) lr 1.0000e-05 eta 0:01:23
epoch [1/2] batch [325/496] loss 0.5063 (0.5218) acc 81.2500 (80.4808) lr 1.0000e-05 eta 0:01:21
epoch [1/2] batch [330/496] loss 0.3867 (0.5212) acc 87.5000 (80.4924) lr 1.0000e-05 eta 0:01:20
epoch [1/2] batch [335/496] loss 0.8076 (0.5210) acc 75.0000 (80.5224) lr 1.0000e-05 eta 0:01:18
epoch [1/2] batch [340/496] loss 0.2837 (0.5212) acc 87.5000 (80.5882) lr 1.0000e-05 eta 0:01:17
epoch [1/2] batch [345/496] loss 0.7168 (0.5209) acc 68.7500 (80.5978) lr 1.0000e-05 eta 0:01:15
epoch [1/2] batch [350/496] loss 0.4558 (0.5205) acc 75.0000 (80.5714) lr 1.0000e-05 eta 0:01:14
epoch [1/2] batch [355/496] loss 0.2920 (0.5186) acc 93.7500 (80.5986) lr 1.0000e-05 eta 0:01:12
epoch [1/2] batch [360/496] loss 0.5503 (0.5203) acc 81.2500 (80.5382) lr 1.0000e-05 eta 0:01:11
epoch [1/2] batch [365/496] loss 0.2118 (0.5172) acc 93.7500 (80.6678) lr 1.0000e-05 eta 0:01:10
epoch [1/2] batch [370/496] loss 0.4751 (0.5198) acc 81.2500 (80.5912) lr 1.0000e-05 eta 0:01:08
epoch [1/2] batch [375/496] loss 0.8423 (0.5193) acc 68.7500 (80.5833) lr 1.0000e-05 eta 0:01:07
epoch [1/2] batch [380/496] loss 0.6777 (0.5196) acc 81.2500 (80.5757) lr 1.0000e-05 eta 0:01:06
epoch [1/2] batch [385/496] loss 0.5532 (0.5189) acc 81.2500 (80.6331) lr 1.0000e-05 eta 0:01:05
epoch [1/2] batch [390/496] loss 0.8184 (0.5209) acc 68.7500 (80.5929) lr 1.0000e-05 eta 0:01:03
epoch [1/2] batch [395/496] loss 0.6675 (0.5213) acc 75.0000 (80.5696) lr 1.0000e-05 eta 0:01:02
epoch [1/2] batch [400/496] loss 0.4548 (0.5197) acc 87.5000 (80.6250) lr 1.0000e-05 eta 0:01:01
epoch [1/2] batch [405/496] loss 0.7627 (0.5195) acc 68.7500 (80.6327) lr 1.0000e-05 eta 0:01:00
epoch [1/2] batch [410/496] loss 0.1682 (0.5186) acc 100.0000 (80.6707) lr 1.0000e-05 eta 0:00:59
epoch [1/2] batch [415/496] loss 0.4927 (0.5169) acc 75.0000 (80.7078) lr 1.0000e-05 eta 0:00:58
epoch [1/2] batch [420/496] loss 0.1653 (0.5163) acc 93.7500 (80.7738) lr 1.0000e-05 eta 0:00:57
epoch [1/2] batch [425/496] loss 0.6772 (0.5183) acc 75.0000 (80.7059) lr 1.0000e-05 eta 0:00:56
epoch [1/2] batch [430/496] loss 0.2913 (0.5158) acc 87.5000 (80.7703) lr 1.0000e-05 eta 0:00:55
epoch [1/2] batch [435/496] loss 0.3420 (0.5147) acc 81.2500 (80.8046) lr 1.0000e-05 eta 0:00:54
epoch [1/2] batch [440/496] loss 0.3618 (0.5153) acc 87.5000 (80.7812) lr 1.0000e-05 eta 0:00:53
epoch [1/2] batch [445/496] loss 0.9097 (0.5159) acc 68.7500 (80.7865) lr 1.0000e-05 eta 0:00:52
epoch [1/2] batch [450/496] loss 0.5757 (0.5166) acc 81.2500 (80.8194) lr 1.0000e-05 eta 0:00:51
epoch [1/2] batch [455/496] loss 0.3464 (0.5153) acc 87.5000 (80.8929) lr 1.0000e-05 eta 0:00:50
epoch [1/2] batch [460/496] loss 0.2229 (0.5139) acc 93.7500 (80.9511) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [465/496] loss 0.9756 (0.5143) acc 75.0000 (80.9677) lr 1.0000e-05 eta 0:00:49
epoch [1/2] batch [470/496] loss 0.5835 (0.5149) acc 81.2500 (80.9309) lr 1.0000e-05 eta 0:00:48
epoch [1/2] batch [475/496] loss 0.5449 (0.5155) acc 68.7500 (80.8816) lr 1.0000e-05 eta 0:00:47
epoch [1/2] batch [480/496] loss 0.2568 (0.5148) acc 93.7500 (80.8984) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [485/496] loss 0.2527 (0.5135) acc 93.7500 (80.9665) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [490/496] loss 0.3733 (0.5127) acc 93.7500 (81.0459) lr 1.0000e-05 eta 0:00:45
epoch [1/2] batch [495/496] loss 0.3975 (0.5121) acc 81.2500 (81.0732) lr 1.0000e-05 eta 0:00:44
epoch [2/2] batch [5/496] loss 0.6948 (0.4993) acc 75.0000 (81.2500) lr 2.0000e-03 eta 0:52:57
epoch [2/2] batch [10/496] loss 0.6460 (0.4968) acc 75.0000 (81.8750) lr 2.0000e-03 eta 0:26:18
epoch [2/2] batch [15/496] loss 0.4055 (0.4875) acc 93.7500 (82.0833) lr 2.0000e-03 eta 0:17:25
epoch [2/2] batch [20/496] loss 0.9385 (0.5377) acc 81.2500 (80.9375) lr 2.0000e-03 eta 0:12:58
epoch [2/2] batch [25/496] loss 0.6040 (0.5711) acc 75.0000 (80.0000) lr 2.0000e-03 eta 0:10:18
epoch [2/2] batch [30/496] loss 0.6353 (0.5750) acc 68.7500 (78.9583) lr 2.0000e-03 eta 0:08:32
epoch [2/2] batch [35/496] loss 0.8198 (0.5820) acc 68.7500 (78.7500) lr 2.0000e-03 eta 0:07:15
epoch [2/2] batch [40/496] loss 0.3450 (0.5779) acc 81.2500 (78.7500) lr 2.0000e-03 eta 0:06:18
epoch [2/2] batch [45/496] loss 0.4246 (0.5635) acc 81.2500 (79.3056) lr 2.0000e-03 eta 0:05:34
epoch [2/2] batch [50/496] loss 0.4258 (0.5635) acc 81.2500 (79.2500) lr 2.0000e-03 eta 0:04:58
epoch [2/2] batch [55/496] loss 0.4568 (0.5562) acc 81.2500 (79.7727) lr 2.0000e-03 eta 0:04:29
epoch [2/2] batch [60/496] loss 0.4265 (0.5509) acc 87.5000 (80.2083) lr 2.0000e-03 eta 0:04:05
epoch [2/2] batch [65/496] loss 0.4177 (0.5381) acc 81.2500 (80.6731) lr 2.0000e-03 eta 0:03:44
epoch [2/2] batch [70/496] loss 0.3430 (0.5269) acc 87.5000 (81.0714) lr 2.0000e-03 eta 0:03:26
epoch [2/2] batch [75/496] loss 0.4424 (0.5243) acc 87.5000 (81.0833) lr 2.0000e-03 eta 0:03:11
epoch [2/2] batch [80/496] loss 0.4409 (0.5158) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:02:57
epoch [2/2] batch [85/496] loss 0.3394 (0.5197) acc 81.2500 (80.9559) lr 2.0000e-03 eta 0:02:46
epoch [2/2] batch [90/496] loss 0.3467 (0.5164) acc 81.2500 (81.1111) lr 2.0000e-03 eta 0:02:35
epoch [2/2] batch [95/496] loss 0.3823 (0.5140) acc 87.5000 (81.0526) lr 2.0000e-03 eta 0:02:26
epoch [2/2] batch [100/496] loss 0.7505 (0.5096) acc 68.7500 (81.1250) lr 2.0000e-03 eta 0:02:17
epoch [2/2] batch [105/496] loss 0.4780 (0.5135) acc 87.5000 (80.9524) lr 2.0000e-03 eta 0:02:09
epoch [2/2] batch [110/496] loss 0.3667 (0.5135) acc 93.7500 (80.9091) lr 2.0000e-03 eta 0:02:02
epoch [2/2] batch [115/496] loss 0.3093 (0.5074) acc 93.7500 (81.2500) lr 2.0000e-03 eta 0:01:56
epoch [2/2] batch [120/496] loss 0.5820 (0.5015) acc 87.5000 (81.5625) lr 2.0000e-03 eta 0:01:50
epoch [2/2] batch [125/496] loss 0.5928 (0.5012) acc 68.7500 (81.5500) lr 2.0000e-03 eta 0:01:45
epoch [2/2] batch [130/496] loss 0.5278 (0.5081) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:01:39
epoch [2/2] batch [135/496] loss 0.4385 (0.5053) acc 87.5000 (81.2963) lr 2.0000e-03 eta 0:01:35
epoch [2/2] batch [140/496] loss 0.4873 (0.5044) acc 87.5000 (81.4286) lr 2.0000e-03 eta 0:01:30
epoch [2/2] batch [145/496] loss 0.3271 (0.5010) acc 87.5000 (81.5517) lr 2.0000e-03 eta 0:01:26
epoch [2/2] batch [150/496] loss 0.6738 (0.5007) acc 68.7500 (81.6250) lr 2.0000e-03 eta 0:01:23
epoch [2/2] batch [155/496] loss 1.0498 (0.5035) acc 68.7500 (81.6935) lr 2.0000e-03 eta 0:01:19
epoch [2/2] batch [160/496] loss 0.3633 (0.5013) acc 87.5000 (81.7188) lr 2.0000e-03 eta 0:01:16
epoch [2/2] batch [165/496] loss 0.5444 (0.5007) acc 75.0000 (81.7424) lr 2.0000e-03 eta 0:01:13
epoch [2/2] batch [170/496] loss 0.6313 (0.5000) acc 75.0000 (81.7279) lr 2.0000e-03 eta 0:01:10
epoch [2/2] batch [175/496] loss 0.5483 (0.4963) acc 81.2500 (81.8571) lr 2.0000e-03 eta 0:01:07
epoch [2/2] batch [180/496] loss 0.3323 (0.4980) acc 87.5000 (81.7014) lr 2.0000e-03 eta 0:01:04
epoch [2/2] batch [185/496] loss 0.3635 (0.4996) acc 87.5000 (81.5878) lr 2.0000e-03 eta 0:01:02
epoch [2/2] batch [190/496] loss 0.3821 (0.5007) acc 93.7500 (81.5461) lr 2.0000e-03 eta 0:00:59
epoch [2/2] batch [195/496] loss 0.6851 (0.5016) acc 68.7500 (81.5064) lr 2.0000e-03 eta 0:00:57
epoch [2/2] batch [200/496] loss 0.7871 (0.5010) acc 62.5000 (81.5625) lr 2.0000e-03 eta 0:00:55
epoch [2/2] batch [205/496] loss 0.5220 (0.4994) acc 75.0000 (81.6159) lr 2.0000e-03 eta 0:00:53
epoch [2/2] batch [210/496] loss 0.3809 (0.5000) acc 81.2500 (81.6071) lr 2.0000e-03 eta 0:00:51
epoch [2/2] batch [215/496] loss 0.6562 (0.5015) acc 81.2500 (81.5988) lr 2.0000e-03 eta 0:00:49
epoch [2/2] batch [220/496] loss 0.3442 (0.4996) acc 87.5000 (81.6477) lr 2.0000e-03 eta 0:00:47
epoch [2/2] batch [225/496] loss 0.7026 (0.4981) acc 75.0000 (81.7500) lr 2.0000e-03 eta 0:00:45
epoch [2/2] batch [230/496] loss 0.8765 (0.4990) acc 75.0000 (81.6848) lr 2.0000e-03 eta 0:00:44
epoch [2/2] batch [235/496] loss 0.6738 (0.5015) acc 62.5000 (81.5160) lr 2.0000e-03 eta 0:00:42
epoch [2/2] batch [240/496] loss 0.3279 (0.5024) acc 87.5000 (81.4844) lr 2.0000e-03 eta 0:00:40
epoch [2/2] batch [245/496] loss 0.2866 (0.5014) acc 87.5000 (81.5561) lr 2.0000e-03 eta 0:00:39
epoch [2/2] batch [250/496] loss 0.7437 (0.5053) acc 68.7500 (81.3500) lr 2.0000e-03 eta 0:00:37
epoch [2/2] batch [255/496] loss 0.3025 (0.5053) acc 87.5000 (81.3971) lr 2.0000e-03 eta 0:00:36
epoch [2/2] batch [260/496] loss 0.4946 (0.5050) acc 81.2500 (81.4183) lr 2.0000e-03 eta 0:00:35
epoch [2/2] batch [265/496] loss 0.4985 (0.5043) acc 81.2500 (81.4151) lr 2.0000e-03 eta 0:00:33
epoch [2/2] batch [270/496] loss 0.8931 (0.5075) acc 56.2500 (81.3194) lr 2.0000e-03 eta 0:00:32
epoch [2/2] batch [275/496] loss 0.1860 (0.5052) acc 93.7500 (81.4318) lr 2.0000e-03 eta 0:00:31
epoch [2/2] batch [280/496] loss 0.0760 (0.5019) acc 100.0000 (81.5402) lr 2.0000e-03 eta 0:00:30
epoch [2/2] batch [285/496] loss 0.2323 (0.5015) acc 93.7500 (81.5132) lr 2.0000e-03 eta 0:00:29
epoch [2/2] batch [290/496] loss 0.1888 (0.5002) acc 93.7500 (81.4655) lr 2.0000e-03 eta 0:00:28
epoch [2/2] batch [295/496] loss 0.4951 (0.4999) acc 87.5000 (81.5254) lr 2.0000e-03 eta 0:00:27
epoch [2/2] batch [300/496] loss 0.3164 (0.4993) acc 87.5000 (81.5625) lr 2.0000e-03 eta 0:00:26
epoch [2/2] batch [305/496] loss 0.4407 (0.4984) acc 81.2500 (81.5984) lr 2.0000e-03 eta 0:00:25
epoch [2/2] batch [310/496] loss 0.8633 (0.4987) acc 68.7500 (81.5726) lr 2.0000e-03 eta 0:00:24
epoch [2/2] batch [315/496] loss 0.3872 (0.4972) acc 87.5000 (81.6270) lr 2.0000e-03 eta 0:00:23
epoch [2/2] batch [320/496] loss 0.4773 (0.4958) acc 75.0000 (81.6602) lr 2.0000e-03 eta 0:00:22
epoch [2/2] batch [325/496] loss 0.2581 (0.4932) acc 87.5000 (81.7692) lr 2.0000e-03 eta 0:00:21
epoch [2/2] batch [330/496] loss 0.2598 (0.4922) acc 93.7500 (81.8371) lr 2.0000e-03 eta 0:00:20
epoch [2/2] batch [335/496] loss 0.6050 (0.4919) acc 75.0000 (81.8470) lr 2.0000e-03 eta 0:00:19
epoch [2/2] batch [340/496] loss 0.4207 (0.4918) acc 87.5000 (81.8750) lr 2.0000e-03 eta 0:00:18
epoch [2/2] batch [345/496] loss 0.8643 (0.4915) acc 75.0000 (81.9022) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [350/496] loss 0.1931 (0.4900) acc 93.7500 (81.9821) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [355/496] loss 0.1318 (0.4898) acc 100.0000 (81.9718) lr 2.0000e-03 eta 0:00:16
epoch [2/2] batch [360/496] loss 0.1904 (0.4884) acc 93.7500 (82.0660) lr 2.0000e-03 eta 0:00:15
epoch [2/2] batch [365/496] loss 0.3538 (0.4884) acc 93.7500 (82.0719) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [370/496] loss 0.2361 (0.4863) acc 100.0000 (82.1959) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [375/496] loss 0.3188 (0.4847) acc 81.2500 (82.2333) lr 2.0000e-03 eta 0:00:13
epoch [2/2] batch [380/496] loss 0.7529 (0.4853) acc 75.0000 (82.2204) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [385/496] loss 0.5415 (0.4843) acc 81.2500 (82.2565) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [390/496] loss 0.4734 (0.4856) acc 87.5000 (82.2115) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [395/496] loss 0.2056 (0.4840) acc 87.5000 (82.2627) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [400/496] loss 0.4368 (0.4846) acc 81.2500 (82.2031) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [405/496] loss 0.4285 (0.4829) acc 81.2500 (82.2840) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [410/496] loss 0.5161 (0.4828) acc 75.0000 (82.2866) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [415/496] loss 0.4226 (0.4841) acc 81.2500 (82.2289) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [420/496] loss 0.2837 (0.4839) acc 87.5000 (82.2024) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [425/496] loss 0.2448 (0.4837) acc 93.7500 (82.2353) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [430/496] loss 0.1436 (0.4819) acc 93.7500 (82.2820) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [435/496] loss 0.4478 (0.4811) acc 93.7500 (82.3276) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [440/496] loss 0.5239 (0.4808) acc 81.2500 (82.3153) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [445/496] loss 0.4888 (0.4831) acc 81.2500 (82.1910) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [450/496] loss 0.7612 (0.4834) acc 81.2500 (82.2083) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [455/496] loss 0.5825 (0.4818) acc 81.2500 (82.2527) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [460/496] loss 0.8496 (0.4827) acc 68.7500 (82.2418) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [465/496] loss 0.7031 (0.4835) acc 68.7500 (82.1909) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [470/496] loss 0.3701 (0.4821) acc 87.5000 (82.2473) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [475/496] loss 0.3279 (0.4803) acc 87.5000 (82.3684) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [480/496] loss 0.5342 (0.4809) acc 81.2500 (82.3307) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [485/496] loss 0.4663 (0.4808) acc 93.7500 (82.3454) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [490/496] loss 0.5278 (0.4806) acc 81.2500 (82.3469) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [495/496] loss 0.5293 (0.4809) acc 81.2500 (82.3232) lr 2.0000e-03 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-PACS-art_painting\model.pth.tar-2
Finish Training
Evaluate on the Test Set
----------  ------
Total #     2,048
Correct #   1,978
Accuracy    96.58%
Error Rate  3.42%
Macro_F1    96.40%
----------  ------
