*** Config ***
***************
** Arguments **
***************
dataset: Digits
gpu: 4
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-Digits-mnist
root: ./data/
seed: 134
source_domains: ['mnist_m', 'mnist', 'svhn']
target_domains: ['syn']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 64
    SAMPLER: RandomSampler
DATASET:
  NAME: Digits
  ROOT: ./data/
  SOURCE_DOMAINS: ['mnist_m', 'mnist', 'svhn']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['syn']
GPU: 4
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-Digits-mnist
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: Digits
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  ----------------------------
Dataset         Digits
Source Domains  ['mnist_m', 'mnist', 'svhn']
Target Domains  ['syn']
# Classes       10
# Train Data    18,000
# Val Data      3,600
# Test Data     6,000
--------------  ----------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
{'original': ['a picture of a 0.', 'a picture of a 1.', 'a picture of a 2.', 'a picture of a 3.', 'a picture of a 4.', 'a picture of a 5.', 'a picture of a 6.', 'a picture of a 7.', 'a picture of a 8.', 'a picture of a 9.'], 'mnist_m': ['a picture of a mnist m 0.', 'a picture of a mnist m 1.', 'a picture of a mnist m 2.', 'a picture of a mnist m 3.', 'a picture of a mnist m 4.', 'a picture of a mnist m 5.', 'a picture of a mnist m 6.', 'a picture of a mnist m 7.', 'a picture of a mnist m 8.', 'a picture of a mnist m 9.'], 'mnist': ['a picture of a mnist 0.', 'a picture of a mnist 1.', 'a picture of a mnist 2.', 'a picture of a mnist 3.', 'a picture of a mnist 4.', 'a picture of a mnist 5.', 'a picture of a mnist 6.', 'a picture of a mnist 7.', 'a picture of a mnist 8.', 'a picture of a mnist 9.'], 'svhn': ['a picture of a svhn 0.', 'a picture of a svhn 1.', 'a picture of a svhn 2.', 'a picture of a svhn 3.', 'a picture of a svhn 4.', 'a picture of a svhn 5.', 'a picture of a svhn 6.', 'a picture of a svhn 7.', 'a picture of a svhn 8.', 'a picture of a svhn 9.']}
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.2.weight', 'adapter.fc.0.weight'}
Build Evaluator: Classification
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [5/281] loss 5.2578 (5.0391) acc 1.5625 (3.1250) lr 1.0000e-05 eta 0:04:52
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [10/281] loss 4.9648 (5.0453) acc 6.2500 (2.6562) lr 1.0000e-05 eta 0:02:28
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [15/281] loss 5.1250 (5.0516) acc 3.1250 (2.5000) lr 1.0000e-05 eta 0:01:40
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [20/281] loss 4.9844 (5.0248) acc 0.0000 (2.1094) lr 1.0000e-05 eta 0:01:16
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [25/281] loss 5.2773 (5.0633) acc 0.0000 (1.8750) lr 1.0000e-05 eta 0:01:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [30/281] loss 5.0352 (5.0522) acc 1.5625 (1.7188) lr 1.0000e-05 eta 0:00:51
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [35/281] loss 4.9492 (5.0542) acc 6.2500 (1.6964) lr 1.0000e-05 eta 0:00:44
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [40/281] loss 5.0703 (5.0540) acc 0.0000 (1.6406) lr 1.0000e-05 eta 0:00:39
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [45/281] loss 4.9805 (5.0484) acc 1.5625 (1.7014) lr 1.0000e-05 eta 0:00:35
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [50/281] loss 5.0117 (5.0501) acc 4.6875 (1.8750) lr 1.0000e-05 eta 0:00:33
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [55/281] loss 5.0625 (5.0630) acc 0.0000 (1.8182) lr 1.0000e-05 eta 0:00:30
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [60/281] loss 5.2539 (5.0614) acc 1.5625 (1.8490) lr 1.0000e-05 eta 0:00:28
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [65/281] loss 5.3008 (5.0643) acc 1.5625 (1.9471) lr 1.0000e-05 eta 0:00:26
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [70/281] loss 5.1953 (5.0540) acc 3.1250 (1.9643) lr 1.0000e-05 eta 0:00:24
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [75/281] loss 4.9180 (5.0471) acc 6.2500 (2.0833) lr 1.0000e-05 eta 0:00:23
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [80/281] loss 5.2266 (5.0491) acc 3.1250 (2.0508) lr 1.0000e-05 eta 0:00:22
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [85/281] loss 4.7891 (5.0419) acc 4.6875 (2.0956) lr 1.0000e-05 eta 0:00:21
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [90/281] loss 5.0898 (5.0354) acc 1.5625 (2.0833) lr 1.0000e-05 eta 0:00:20
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [95/281] loss 5.1211 (5.0400) acc 0.0000 (2.0559) lr 1.0000e-05 eta 0:00:19
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [100/281] loss 4.7578 (5.0393) acc 3.1250 (2.0469) lr 1.0000e-05 eta 0:00:19
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [105/281] loss 4.9688 (5.0340) acc 0.0000 (2.0536) lr 1.0000e-05 eta 0:00:18
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [110/281] loss 4.6484 (5.0259) acc 1.5625 (2.0312) lr 1.0000e-05 eta 0:00:17
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [115/281] loss 5.0742 (5.0268) acc 3.1250 (2.0380) lr 1.0000e-05 eta 0:00:17
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [120/281] loss 5.1719 (5.0244) acc 1.5625 (2.0312) lr 1.0000e-05 eta 0:00:16
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [125/281] loss 4.9766 (5.0366) acc 1.5625 (2.0250) lr 1.0000e-05 eta 0:00:16
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [130/281] loss 4.7930 (5.0332) acc 6.2500 (2.0673) lr 1.0000e-05 eta 0:00:15
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [135/281] loss 5.1680 (5.0306) acc 3.1250 (2.0833) lr 1.0000e-05 eta 0:00:15
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [140/281] loss 4.9648 (5.0263) acc 1.5625 (2.0759) lr 1.0000e-05 eta 0:00:15
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [145/281] loss 5.0547 (5.0223) acc 4.6875 (2.0905) lr 1.0000e-05 eta 0:00:14
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [150/281] loss 5.1680 (5.0220) acc 0.0000 (2.1042) lr 1.0000e-05 eta 0:00:14
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [155/281] loss 5.0234 (5.0176) acc 3.1250 (2.0867) lr 1.0000e-05 eta 0:00:13
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [160/281] loss 4.9414 (5.0121) acc 1.5625 (2.0703) lr 1.0000e-05 eta 0:00:13
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [165/281] loss 4.9219 (5.0084) acc 1.5625 (2.0739) lr 1.0000e-05 eta 0:00:13
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [170/281] loss 5.0234 (5.0041) acc 0.0000 (2.1048) lr 1.0000e-05 eta 0:00:12
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [175/281] loss 4.8516 (5.0014) acc 1.5625 (2.0625) lr 1.0000e-05 eta 0:00:12
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [180/281] loss 4.9805 (4.9990) acc 3.1250 (2.0833) lr 1.0000e-05 eta 0:00:12
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [185/281] loss 4.5820 (4.9954) acc 4.6875 (2.0608) lr 1.0000e-05 eta 0:00:12
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [190/281] loss 5.1523 (4.9929) acc 4.6875 (2.0641) lr 1.0000e-05 eta 0:00:11
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [195/281] loss 5.1016 (4.9941) acc 1.5625 (2.0593) lr 1.0000e-05 eta 0:00:11
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [200/281] loss 4.7695 (4.9897) acc 1.5625 (2.0938) lr 1.0000e-05 eta 0:00:11
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [205/281] loss 5.0977 (4.9883) acc 4.6875 (2.1037) lr 1.0000e-05 eta 0:00:11
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [210/281] loss 4.6250 (4.9865) acc 0.0000 (2.1354) lr 1.0000e-05 eta 0:00:10
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [215/281] loss 4.5820 (4.9864) acc 4.6875 (2.1512) lr 1.0000e-05 eta 0:00:10
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [220/281] loss 5.2617 (4.9858) acc 4.6875 (2.1449) lr 1.0000e-05 eta 0:00:10
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [225/281] loss 4.8203 (4.9873) acc 1.5625 (2.1528) lr 1.0000e-05 eta 0:00:10
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [230/281] loss 4.6211 (4.9848) acc 3.1250 (2.1671) lr 1.0000e-05 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [235/281] loss 4.8867 (4.9819) acc 1.5625 (2.1875) lr 1.0000e-05 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [240/281] loss 4.9844 (4.9830) acc 0.0000 (2.1680) lr 1.0000e-05 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [245/281] loss 5.2539 (4.9821) acc 0.0000 (2.1429) lr 1.0000e-05 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [250/281] loss 4.8750 (4.9805) acc 0.0000 (2.1313) lr 1.0000e-05 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [255/281] loss 4.8789 (4.9805) acc 0.0000 (2.1201) lr 1.0000e-05 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [260/281] loss 5.1367 (4.9800) acc 4.6875 (2.1635) lr 1.0000e-05 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [265/281] loss 4.9336 (4.9784) acc 1.5625 (2.1521) lr 1.0000e-05 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [270/281] loss 5.0039 (4.9783) acc 0.0000 (2.1412) lr 1.0000e-05 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [275/281] loss 5.0781 (4.9748) acc 1.5625 (2.1136) lr 1.0000e-05 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [1/2] batch [280/281] loss 4.4688 (4.9741) acc 1.5625 (2.1150) lr 1.0000e-05 eta 0:00:07
mode:train
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [5/281] loss 4.1523 (4.5219) acc 4.6875 (5.0000) lr 2.0000e-03 eta 0:01:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [10/281] loss 3.0781 (3.9500) acc 7.8125 (7.1875) lr 2.0000e-03 eta 0:00:34
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [15/281] loss 2.5430 (3.5082) acc 10.9375 (9.6875) lr 2.0000e-03 eta 0:00:23
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [20/281] loss 2.3906 (3.2283) acc 14.0625 (10.6250) lr 2.0000e-03 eta 0:00:18
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [25/281] loss 2.2422 (3.0509) acc 15.6250 (11.3750) lr 2.0000e-03 eta 0:00:14
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [30/281] loss 2.4062 (2.9294) acc 7.8125 (11.8229) lr 2.0000e-03 eta 0:00:12
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [35/281] loss 2.3789 (2.8461) acc 9.3750 (12.0089) lr 2.0000e-03 eta 0:00:11
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [40/281] loss 2.3496 (2.7812) acc 10.9375 (12.1875) lr 2.0000e-03 eta 0:00:10
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [45/281] loss 2.2930 (2.7308) acc 17.1875 (12.3264) lr 2.0000e-03 eta 0:00:09
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [50/281] loss 2.3652 (2.6873) acc 14.0625 (12.6250) lr 2.0000e-03 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [55/281] loss 2.3066 (2.6541) acc 18.7500 (12.8693) lr 2.0000e-03 eta 0:00:08
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [60/281] loss 2.2695 (2.6214) acc 14.0625 (13.2292) lr 2.0000e-03 eta 0:00:07
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [65/281] loss 2.2930 (2.5970) acc 14.0625 (13.3173) lr 2.0000e-03 eta 0:00:07
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [70/281] loss 2.2988 (2.5748) acc 12.5000 (13.3705) lr 2.0000e-03 eta 0:00:06
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [75/281] loss 2.2598 (2.5560) acc 17.1875 (13.4375) lr 2.0000e-03 eta 0:00:06
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [80/281] loss 2.2812 (2.5375) acc 10.9375 (13.4766) lr 2.0000e-03 eta 0:00:06
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [85/281] loss 2.2969 (2.5244) acc 10.9375 (13.3456) lr 2.0000e-03 eta 0:00:05
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [90/281] loss 2.2715 (2.5095) acc 17.1875 (13.5938) lr 2.0000e-03 eta 0:00:05
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [95/281] loss 2.2441 (2.4969) acc 14.0625 (13.7500) lr 2.0000e-03 eta 0:00:05
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [100/281] loss 2.3105 (2.4868) acc 10.9375 (13.6406) lr 2.0000e-03 eta 0:00:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [105/281] loss 2.3066 (2.4757) acc 6.2500 (13.7351) lr 2.0000e-03 eta 0:00:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [110/281] loss 2.2715 (2.4653) acc 14.0625 (13.7500) lr 2.0000e-03 eta 0:00:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [115/281] loss 2.2598 (2.4567) acc 17.1875 (13.8043) lr 2.0000e-03 eta 0:00:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [120/281] loss 2.2207 (2.4483) acc 14.0625 (13.7891) lr 2.0000e-03 eta 0:00:04
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [125/281] loss 2.3340 (2.4418) acc 10.9375 (13.7375) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [130/281] loss 2.2891 (2.4365) acc 17.1875 (13.7019) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [135/281] loss 2.2715 (2.4313) acc 12.5000 (13.6343) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [140/281] loss 2.2344 (2.4251) acc 18.7500 (13.6161) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [145/281] loss 2.2578 (2.4192) acc 17.1875 (13.7392) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [150/281] loss 2.2188 (2.4143) acc 25.0000 (13.8021) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [155/281] loss 2.3047 (2.4097) acc 17.1875 (13.9113) lr 2.0000e-03 eta 0:00:03
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [160/281] loss 2.2500 (2.4051) acc 18.7500 (13.9453) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [165/281] loss 2.2539 (2.4015) acc 6.2500 (13.7784) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [170/281] loss 2.3027 (2.3979) acc 10.9375 (13.7960) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [175/281] loss 2.2812 (2.3943) acc 10.9375 (13.7321) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [180/281] loss 2.2324 (2.3914) acc 17.1875 (13.7326) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [185/281] loss 2.2773 (2.3885) acc 17.1875 (13.8514) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [190/281] loss 2.2305 (2.3854) acc 18.7500 (13.9391) lr 2.0000e-03 eta 0:00:02
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [195/281] loss 2.3320 (2.3829) acc 10.9375 (13.9744) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [200/281] loss 2.2812 (2.3809) acc 14.0625 (13.9531) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [205/281] loss 2.2539 (2.3780) acc 14.0625 (13.9634) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [210/281] loss 2.2695 (2.3751) acc 10.9375 (13.9807) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [215/281] loss 2.2812 (2.3726) acc 12.5000 (13.9608) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [220/281] loss 2.2461 (2.3704) acc 15.6250 (13.9631) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [225/281] loss 2.3242 (2.3687) acc 17.1875 (13.9167) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [230/281] loss 2.2988 (2.3669) acc 12.5000 (13.9130) lr 2.0000e-03 eta 0:00:01
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [235/281] loss 2.2402 (2.3644) acc 15.6250 (13.9029) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [240/281] loss 2.3086 (2.3627) acc 10.9375 (13.9583) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [245/281] loss 2.2461 (2.3609) acc 15.6250 (13.9796) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [250/281] loss 2.2832 (2.3596) acc 12.5000 (13.9812) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [255/281] loss 2.2344 (2.3577) acc 23.4375 (14.0257) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [260/281] loss 2.3008 (2.3566) acc 14.0625 (13.9543) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [265/281] loss 2.2676 (2.3552) acc 14.0625 (13.9446) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [270/281] loss 2.2344 (2.3536) acc 20.3125 (13.9815) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [275/281] loss 2.2754 (2.3520) acc 10.9375 (13.9659) lr 2.0000e-03 eta 0:00:00
mode:train
mode:train
mode:train
mode:train
mode:train
epoch [2/2] batch [280/281] loss 2.2520 (2.3505) acc 12.5000 (13.9509) lr 2.0000e-03 eta 0:00:00
mode:train
Model Saved to: output/CLIPAdapter-ViTB32-Digits-mnist/model.pth.tar-2
Finish Training
Evaluate on the Test Set
{'original': ['a picture of a 0.', 'a picture of a 1.', 'a picture of a 2.', 'a picture of a 3.', 'a picture of a 4.', 'a picture of a 5.', 'a picture of a 6.', 'a picture of a 7.', 'a picture of a 8.', 'a picture of a 9.'], 'syn': ['a picture of a syn 0.', 'a picture of a syn 1.', 'a picture of a syn 2.', 'a picture of a syn 3.', 'a picture of a syn 4.', 'a picture of a syn 5.', 'a picture of a syn 6.', 'a picture of a syn 7.', 'a picture of a syn 8.', 'a picture of a syn 9.']}
{'original': tensor([[ 0.0056,  0.0070, -0.0155,  ..., -0.0516, -0.0005,  0.0214],
        [ 0.0027,  0.0012, -0.0182,  ..., -0.0464, -0.0058,  0.0013],
        [-0.0100,  0.0206, -0.0230,  ..., -0.0352, -0.0070,  0.0087],
        ...,
        [-0.0050,  0.0148, -0.0157,  ..., -0.0466, -0.0228,  0.0067],
        [-0.0096,  0.0044, -0.0089,  ..., -0.0486, -0.0135,  0.0110],
        [-0.0076, -0.0037, -0.0106,  ..., -0.0497, -0.0120,  0.0157]],
       device='cuda:4', dtype=torch.float16), 'syn': tensor([[ 0.0530,  0.0715,  0.0257,  ...,  0.0082,  0.0163, -0.0439],
        [ 0.0452,  0.0656,  0.0288,  ...,  0.0162,  0.0166, -0.0482],
        [ 0.0378,  0.0569,  0.0180,  ...,  0.0030,  0.0089, -0.0460],
        ...,
        [ 0.0209,  0.0424,  0.0245,  ..., -0.0107, -0.0040, -0.0398],
        [ 0.0329,  0.0427,  0.0237,  ..., -0.0188,  0.0055, -0.0322],
        [ 0.0357,  0.0500,  0.0341,  ..., -0.0005,  0.0155, -0.0371]],
       device='cuda:4', dtype=torch.float16)}
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
mode:eval
----------  ------
Total #     6,000
Correct #   2,363
Accuracy    39.38%
Error Rate  60.62%
Macro_F1    35.14%
----------  ------
