*** Config ***
***************
** Arguments **
***************
dataset: Digits
gpu: 0
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-Digits-mnist
root: ./data/
seed: 134
source_domains: ['mnist_m', 'svhn', 'syn']
target_domains: ['mnist']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 64
    SAMPLER: RandomSampler
DATASET:
  NAME: Digits
  ROOT: ./data/
  SOURCE_DOMAINS: ['mnist_m', 'svhn', 'syn']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['mnist']
GPU: 0
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-Digits-mnist
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: Digits
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  --------------------------
Dataset         Digits
Source Domains  ['mnist_m', 'svhn', 'syn']
Target Domains  ['mnist']
# Classes       10
# Train Data    18,000
# Val Data      3,600
# Test Data     6,000
--------------  --------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
{'original': ['a picture of a 0.', 'a picture of a 1.', 'a picture of a 2.', 'a picture of a 3.', 'a picture of a 4.', 'a picture of a 5.', 'a picture of a 6.', 'a picture of a 7.', 'a picture of a 8.', 'a picture of a 9.'], 'mnist_m': ['a picture of a mnist m 0.', 'a picture of a mnist m 1.', 'a picture of a mnist m 2.', 'a picture of a mnist m 3.', 'a picture of a mnist m 4.', 'a picture of a mnist m 5.', 'a picture of a mnist m 6.', 'a picture of a mnist m 7.', 'a picture of a mnist m 8.', 'a picture of a mnist m 9.'], 'svhn': ['a picture of a svhn 0.', 'a picture of a svhn 1.', 'a picture of a svhn 2.', 'a picture of a svhn 3.', 'a picture of a svhn 4.', 'a picture of a svhn 5.', 'a picture of a svhn 6.', 'a picture of a svhn 7.', 'a picture of a svhn 8.', 'a picture of a svhn 9.'], 'syn': ['a picture of a syn 0.', 'a picture of a syn 1.', 'a picture of a syn 2.', 'a picture of a syn 3.', 'a picture of a syn 4.', 'a picture of a syn 5.', 'a picture of a syn 6.', 'a picture of a syn 7.', 'a picture of a syn 8.', 'a picture of a syn 9.']}
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapter.fc.2.weight', 'adapter.fc.0.weight'}
Build Evaluator: Classification
epoch [1/2] batch [5/281] loss 2.3867 (2.3238) acc 10.9375 (13.7500) lr 1.0000e-05 eta 0:04:18
epoch [1/2] batch [10/281] loss 2.2812 (2.3396) acc 14.0625 (12.9688) lr 1.0000e-05 eta 0:02:11
epoch [1/2] batch [15/281] loss 2.2734 (2.3380) acc 17.1875 (12.9167) lr 1.0000e-05 eta 0:01:29
epoch [1/2] batch [20/281] loss 2.3105 (2.3381) acc 21.8750 (13.4375) lr 1.0000e-05 eta 0:01:07
epoch [1/2] batch [25/281] loss 2.3770 (2.3408) acc 6.2500 (13.0000) lr 1.0000e-05 eta 0:00:54
epoch [1/2] batch [30/281] loss 2.2793 (2.3411) acc 15.6250 (12.9167) lr 1.0000e-05 eta 0:00:46
epoch [1/2] batch [35/281] loss 2.2715 (2.3412) acc 18.7500 (13.1250) lr 1.0000e-05 eta 0:00:40
epoch [1/2] batch [40/281] loss 2.3086 (2.3441) acc 10.9375 (12.6562) lr 1.0000e-05 eta 0:00:36
epoch [1/2] batch [45/281] loss 2.3672 (2.3417) acc 7.8125 (12.6736) lr 1.0000e-05 eta 0:00:33
epoch [1/2] batch [50/281] loss 2.3496 (2.3415) acc 14.0625 (12.7500) lr 1.0000e-05 eta 0:00:30
epoch [1/2] batch [55/281] loss 2.3262 (2.3422) acc 9.3750 (12.6705) lr 1.0000e-05 eta 0:00:28
epoch [1/2] batch [60/281] loss 2.3809 (2.3434) acc 17.1875 (12.7083) lr 1.0000e-05 eta 0:00:26
epoch [1/2] batch [65/281] loss 2.4023 (2.3437) acc 6.2500 (12.5240) lr 1.0000e-05 eta 0:00:25
epoch [1/2] batch [70/281] loss 2.3242 (2.3432) acc 10.9375 (12.6786) lr 1.0000e-05 eta 0:00:23
epoch [1/2] batch [75/281] loss 2.2969 (2.3417) acc 10.9375 (12.6458) lr 1.0000e-05 eta 0:00:22
epoch [1/2] batch [80/281] loss 2.3008 (2.3417) acc 14.0625 (12.6562) lr 1.0000e-05 eta 0:00:21
epoch [1/2] batch [85/281] loss 2.3398 (2.3409) acc 12.5000 (12.7022) lr 1.0000e-05 eta 0:00:20
epoch [1/2] batch [90/281] loss 2.3711 (2.3414) acc 10.9375 (12.5694) lr 1.0000e-05 eta 0:00:19
epoch [1/2] batch [95/281] loss 2.3574 (2.3411) acc 14.0625 (12.5822) lr 1.0000e-05 eta 0:00:19
epoch [1/2] batch [100/281] loss 2.3242 (2.3396) acc 7.8125 (12.5156) lr 1.0000e-05 eta 0:00:18
epoch [1/2] batch [105/281] loss 2.3320 (2.3387) acc 15.6250 (12.5744) lr 1.0000e-05 eta 0:00:17
epoch [1/2] batch [110/281] loss 2.2852 (2.3369) acc 10.9375 (12.6989) lr 1.0000e-05 eta 0:00:17
epoch [1/2] batch [115/281] loss 2.2930 (2.3365) acc 20.3125 (12.7446) lr 1.0000e-05 eta 0:00:16
epoch [1/2] batch [120/281] loss 2.4062 (2.3374) acc 4.6875 (12.6172) lr 1.0000e-05 eta 0:00:16
epoch [1/2] batch [125/281] loss 2.3730 (2.3374) acc 7.8125 (12.6625) lr 1.0000e-05 eta 0:00:15
epoch [1/2] batch [130/281] loss 2.3047 (2.3369) acc 10.9375 (12.6803) lr 1.0000e-05 eta 0:00:15
epoch [1/2] batch [135/281] loss 2.2852 (2.3362) acc 17.1875 (12.7546) lr 1.0000e-05 eta 0:00:14
epoch [1/2] batch [140/281] loss 2.3594 (2.3366) acc 15.6250 (12.7567) lr 1.0000e-05 eta 0:00:14
epoch [1/2] batch [145/281] loss 2.3516 (2.3375) acc 10.9375 (12.6724) lr 1.0000e-05 eta 0:00:14
epoch [1/2] batch [150/281] loss 2.3633 (2.3374) acc 10.9375 (12.6667) lr 1.0000e-05 eta 0:00:13
epoch [1/2] batch [155/281] loss 2.3379 (2.3367) acc 12.5000 (12.7218) lr 1.0000e-05 eta 0:00:13
epoch [1/2] batch [160/281] loss 2.3496 (2.3369) acc 9.3750 (12.6074) lr 1.0000e-05 eta 0:00:13
epoch [1/2] batch [165/281] loss 2.3184 (2.3366) acc 10.9375 (12.5758) lr 1.0000e-05 eta 0:00:12
epoch [1/2] batch [170/281] loss 2.3379 (2.3355) acc 20.3125 (12.6654) lr 1.0000e-05 eta 0:00:12
epoch [1/2] batch [175/281] loss 2.3086 (2.3358) acc 15.6250 (12.6607) lr 1.0000e-05 eta 0:00:12
epoch [1/2] batch [180/281] loss 2.3477 (2.3361) acc 15.6250 (12.6562) lr 1.0000e-05 eta 0:00:11
epoch [1/2] batch [185/281] loss 2.2930 (2.3363) acc 17.1875 (12.7365) lr 1.0000e-05 eta 0:00:11
epoch [1/2] batch [190/281] loss 2.2832 (2.3353) acc 20.3125 (12.7303) lr 1.0000e-05 eta 0:00:11
epoch [1/2] batch [195/281] loss 2.3438 (2.3353) acc 12.5000 (12.7083) lr 1.0000e-05 eta 0:00:11
epoch [1/2] batch [200/281] loss 2.3652 (2.3353) acc 18.7500 (12.7031) lr 1.0000e-05 eta 0:00:10
epoch [1/2] batch [205/281] loss 2.3574 (2.3359) acc 15.6250 (12.6753) lr 1.0000e-05 eta 0:00:10
epoch [1/2] batch [210/281] loss 2.3477 (2.3359) acc 12.5000 (12.6190) lr 1.0000e-05 eta 0:00:10
epoch [1/2] batch [215/281] loss 2.2852 (2.3355) acc 9.3750 (12.6453) lr 1.0000e-05 eta 0:00:10
epoch [1/2] batch [220/281] loss 2.3906 (2.3358) acc 14.0625 (12.6491) lr 1.0000e-05 eta 0:00:09
epoch [1/2] batch [225/281] loss 2.2754 (2.3359) acc 21.8750 (12.6806) lr 1.0000e-05 eta 0:00:09
epoch [1/2] batch [230/281] loss 2.3359 (2.3365) acc 9.3750 (12.6291) lr 1.0000e-05 eta 0:00:09
epoch [1/2] batch [235/281] loss 2.3105 (2.3368) acc 7.8125 (12.5798) lr 1.0000e-05 eta 0:00:09
epoch [1/2] batch [240/281] loss 2.3867 (2.3374) acc 3.1250 (12.5456) lr 1.0000e-05 eta 0:00:09
epoch [1/2] batch [245/281] loss 2.3438 (2.3374) acc 9.3750 (12.5510) lr 1.0000e-05 eta 0:00:08
epoch [1/2] batch [250/281] loss 2.3047 (2.3375) acc 17.1875 (12.5500) lr 1.0000e-05 eta 0:00:08
epoch [1/2] batch [255/281] loss 2.3926 (2.3375) acc 9.3750 (12.5245) lr 1.0000e-05 eta 0:00:08
epoch [1/2] batch [260/281] loss 2.3164 (2.3373) acc 14.0625 (12.4760) lr 1.0000e-05 eta 0:00:08
epoch [1/2] batch [265/281] loss 2.3125 (2.3368) acc 18.7500 (12.5531) lr 1.0000e-05 eta 0:00:08
epoch [1/2] batch [270/281] loss 2.3281 (2.3365) acc 15.6250 (12.5347) lr 1.0000e-05 eta 0:00:07
epoch [1/2] batch [275/281] loss 2.3594 (2.3364) acc 17.1875 (12.5795) lr 1.0000e-05 eta 0:00:07
epoch [1/2] batch [280/281] loss 2.3477 (2.3365) acc 6.2500 (12.5502) lr 1.0000e-05 eta 0:00:07
epoch [2/2] batch [5/281] loss 2.3789 (2.3305) acc 6.2500 (12.8125) lr 2.0000e-03 eta 0:00:58
epoch [2/2] batch [10/281] loss 2.2949 (2.3285) acc 20.3125 (12.8125) lr 2.0000e-03 eta 0:00:32
epoch [2/2] batch [15/281] loss 2.3672 (2.3264) acc 14.0625 (12.2917) lr 2.0000e-03 eta 0:00:22
epoch [2/2] batch [20/281] loss 2.3301 (2.3198) acc 10.9375 (12.1875) lr 2.0000e-03 eta 0:00:17
epoch [2/2] batch [25/281] loss 2.2930 (2.3188) acc 14.0625 (12.2500) lr 2.0000e-03 eta 0:00:14
epoch [2/2] batch [30/281] loss 2.3965 (2.3206) acc 4.6875 (11.9792) lr 2.0000e-03 eta 0:00:12
epoch [2/2] batch [35/281] loss 2.3691 (2.3196) acc 15.6250 (12.5446) lr 2.0000e-03 eta 0:00:11
epoch [2/2] batch [40/281] loss 2.3770 (2.3221) acc 6.2500 (11.9922) lr 2.0000e-03 eta 0:00:10
epoch [2/2] batch [45/281] loss 2.3379 (2.3234) acc 9.3750 (12.0139) lr 2.0000e-03 eta 0:00:09
epoch [2/2] batch [50/281] loss 2.3555 (2.3225) acc 7.8125 (11.8750) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [55/281] loss 2.3359 (2.3236) acc 12.5000 (12.0455) lr 2.0000e-03 eta 0:00:08
epoch [2/2] batch [60/281] loss 2.3086 (2.3233) acc 12.5000 (12.3438) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [65/281] loss 2.3496 (2.3243) acc 15.6250 (12.5000) lr 2.0000e-03 eta 0:00:07
epoch [2/2] batch [70/281] loss 2.3086 (2.3220) acc 17.1875 (12.6786) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [75/281] loss 2.3359 (2.3221) acc 17.1875 (12.7500) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [80/281] loss 2.3672 (2.3212) acc 6.2500 (12.4609) lr 2.0000e-03 eta 0:00:06
epoch [2/2] batch [85/281] loss 2.3398 (2.3225) acc 17.1875 (12.5551) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [90/281] loss 2.2969 (2.3222) acc 12.5000 (12.4826) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [95/281] loss 2.2520 (2.3215) acc 12.5000 (12.5164) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [100/281] loss 2.3398 (2.3231) acc 7.8125 (12.4219) lr 2.0000e-03 eta 0:00:05
epoch [2/2] batch [105/281] loss 2.3633 (2.3227) acc 10.9375 (12.4107) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [110/281] loss 2.3750 (2.3235) acc 9.3750 (12.4148) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [115/281] loss 2.3398 (2.3239) acc 10.9375 (12.4864) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [120/281] loss 2.3008 (2.3231) acc 17.1875 (12.4479) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [125/281] loss 2.3164 (2.3230) acc 10.9375 (12.3500) lr 2.0000e-03 eta 0:00:04
epoch [2/2] batch [130/281] loss 2.2871 (2.3230) acc 14.0625 (12.3558) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [135/281] loss 2.3398 (2.3236) acc 12.5000 (12.3380) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [140/281] loss 2.2910 (2.3229) acc 14.0625 (12.4442) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [145/281] loss 2.3047 (2.3231) acc 9.3750 (12.3491) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [150/281] loss 2.2754 (2.3216) acc 10.9375 (12.5417) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [155/281] loss 2.3105 (2.3220) acc 12.5000 (12.5504) lr 2.0000e-03 eta 0:00:03
epoch [2/2] batch [160/281] loss 2.3203 (2.3213) acc 12.5000 (12.5000) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [165/281] loss 2.3398 (2.3214) acc 10.9375 (12.5189) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [170/281] loss 2.3164 (2.3209) acc 7.8125 (12.5368) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [175/281] loss 2.3105 (2.3203) acc 15.6250 (12.5893) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [180/281] loss 2.3008 (2.3200) acc 14.0625 (12.5955) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [185/281] loss 2.3242 (2.3194) acc 6.2500 (12.6182) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [190/281] loss 2.3223 (2.3193) acc 17.1875 (12.6727) lr 2.0000e-03 eta 0:00:02
epoch [2/2] batch [195/281] loss 2.2988 (2.3185) acc 14.0625 (12.6362) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [200/281] loss 2.3359 (2.3191) acc 14.0625 (12.6484) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [205/281] loss 2.3242 (2.3185) acc 15.6250 (12.6677) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [210/281] loss 2.3164 (2.3179) acc 23.4375 (12.7530) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [215/281] loss 2.2930 (2.3174) acc 18.7500 (12.8270) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [220/281] loss 2.3223 (2.3173) acc 21.8750 (12.9119) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [225/281] loss 2.3086 (2.3172) acc 10.9375 (12.9167) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [230/281] loss 2.3086 (2.3170) acc 10.9375 (12.9484) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [235/281] loss 2.3340 (2.3170) acc 10.9375 (12.9255) lr 2.0000e-03 eta 0:00:01
epoch [2/2] batch [240/281] loss 2.3281 (2.3168) acc 10.9375 (12.8906) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [245/281] loss 2.3203 (2.3165) acc 20.3125 (12.9273) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [250/281] loss 2.2812 (2.3164) acc 17.1875 (12.9187) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [255/281] loss 2.2891 (2.3154) acc 14.0625 (12.9657) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [260/281] loss 2.2949 (2.3155) acc 12.5000 (12.9087) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [265/281] loss 2.3281 (2.3155) acc 9.3750 (12.8833) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [270/281] loss 2.2441 (2.3151) acc 10.9375 (12.8530) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [275/281] loss 2.3047 (2.3148) acc 17.1875 (12.8864) lr 2.0000e-03 eta 0:00:00
epoch [2/2] batch [280/281] loss 2.2500 (2.3145) acc 12.5000 (12.8850) lr 2.0000e-03 eta 0:00:00
Model Saved to: output/CLIPAdapter-ViTB32-Digits-mnist/model.pth.tar-2
Finish Training
Evaluate on the Test Set
{'original': ['a picture of a 0.', 'a picture of a 1.', 'a picture of a 2.', 'a picture of a 3.', 'a picture of a 4.', 'a picture of a 5.', 'a picture of a 6.', 'a picture of a 7.', 'a picture of a 8.', 'a picture of a 9.'], 'mnist': ['a picture of a mnist 0.', 'a picture of a mnist 1.', 'a picture of a mnist 2.', 'a picture of a mnist 3.', 'a picture of a mnist 4.', 'a picture of a mnist 5.', 'a picture of a mnist 6.', 'a picture of a mnist 7.', 'a picture of a mnist 8.', 'a picture of a mnist 9.']}
{'original': tensor([[ 0.0056,  0.0070, -0.0155,  ..., -0.0516, -0.0005,  0.0214],
        [ 0.0027,  0.0012, -0.0182,  ..., -0.0464, -0.0058,  0.0013],
        [-0.0100,  0.0206, -0.0230,  ..., -0.0352, -0.0070,  0.0087],
        ...,
        [-0.0050,  0.0148, -0.0157,  ..., -0.0466, -0.0228,  0.0067],
        [-0.0096,  0.0044, -0.0089,  ..., -0.0486, -0.0135,  0.0110],
        [-0.0076, -0.0037, -0.0106,  ..., -0.0497, -0.0120,  0.0157]],
       device='cuda:0', dtype=torch.float16), 'mnist': tensor([[ 4.0436e-02,  1.8021e-02, -2.7100e-02,  ...,  4.8065e-03,
         -1.7395e-02,  6.8283e-03],
        [ 4.2969e-02,  2.2797e-02, -3.1067e-02,  ...,  1.4143e-03,
         -1.9562e-02, -3.3321e-03],
        [ 3.8940e-02,  1.9974e-02, -2.5101e-02,  ..., -8.5413e-05,
         -1.7715e-02,  1.6575e-03],
        ...,
        [ 3.8849e-02,  1.8814e-02, -2.5009e-02,  ..., -1.2054e-03,
         -2.6123e-02, -3.0689e-03],
        [ 3.9612e-02,  1.8906e-02, -1.1856e-02,  ...,  2.9707e-04,
         -1.5610e-02, -1.9341e-03],
        [ 3.9001e-02,  2.1744e-02, -1.5488e-02,  ..., -3.4008e-03,
         -1.3130e-02, -2.6464e-04]], device='cuda:0', dtype=torch.float16)}
----------  ------
Total #     6,000
Correct #   1,205
Accuracy    20.08%
Error Rate  79.92%
Macro_F1    16.16%
----------  ------
