*** Config ***
***************
** Arguments **
***************
dataset: VLCS
gpu: 1
model: CLIPAdapter
model_config_file: config/clipadapter.yaml
output_dir: output/CLIPAdapter-ViTB32-VLCS-labelme
root: ./data/
seed: 134
source_domains: ['caltech', 'labelme', 'sun']
target_domains: ['pascal']
************
** Config **
************
DATALOADER:
  NUM_WORKERS: 8
  TEST:
    BATCH_SIZE: 64
    SAMPLER: SequentialSampler
  TRAIN:
    BATCH_SIZE: 9
    SAMPLER: RandomSampler
DATASET:
  NAME: VLCS
  ROOT: ./data/
  SOURCE_DOMAINS: ['caltech', 'labelme', 'sun']
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ['pascal']
GPU: 1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ['random_resized_crop', 'random_flip', 'normalize']
MODEL:
  CLIPAdapter:
    BACKBONE: ViT-B/32
  NAME: CLIPAdapter
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: Cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  SGD_DAMPENING: 0
  SGD_NESTEROV: False
  STEP_SIZE: -1
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/CLIPAdapter-ViTB32-VLCS-labelme
SEED: 134
TEST:
  EVALUATOR: Classification
  FINAL_Model: last_step
  SPLIT: Test
TRAIN:
  PRINT_FREQ: 5
Build Trainer: CLIPAdapter
Build Dataset: VLCS
Transform for Train: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
Transform for Test: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
)
--------------  -----------------------------
Dataset         VLCS
Source Domains  ['caltech', 'labelme', 'sun']
Target Domains  ['pascal']
# Classes       5
# Train Data    4,630
# Val Data      517
# Test Data     1,013
--------------  -----------------------------
Loading CLIP Backbone: ViT-B/32
Building Custom CLIP
{'original': ['a picture of a bird.', 'a picture of a car.', 'a picture of a chair.', 'a picture of a dog.', 'a picture of a person.'], 'caltech': ['a picture of a caltech bird.', 'a picture of a caltech car.', 'a picture of a caltech chair.', 'a picture of a caltech dog.', 'a picture of a caltech person.'], 'labelme': ['a picture of a labelme bird.', 'a picture of a labelme car.', 'a picture of a labelme chair.', 'a picture of a labelme dog.', 'a picture of a labelme person.'], 'sun': ['a picture of a sun bird.', 'a picture of a sun car.', 'a picture of a sun chair.', 'a picture of a sun dog.', 'a picture of a sun person.']}
{'original': ['a picture of a bird.', 'a picture of a car.', 'a picture of a chair.', 'a picture of a dog.', 'a picture of a person.'], 'pascal': ['a picture of a pascal bird.', 'a picture of a pascal car.', 'a picture of a pascal chair.', 'a picture of a pascal dog.', 'a picture of a pascal person.']}
key:original
v:tensor([[ 0.0261,  0.0139, -0.0079,  ..., -0.0452,  0.0063, -0.0239],
        [ 0.0127,  0.0005, -0.0155,  ..., -0.0260, -0.0305,  0.0026],
        [-0.0192, -0.0017, -0.0055,  ..., -0.0482, -0.0271,  0.0161],
        [ 0.0130,  0.0088, -0.0394,  ..., -0.0427, -0.0132,  0.0195],
        [ 0.0049,  0.0079, -0.0012,  ..., -0.0593, -0.0150, -0.0023]],
       device='cuda:1', dtype=torch.float16)
key:caltech
v:tensor([[ 0.0276,  0.0288, -0.0238,  ...,  0.0355, -0.0157, -0.0562],
        [-0.0055, -0.0110, -0.0132,  ...,  0.0552, -0.0457, -0.0298],
        [-0.0238, -0.0109, -0.0115,  ...,  0.0328, -0.0339, -0.0108],
        [ 0.0197,  0.0110, -0.0477,  ...,  0.0368, -0.0341, -0.0074],
        [ 0.0067,  0.0021, -0.0213,  ...,  0.0265, -0.0194, -0.0214]],
       device='cuda:1', dtype=torch.float16)
key:labelme
v:tensor([[ 0.0263,  0.0403,  0.0055,  ..., -0.0258,  0.0183, -0.0487],
        [ 0.0159,  0.0213,  0.0028,  ..., -0.0226, -0.0081, -0.0183],
        [-0.0204,  0.0261,  0.0291,  ..., -0.0356, -0.0020, -0.0066],
        [ 0.0111,  0.0233, -0.0202,  ..., -0.0281,  0.0006,  0.0045],
        [ 0.0067,  0.0134,  0.0117,  ..., -0.0306,  0.0113, -0.0201]],
       device='cuda:1', dtype=torch.float16)
key:sun
v:tensor([[ 0.0368,  0.0010,  0.0301,  ...,  0.0216, -0.0019, -0.0079],
        [-0.0107, -0.0012, -0.0032,  ..., -0.0395, -0.0115, -0.0178],
        [-0.0456, -0.0118,  0.0098,  ..., -0.0246, -0.0250,  0.0122],
        [-0.0116,  0.0493,  0.0151,  ...,  0.0018,  0.0143,  0.0309],
        [-0.0170,  0.0112, -0.0035,  ..., -0.0395,  0.0077, -0.0065]],
       device='cuda:1', dtype=torch.float16)
Turning Off Gradients in Image and Text Encoder
Parameters to be updated: {'adapters.1.fc.2.weight', 'adapters.0.fc.0.weight', 'adapters.2.fc.0.weight', 'adapters.2.fc.2.weight', 'adapter.fc.2.weight', 'adapters.1.fc.0.weight', 'adapters.0.fc.2.weight', 'adapter.fc.0.weight'}
Build Evaluator: Classification
Domains outputs: (tensor([[20.5469, 19.5000, 21.0156, 19.9688, 21.9531],
        [20.6094, 21.8594, 19.8750, 20.3281, 22.2188],
        [20.7500, 19.9688, 21.0312, 19.6094, 21.9688],
        [17.2969, 15.4844, 18.1406, 15.7812, 17.9844],
        [20.0625, 19.3906, 20.3281, 19.4375, 23.5781],
        [20.4844, 22.8906, 20.4531, 20.0000, 21.9844],
        [17.4844, 18.1562, 19.5469, 17.5000, 18.8281],
        [17.8438, 21.5469, 16.4844, 17.3906, 19.5781],
        [17.8750, 16.7812, 16.6094, 17.6250, 23.9531]], device='cuda:1',
       dtype=torch.float16, grad_fn=<SplitBackward0>), tensor([[19.5625, 18.7812, 20.0938, 19.8750, 20.7969],
        [19.0156, 19.7500, 19.5469, 19.3906, 20.4531],
        [20.7969, 20.0312, 22.1719, 21.2969, 22.5781],
        [18.4531, 17.0469, 20.2812, 18.3906, 20.0781],
        [21.9688, 21.5938, 23.1562, 20.9219, 25.3125],
        [17.9062, 21.0781, 19.6875, 17.3438, 20.5625],
        [17.3281, 18.0938, 20.2344, 17.6875, 19.7344],
        [14.3438, 16.7188, 14.3203, 13.8281, 16.4062],
        [19.0781, 15.9766, 19.0000, 18.6719, 23.5000]], device='cuda:1',
       dtype=torch.float16, grad_fn=<SplitBackward0>), tensor([[22.1250, 22.2656, 22.9062, 22.1406, 22.6719],
        [21.8438, 23.9219, 21.9219, 21.8750, 22.8281],
        [23.0000, 23.4531, 24.3594, 22.5000, 23.6719],
        [18.9844, 18.2344, 20.9375, 17.9688, 19.6562],
        [21.4688, 20.9688, 21.4375, 20.1875, 24.5938],
        [22.0781, 25.0000, 21.7812, 21.8438, 23.0000],
        [19.6875, 20.0000, 22.2812, 20.0312, 20.4062],
        [19.2969, 23.5625, 17.9531, 19.0938, 20.4688],
        [20.7500, 18.5781, 18.7812, 19.9531, 24.9531]], device='cuda:1',
       dtype=torch.float16, grad_fn=<SplitBackward0>), tensor([[18.9844, 23.0781, 21.5469, 19.7031, 22.8281],
        [17.2188, 21.7188, 18.2188, 14.6797, 19.9219],
        [19.7969, 22.1562, 20.8750, 18.7344, 21.7031],
        [17.3906, 18.5156, 18.0938, 15.9453, 16.8281],
        [19.0469, 21.5312, 20.3438, 17.3750, 22.9062],
        [17.3125, 24.3281, 20.8906, 17.0625, 21.1562],
        [18.3594, 20.2969, 18.9844, 17.7188, 18.1250],
        [15.6016, 22.7656, 16.1406, 15.4922, 20.0312],
        [15.9062, 17.3906, 16.2812, 15.5781, 21.5938]], device='cuda:1',
       dtype=torch.float16, grad_fn=<SplitBackward0>))
